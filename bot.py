import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path

import io
import numpy as np
import cv2
import pytesseract
from PIL import Image






STATE_DIR = Path("state")
POSTED_FILE = STATE_DIR / "load_posted.json"

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))

def strip_intro_phrases(text: str) -> str:
    patterns = [
        r"^–≤\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–∫–∞–∂–¥(—É—é|—ã–π|–æ–µ)?\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–≤\s+\w+\s+",  # –≤ —è–Ω–≤–∞—Ä–µ, –≤ –º–∞—Ä—Ç–µ –∏ —Ç.–ø.
        r"^–ø—Ä–∏–≥–ª–∞—à–∞–µ–º\s+",
        r"^—Å–æ—Å—Ç–æ–∏—Ç—Å—è\s+",
        r"^–±—É–¥–µ—Ç\s+",
    ]

    s = text.strip()

    for p in patterns:
        s = re.sub(p, "", s, flags=re.IGNORECASE)

    return s.strip(" -‚Äì‚Ä¢,")

def remove_city_from_title(title: str) -> str:
    for city_key in KZ_CITIES.keys():
        pattern = re.compile(rf"{city_key}", re.IGNORECASE)
        title = pattern.sub("", title)

    title = re.sub(r"\s{2,}", " ", title)
    return title.strip(" -‚Äì‚Ä¢,")

def fix_glued_words(text: str) -> str:
    text = re.sub(r'([–∞-—è—ë])([A-Z])', r'\1 \2', text)
    text = re.sub(r'([a-z])([–ê-–Ø–Å])', r'\1 \2', text)
    text = re.sub(r'([–∞-—è—ë])([–ê-–Ø–Å])', r'\1 \2', text)
    return text

def extract_city_from_title(title: str) -> Optional[str]:
    lower = title.lower()
    for key, value in KZ_CITIES.items():
        if key in lower:
            return value
    return None

def is_clean_photo(url: str) -> bool:
    url = url.lower()
    blacklist = [
        "banner", "poster", "event", "flyer",
        "afisha", "1080x", "square", "card",
    ]
    return not any(word in url for word in blacklist)

def remove_weekday_from_start(text: str) -> str:
    lower = text.lower()
    for key in WEEK_DAYS.keys():
        if lower.startswith(key + " "):
            return text[len(key):].strip(" -‚Äì‚Ä¢, ")
        if lower.startswith("–≤ " + key + " "):
            return text[len("–≤ " + key):].strip(" -‚Äì‚Ä¢, ")
        if lower.startswith("–∫–∞–∂–¥—É—é " + key + " "):
            return text[len("–∫–∞–∂–¥—É—é " + key):].strip(" -‚Äì‚Ä¢, ")
    return text

def generate_universal_description(full_text: str, title: str) -> str:
    text = strip_emoji(full_text)
    text = normalize_glued_text(text)

    if title:
        text = re.sub(re.escape(title), "", text, flags=re.IGNORECASE)

    text = re.sub(r"http\S+", "", text)
    paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 50]

    for p in paragraphs:
        low = p.lower()
        if any(w in low for w in [
            "—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å", "telegram", "facebook",
            "whatsapp", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"
        ]):
            continue

        if any(x in low for x in [
            "—É–ª.", "—É–ª–∏—Ü–∞", "–ø—Ä.", "–ø—Ä–æ—Å–ø–µ–∫—Ç",
            "—ç—Ç–∞–∂", "–æ—Ñ–∏—Å", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü",
            "–∑–¥–∞–Ω–∏–µ", "—Ä–∞–π–æ–Ω", "–æ—Å—Ç–∞–Ω–æ–≤"
        ]):
            continue

        if re.search(r"\d{1,2}:\d{2}", p):
            continue

        if re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]+", p):
            continue

        words = p.split()
        if len(words) > 15:
            if len(words) > 30:
                return " ".join(words[:30]) + "..."
            return p

    return ""

def generate_fallback_description(title: str) -> str:
    t = title.lower()
    if "career" in t:
        return "–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤ –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –æ –≤—ã–±–æ—Ä–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –∫–∞—Ä—å–µ—Ä–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö."
    if "movie" in t:
        return "–ö–∏–Ω–æ-–≤—Å—Ç—Ä–µ—á–∞ —Å –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ IT."
    if "ai" in t or "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in t:
        return "–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ, –ø–æ—Å–≤—è—â—ë–Ω–Ω–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –≤ –±–∏–∑–Ω–µ—Å–µ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö."
    if "meetup" in t:
        return "–ù–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–∞—è –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±–º–µ–Ω–∞ –æ–ø—ã—Ç–æ–º –∏ –Ω–µ—Ç–≤–æ—Ä–∫–∏–Ω–≥–∞."
    if "—Ñ–æ—Ä—É–º" in t or "conference" in t:
        return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ —Å —É—á–∞—Å—Ç–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ç–µ–º."
    return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π."

def extract_program_block(full_text: str) -> str:
    text = strip_emoji(full_text)
    lines = text.split("\n")
    trigger_words = ["—á—Ç–æ —Ç–µ–±—è –∂–¥", "—á—Ç–æ –≤–∞—Å –∂–¥", "–≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ", "–ø—Ä–æ–≥—Ä–∞–º–º–∞", "–≤—ã —É–∑–Ω–∞–µ—Ç–µ"]
    start_index = None

    for i, line in enumerate(lines):
        low = line.lower()
        if any(word in low for word in trigger_words):
            start_index = i
            break

    if start_index is None:
        return ""

    collected = []
    for line in lines[start_index + 1:]:
        clean = line.strip()
        if not clean:
            continue
        if any(x in clean.lower() for x in ["üìç", "‚è∞", "http", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ"]):
            break
        if re.search(r"\d{1,2}:\d{2}", clean):
            break
        if len(clean) < 5:
            continue

        collected.append(clean)
        if len(collected) >= 4:
            break

    if not collected:
        return ""

    result = "\n".join(collected)
    words = result.split()
    if len(words) > 40:
        result = " ".join(words[:40]) + "..."

    return result.strip()

# OCR
DATE_REGEX = re.compile(
    r"\b\d{1,2}[:.]\d{2}\b|"
    r"\b\d{1,2}\s*(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)\b|"
    r"\b\d{4}\b|"
    r"\b(january|february|march|april|may|june|july|august|september|october|november|december)\b",
    re.IGNORECASE
)

async def smart_crop_text_zones(session, image_url: str):
    try:
        async with session.get(image_url, timeout=20) as resp:
            if resp.status != 200:
                return None
            data = await resp.read()

        pil_img = Image.open(io.BytesIO(data)).convert("RGB")
        img = np.array(pil_img)
        h, w, _ = img.shape
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        ocr = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)

        crop_top, crop_bottom = 0, h
        detected_top, detected_bottom = [], []

        for i, text in enumerate(ocr["text"]):
            text = text.strip()
            if not text:
                continue

            if DATE_REGEX.search(text):
                y = ocr["top"][i]
                bh = ocr["height"][i]
                if y < h * 0.45:
                    detected_top.append(y + bh)
                if y > h * 0.55:
                    detected_bottom.append(y)

        if detected_top:
            crop_top = max(detected_top) + 30
        if detected_bottom:
            crop_bottom = min(detected_bottom) - 30

        if crop_bottom - crop_top < h * 0.45:
            return None

        cropped = img[crop_top:crop_bottom, 0:w]
        if cropped.size == 0:
            return None

        final_img = Image.fromarray(cropped)
        buffer = io.BytesIO()
        final_img.save(buffer, format="JPEG", quality=95)
        buffer.seek(0)
        return buffer

    except Exception as e:
        print("OCR crop error:", e)
        return None

def normalize_link(link: str) -> str:
    if not link:
        return ""
    link = link.strip()
    link = link.replace("https://t.me/s/", "https://t.me/")
    link = link.split("?")[0]
    link = link.rstrip("/")
    return link

def load_posted() -> set:
    if POSTED_FILE.exists():
        try:
            with open(POSTED_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_posted(posted: set):
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)
        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(list(posted), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è posted_links: {e}")

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {"—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4, "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8, "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12}
MONTHS_SHORT = {"—è–Ω–≤": 1, "—Ñ–µ–≤": 2, "–º–∞—Ä": 3, "–∞–ø—Ä": 4, "–º–∞–π": 5, "–∏—é–Ω": 6, "–∏—é–ª": 7, "–∞–≤–≥": 8, "—Å–µ–Ω": 9, "–æ–∫—Ç": 10, "–Ω–æ—è": 11, "–¥–µ–∫": 12}

EVENT_WORDS = [
    "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è", "conference", "—Ñ–æ—Ä—É–º", "forum", "summit", "—Å–∞–º–º–∏—Ç", "meetup", "–º–∏—Ç–∞–ø",
    "—Ö–∞–∫–∞—Ç–æ–Ω", "hackathon", "–≤–æ—Ä–∫—à–æ–ø", "workshop", "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å", "masterclass", "–≤–µ–±–∏–Ω–∞—Ä",
    "webinar", "—Å–µ–º–∏–Ω–∞—Ä", "pitch", "–ø–∏—Ç—á", "demo day", "–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä", "accelerator",
    "bootcamp", "–±—É—Ç–∫–µ–º–ø", "–≤—ã—Å—Ç–∞–≤–∫–∞", "–∫–æ–Ω–∫—É—Ä—Å", "competition", "—Ç—Ä–µ–Ω–∏–Ω–≥", "training",
    "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ", "–∏–≤–µ–Ω—Ç", "event", "–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç", "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º", "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"
]
NOT_EVENT_WORDS = [
    "research", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ", "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª", "–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥", "–º–ª–Ω $", "–º–ª—Ä–¥ $",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω", "—É–≤–æ–ª–µ–Ω", "–æ—Ç—á–µ—Ç", "–≤—ã—Ä—É—á–∫–∞", "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–±–∏—Ä–∂–∞", "–∞–∫—Ü–∏–∏", "—Ç–æ–∫–∞–µ–≤", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ"
]
SITE_STOP_WORDS = [
    "–∫–æ–Ω—Ç–∞–∫—Ç—ã", "–æ –Ω–∞—Å", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–≤–æ–π—Ç–∏", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞", "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è",
    "–ø–æ–∏—Å–∫", "–≥–ª–∞–≤–Ω–∞—è", "–º–µ–Ω—é", "–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ", "privacy", "terms", "cookie"
]
DESCRIPTION_SIGNALS = [
    "—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏", "–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤", "–≤—ã —É–∑–Ω–∞–µ—Ç–µ", "–º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º", "–Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏",
    "–≤ —Ä–∞–º–∫–∞—Ö", "—Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤—Å—Ç—Ä–µ—á–∞", "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º –≤–∞—Å", "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Å—Å—ã–ª–∫–µ",
    "—Å–≤–æ–±–æ–¥–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ", "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç –≤–∞—Å –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ", "–≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–π—Ç–∏",
    "–ø–æ–≥–æ–≤–æ—Ä–∏–º –æ", "–æ–±—Å—É–¥–∏–º", "—Ä–∞–∑–±–µ—Ä–µ–º" 
]

KZ_CITIES = {
    "–∞–ª–º–∞—Ç—ã": "–ê–ª–º–∞—Ç—ã", "almaty": "–ê–ª–º–∞—Ç—ã", "–∞—Å—Ç–∞–Ω–∞": "–ê—Å—Ç–∞–Ω–∞", "astana": "–ê—Å—Ç–∞–Ω–∞", "nur-sultan": "–ê—Å—Ç–∞–Ω–∞",
    "nursultan": "–ê—Å—Ç–∞–Ω–∞", "–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω": "–ê—Å—Ç–∞–Ω–∞", "—à—ã–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç", "shymkent": "–®—ã–º–∫–µ–Ω—Ç",
    "—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "ust-kamenogorsk": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "oskemen": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",
    "–∫—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "kyzylorda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "–∞–∫—Ç–æ–±–µ": "–ê–∫—Ç–æ–±–µ", "aktobe": "–ê–∫—Ç–æ–±–µ",
    "—Ç–∞—Ä–∞–∑": "–¢–∞—Ä–∞–∑", "taraz": "–¢–∞—Ä–∞–∑", "–ø–∞–≤–ª–æ–¥–∞—Ä": "–ü–∞–≤–ª–æ–¥–∞—Ä", "pavlodar": "–ü–∞–≤–ª–æ–¥–∞—Ä",
    "–ø–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "petropavlovsk": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "—Å–µ–º–µ–π": "–°–µ–º–µ–π", "semey": "–°–µ–º–µ–π",
    "–∞—Ç—ã—Ä–∞—É": "–ê—Ç—ã—Ä–∞—É", "atyrau": "–ê—Ç—ã—Ä–∞—É", "–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω", "–∂–µ–∑“õ–∞–∑“ì–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "zhezkazgan": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω", "jezqazgan": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω", "–∞–∫—Ç–∞—É": "–ê–∫—Ç–∞—É", "aktau": "–ê–∫—Ç–∞—É",
    "–∫–æ–Ω–∞–µ–≤": "–ö–æ–Ω–∞–µ–≤", "qonaev": "–ö–æ–Ω–∞–µ–≤", "konaev": "–ö–æ–Ω–∞–µ–≤", "qonayev": "–ö–æ–Ω–∞–µ–≤", # üî• –î–æ–±–∞–≤–∏–ª–∏ –ö–æ–Ω–∞–µ–≤
    "–æ–Ω–ª–∞–π–Ω": "–û–Ω–ª–∞–π–Ω", "online": "–û–Ω–ª–∞–π–Ω", "zoom": "–û–Ω–ª–∞–π–Ω (Zoom)", "—Ç–∞—à–∫–µ–Ω—Ç": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω", "tashkent": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω",
}

WEEK_DAYS = {
    "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫—É": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–æ–º": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "monday": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "–≤—Ç–æ—Ä–Ω–∏–∫": "–í—Ç–æ—Ä–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫–∞": "–í—Ç–æ—Ä–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫—É": "–í—Ç–æ—Ä–Ω–∏–∫", "tuesday": "–í—Ç–æ—Ä–Ω–∏–∫",
    "—Å—Ä–µ–¥–∞": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥—É": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥—ã": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥–µ": "–°—Ä–µ–¥–∞", "wednesday": "–°—Ä–µ–¥–∞",
    "—á–µ—Ç–≤–µ—Ä–≥": "–ß–µ—Ç–≤–µ—Ä–≥", "—á–µ—Ç–≤–µ—Ä–≥–∞": "–ß–µ—Ç–≤–µ—Ä–≥", "—á–µ—Ç–≤–µ—Ä–≥—É": "–ß–µ—Ç–≤–µ—Ä–≥", "thursday": "–ß–µ—Ç–≤–µ—Ä–≥",
    "–ø—è—Ç–Ω–∏—Ü–∞": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü—É": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü—ã": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü–µ": "–ü—è—Ç–Ω–∏—Ü–∞", "friday": "–ü—è—Ç–Ω–∏—Ü–∞",
    "—Å—É–±–±–æ—Ç–∞": "–°—É–±–±–æ—Ç–∞", "—Å—É–±–±–æ—Ç—É": "–°—É–±–±–æ—Ç–∞", "—Å—É–±–±–æ—Ç—ã": "–°—É–±–±–æ—Ç–∞", "saturday": "–°—É–±–±–æ—Ç–∞",
    "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—è": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—é": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "sunday": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
}

EMOJI_RE = re.compile("[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]", re.UNICODE)

# Helpers
def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()

def is_future(dt: Optional[datetime]) -> bool:
    if not dt: return False
    return dt.date() > datetime.now().date()

def parse_date(text: str) -> Optional[datetime]:
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try: return datetime(year, month, day)
        except: return None

    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month: return make_dt(year, month, int(m.group(2)))

    m = re.search(r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(r"(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12: return make_dt(year, month, int(m.group(1)))

    return None

def format_date(dt: datetime, time_str: str = None) -> str:
    months = {1:"—è–Ω–≤–∞—Ä—è", 2:"—Ñ–µ–≤—Ä–∞–ª—è", 3:"–º–∞—Ä—Ç–∞", 4:"–∞–ø—Ä–µ–ª—è", 5:"–º–∞—è", 6:"–∏—é–Ω—è", 7:"–∏—é–ª—è", 8:"–∞–≤–≥—É—Å—Ç–∞", 9:"—Å–µ–Ω—Ç—è–±—Ä—è", 10:"–æ–∫—Ç—è–±—Ä—è", 11:"–Ω–æ—è–±—Ä—è", 12:"–¥–µ–∫–∞–±—Ä—è"}
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s

def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t: return value
    return None

def extract_venue(text: str) -> Optional[str]:
    known = ["Narxoz", "Nazarbayev", "KBTU", "–ö–ë–¢–£", "Astana Hub", "IT Park", "MOST IT Hub", "Holiday Inn", "Esentai", "Yandex", "Smart Point", "Almaty Arena"]
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf"{re.escape(v)}[^\n,.]*", text, re.IGNORECASE)
            if m: return m.group(0).strip()[:60]
    at = re.search(r"@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)", text)
    if at: return at.group(1).strip()[:60]
    return None

def is_real_event(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS)

def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)

def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)

def dedup_title(title: str) -> str:
    half = len(title) // 2
    for i in range(10, half):
        if title[i:].strip() == title[:len(title)-i].strip():
            return title[:len(title)-i].strip()
    return title

def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-z–ê-–Ø–∞-—è–Å—ë])", r"\1 ", s)
    s = re.sub(r"([–∞-—è—ë–ê-–Ø–Å]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)
    s = re.sub(r"\s{2,}", " ", s)
    return s

def strip_leading_datetime_from_title(title: str) -> str:
    t = strip_emoji(title).strip()
    t = normalize_glued_text(t)
    t = re.sub(r"^\s*\d{1,2}\s+[–ê-–Ø–∞-—è–Å—ëA-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^\s*\d{1,2}\s+[–∞-—è—ë]{3,}(?:\s+\d{4})?\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)
    return t.strip(" -‚Äì‚Ä¢.,").strip()

def clean_title_deterministic(raw_title: str) -> Optional[str]:
    s = strip_leading_datetime_from_title(raw_title)
    s = remove_weekday_from_start(s)
    s = strip_intro_phrases(s)
    s = fix_glued_words(s)
    s = dedup_title(s)
    s = remove_city_from_title(s)

    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx != -1 and idx > 12:
            s = s[:idx].strip(" -‚Äì‚Ä¢.,")
            break


    s = re.sub(r'\s+(–≤|–Ω–∞|—Å|–∏|–¥–ª—è|–æ—Ç|–∑–∞|–∫|–ø–æ|–∏–∑|—É|–æ|–æ–±|at|in|on|for|and|to|the)\s*$', '', s, flags=re.IGNORECASE)

    s = re.sub(r"\s{2,}", " ", s).strip(" -‚Äì‚Ä¢.,")
    
    if len(s) < 5 or looks_like_description(s): return None
    return s[:120]

city_pattern = "|".join([re.escape(v) for v in KZ_CITIES.values()])
_GLUE_RE = re.compile(
    rf"^(\d{{1,2}})\s+([–ê-–Ø–Å–∞-—è—ëA-Za-z]{{3,}})[,\s]+(\d{{1,2}}:\d{{2}})\s*(?:(–û–Ω–ª–∞–π–Ω|online|zoom|{city_pattern})\s*)?(.+)$",
    re.IGNORECASE
)

def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m: return None

    day_s, month_s, time_str = m.group(1), m.group(2).lower(), m.group(3)
    possible_city, title_raw = (m.group(4) or "").strip(), m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num: return None

    try: dt = datetime(datetime.now().year, month_num, int(day_s))
    except: return None
    if not is_future(dt): return None

    city = KZ_CITIES.get(possible_city.lower(), possible_city) if possible_city else None
    title_raw = dedup_title(title_raw)
    if len(title_raw) < 5: return None

    return {"dt": dt, "time_str": time_str, "city": city, "title_raw": title_raw[:300], "date_formatted": format_date(dt, time_str)}

# ‚îÄ‚îÄ‚îÄ Formatting post ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def make_post(event: Dict) -> str:
    title = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get("location", "")
    venue = event.get("venue", "")
    title = strip_leading_datetime_from_title(title)

    lines = [f"üéØ <b>{title}</b>"]

    # üî• 1Ô∏è‚É£ –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º: —Å–ø–∞—Ä—Å–∏–ª–∏ –ª–∏ –º—ã –≥–ª—É–±–æ–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
    deep_description = event.get("deep_description", "")
    
    # 2Ô∏è‚É£ –ë–ª–æ–∫ "–ß—Ç–æ —Ç–µ–±—è –∂–¥–µ—Ç"
    program_block = extract_program_block(event.get("full_text", ""))

    if deep_description:
        description = deep_description
    elif program_block:
        description = program_block
    else:
        # 3Ô∏è‚É£ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
        description = generate_universal_description(event.get("full_text", ""), title)

    # 4Ô∏è‚É£ –§–æ–ª–±–µ–∫
    if not description:
        description = generate_fallback_description(title)

    if description:
        lines.append("")
        lines.append(f"üìù {description}")

    if location in ("–û–Ω–ª–∞–π–Ω", "–û–Ω–ª–∞–π–Ω (Zoom)"):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue: lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {date_str}")
    lines.append(f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ Bot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            # –û–±–Ω–æ–≤–∏–º User-Agent, —á—Ç–æ–±—ã —Å–∞–π—Ç—ã —Ä–µ–∂–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞–ª–∏ –∑–∞–ø—Ä–æ—Å—ã
            self.session = aiohttp.ClientSession(
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"}
            )
        return self.session

    async def close(self):
        if self.session: await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    # üî• –ù–û–í–ê–Ø –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –ì–õ–£–ë–û–ö–û–ì–û –ü–ê–†–°–ò–ù–ì–ê –°–ê–ô–¢–û–í
    async def fetch_event_details(self, url: str) -> str:
        """–ü–µ—Ä–µ—Ö–æ–¥–∏—Ç –ø–æ —Å—Å—ã–ª–∫–µ –∏ –≤—ã—Ç—è–≥–∏–≤–∞–µ—Ç –≥–ª–∞–≤–Ω—ã–π –∞–±–∑–∞—Ü —Ç–µ–∫—Å—Ç–∞"""
        if not url or not url.startswith("http") or "t.me" in url:
            return ""

        try:
            html = await self.fetch(url)
            if not html: return ""
            soup = BeautifulSoup(html, "html.parser")

            # –£–¥–∞–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –º—É—Å–æ—Ä
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "menu", "form"]):
                tag.decompose()

            # –ò—â–µ–º —Ç–µ–≥–∏ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤
            paragraphs = soup.find_all("p")
            
            for p in paragraphs:
                text = p.get_text(separator=" ", strip=True)
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –∫–æ—Ä–æ—Ç–∫–∏–µ —Ç–µ–∫—Å—Ç—ã
                if len(text) > 80:
                    low = text.lower()
                    if any(bad in low for bad in ["cookie", "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è", "–ø–∞—Ä–æ–ª—å"]):
                        continue
                    
                    # –ü—Ä–∏—á–µ—Å—ã–≤–∞–µ–º —Ç–µ–∫—Å—Ç
                    text = re.sub(r"\s{2,}", " ", text)
                    
                    # –ö—Ä–∞—Å–∏–≤–æ –æ–±—Ä–µ–∑–∞–µ–º, –µ—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
                    words = text.split()
                    if len(words) > 40:
                        return " ".join(words[:40]) + "..."
                    
                    return text

            # –§–æ–ª–±–µ–∫: –∏—â–µ–º –º–µ—Ç–∞-–æ–ø–∏—Å–∞–Ω–∏–µ (SEO)
            meta_desc = soup.find("meta", attrs={"name": "description"})
            if meta_desc and meta_desc.get("content"):
                desc = meta_desc["content"].strip()
                if len(desc) > 50:
                    return desc
                    
        except Exception as e:
            logger.error(f"Error deep parsing {url}: {e}")
            
        return ""

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+\d{4})?)", line, re.IGNORECASE)
            if not dm:
                i += 1; continue

            date_raw = dm.group(0)
            rest = line[dm.end():].strip()
            tm = re.search(r"(?:–≤\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm: rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -‚Äì‚Ä¢")
            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"): link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith("http"): link = "https://" + link
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt): title_raw = nxt

            if len(title_raw) < 5:
                i += 1; continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                i += 1; continue

            ctx = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_location(ctx) or extract_location(text)
            title_clean = clean_title_deterministic(title_raw) or dedup_title(title_raw[:120])
            if not title_clean:
                i += 1; continue

            events.append({
                "title": title_clean, "date": format_date(dt, time_str), "location": location or "",
                "venue": extract_venue(ctx), "link": link or post_link, "source": source, "image_url": image_url
            })
            i += 1
        return events

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html: return []
        soup = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td: continue
                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30: continue

                le = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"
                norm_link = normalize_link(post_link)

                external_link = None
                links_in_text = re.findall(r"(https?://[^\s]+)", text)
                for l in links_in_text:
                    clean_l = normalize_link(l)
                    if "t.me" not in clean_l:
                        external_link = clean_l
                        break

                final_link = external_link if external_link else norm_link
                if norm_link in self.posted: continue

                external_link = None
                for a in td.find_all("a", href=True):
                    href = normalize_link(a["href"])
                    if "t.me" not in href:
                        external_link = href
                        break

                if not external_link:
                    links_in_text = re.findall(r"(https?://[^\s]+)", text)
                    for l in links_in_text:
                        clean_l = normalize_link(l)
                        if "t.me" not in clean_l:
                            external_link = clean_l
                            break

                final_link = external_link if external_link else norm_link
                image_url = None

                photo_wrap = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if photo_wrap:
                    style = photo_wrap.get("style", "")
                    match = re.search(r"url\('([^']+)'\)", style)
                    if match: image_url = match.group(1)

                if not image_url:
                    img_tag = td.find("img")
                    if img_tag and img_tag.get("src"): image_url = img_tag["src"]

                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    continue

                if not is_real_event(text): continue
                dt = parse_date(text)
                if not is_future(dt): continue

                title_candidate = None
                for ln in text.split("\n"):
                    ln = strip_emoji(ln).strip()
                    if len(ln) > 10:
                        title_candidate = ln
                        break

                raw_title = title_candidate or ""
                city_from_title = extract_city_from_title(raw_title)
                title = clean_title_deterministic(raw_title)
                if not title: continue

                tm2 = re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append({
                    "title": title, "date": format_date(dt, time_str), "location": extract_location(text) or city_from_title or "",
                    "venue": extract_venue(text), "link": final_link, "source": channel["name"], "full_text": text, "image_url": image_url
                })
            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue
        return all_events
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []
        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)

        return all_events

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site["url"])
        if not html: return []
        soup = BeautifulSoup(html, "html.parser")
        events = []

        for link in soup.find_all("a", href=True)[:80]:
            try:
                href = link.get("href", "")
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15: continue
                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)

                href = normalize_link(href)
                if href.rstrip("/") == normalize_link(site["url"]).rstrip("/"): continue
                if href in self.posted: continue
                if is_site_trash(title_raw): continue
                if not is_real_event(title_raw): continue

                parent = link.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt): continue
                image_url = None

                if parent:
                    imgs = parent.find_all("img")
                    for img in imgs:
                        src = img.get("src") or img.get("data-src")
                        if not src: continue
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src
                            break

                if not image_url and parent:
                    style = parent.get("style", "")
                    match = re.search(r"url\(['\"]?([^'\")]+)", style)
                    if match:
                        src = match.group(1)
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src

                title_clean = clean_title_deterministic(title_raw) or strip_emoji(dedup_title(title_raw))[:120]
                events.append({
                    "title": title_clean, "date": format_date(dt), "location": extract_location(context) or "",
                    "venue": extract_venue(context), "link": href, "full_text": context, "source": site["name"], "image_url": image_url
                })
                if len(events) >= 5: break
            except Exception:
                continue
        return events

# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot_api = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")
        logger.info(f"üì¶ –£–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {len(bot_obj.posted)}")

        posted = 0

        for event in unique[:15]:
            norm_link = normalize_link(event.get("link", ""))

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if norm_link in bot_obj.posted:
                logger.info(f"‚è≠Ô∏è –£–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ—Å—å: {event.get('title')[:50]}")
                continue

            # üî• ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ì–õ–£–ë–û–ö–ò–ô –ü–ê–†–°–ò–ù–ì: –ò–¥–µ–º –Ω–∞ —Å–∞–π—Ç –∑–∞ –æ–ø–∏—Å–∞–Ω–∏–µ–º ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ üî•
            deep_desc = await bot_obj.fetch_event_details(norm_link)
            if deep_desc:
                event["deep_description"] = deep_desc
                logger.info(f"–£—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—Å–∏–ª–∏ –∂–∏–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ —Å—Å—ã–ª–∫–µ: {norm_link}")

            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ—Å—Ç —Å —É—á–µ—Ç–æ–º –¥–æ–±—ã—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            text = make_post(event)
            if not text:
                continue

            try:
                photo_to_send = None

                if event.get("image_url"):
                    session = await bot_obj.get_session()
                    photo_to_send = await smart_crop_text_zones(session, event["image_url"])

                if photo_to_send:
                    await bot_api.send_photo(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        photo=photo_to_send,
                        caption=text,
                        parse_mode="HTML",
                    )
                else:
                    await bot_api.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                bot_obj.posted.add(norm_link)
                save_posted(bot_obj.posted)

                posted += 1
                logger.info(f"‚úÖ ({posted}) {event.get('title','')[:50]}")

                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –Ω–æ–≤—ã—Ö: {posted}")

    finally:
        await bot_obj.close()


if __name__ == "__main__":
    asyncio.run(main())
