import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1,
    "—Ñ–µ–≤—Ä–∞–ª—è": 2,
    "–º–∞—Ä—Ç–∞": 3,
    "–∞–ø—Ä–µ–ª—è": 4,
    "–º–∞—è": 5,
    "–∏—é–Ω—è": 6,
    "–∏—é–ª—è": 7,
    "–∞–≤–≥—É—Å—Ç–∞": 8,
    "—Å–µ–Ω—Ç—è–±—Ä—è": 9,
    "–æ–∫—Ç—è–±—Ä—è": 10,
    "–Ω–æ—è–±—Ä—è": 11,
    "–¥–µ–∫–∞–±—Ä—è": 12,
}
MONTHS_SHORT = {
    "—è–Ω–≤": 1,
    "—Ñ–µ–≤": 2,
    "–º–∞—Ä": 3,
    "–∞–ø—Ä": 4,
    "–º–∞–π": 5,
    "–∏—é–Ω": 6,
    "–∏—é–ª": 7,
    "–∞–≤–≥": 8,
    "—Å–µ–Ω": 9,
    "–æ–∫—Ç": 10,
    "–Ω–æ—è": 11,
    "–¥–µ–∫": 12,
}

EVENT_WORDS = [
    "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è",
    "conference",
    "—Ñ–æ—Ä—É–º",
    "forum",
    "summit",
    "—Å–∞–º–º–∏—Ç",
    "meetup",
    "–º–∏—Ç–∞–ø",
    "—Ö–∞–∫–∞—Ç–æ–Ω",
    "hackathon",
    "–≤–æ—Ä–∫—à–æ–ø",
    "workshop",
    "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å",
    "masterclass",
    "–≤–µ–±–∏–Ω–∞—Ä",
    "webinar",
    "—Å–µ–º–∏–Ω–∞—Ä",
    "pitch",
    "–ø–∏—Ç—á",
    "demo day",
    "–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä",
    "accelerator",
    "bootcamp",
    "–±—É—Ç–∫–µ–º–ø",
    "–≤—ã—Å—Ç–∞–≤–∫–∞",
    "–∫–æ–Ω–∫—É—Ä—Å",
    "competition",
    "—Ç—Ä–µ–Ω–∏–Ω–≥",
    "training",
    "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ",
    "–∏–≤–µ–Ω—Ç",
    "event",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º",
    "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è",
    "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è",
]
NOT_EVENT_WORDS = [
    "research",
    "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª",
    "–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥",
    "–º–ª–Ω $",
    "–º–ª—Ä–¥ $",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω",
    "—É–≤–æ–ª–µ–Ω",
    "–æ—Ç—á–µ—Ç",
    "–≤—ã—Ä—É—á–∫–∞",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞",
    "–±–∏—Ä–∂–∞",
    "–∞–∫—Ü–∏–∏",
    "—Ç–æ–∫–∞–µ–≤",
    "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ",
]
SITE_STOP_WORDS = [
    "–∫–æ–Ω—Ç–∞–∫—Ç—ã",
    "–æ –Ω–∞—Å",
    "–ø–æ–ª–∏—Ç–∏–∫–∞",
    "–≤–æ–π—Ç–∏",
    "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞",
    "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è",
    "–ø–æ–∏—Å–∫",
    "–≥–ª–∞–≤–Ω–∞—è",
    "–º–µ–Ω—é",
    "–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏",
    "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ",
    "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ",
    "privacy",
    "terms",
    "cookie",
]
DESCRIPTION_SIGNALS = [
    "—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏",
    "–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤",
    "–≤—ã —É–∑–Ω–∞–µ—Ç–µ",
    "–º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º",
    "–Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏",
    "–≤ —Ä–∞–º–∫–∞—Ö",
    "—Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤—Å—Ç—Ä–µ—á–∞",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º –≤–∞—Å",
    "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Å—Å—ã–ª–∫–µ",
    "—Å–≤–æ–±–æ–¥–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ",
    "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç –≤–∞—Å –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ",
    "–≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–π—Ç–∏",
]

KZ_CITIES = {
    "–∞–ª–º–∞—Ç—ã": "–ê–ª–º–∞—Ç—ã",
    "–∞—Å—Ç–∞–Ω–∞": "–ê—Å—Ç–∞–Ω–∞",
    "—à—ã–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç",
    "–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω": "–ê—Å—Ç–∞–Ω–∞",
    "—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",
    "–∫—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞",
    "–∞–∫—Ç–æ–±–µ": "–ê–∫—Ç–æ–±–µ",
    "—Ç–∞—Ä–∞–∑": "–¢–∞—Ä–∞–∑",
    "–ø–∞–≤–ª–æ–¥–∞—Ä": "–ü–∞–≤–ª–æ–¥–∞—Ä",
    "—Å–µ–º–µ–π": "–°–µ–º–µ–π",
    "–∞—Ç—ã—Ä–∞—É": "–ê—Ç—ã—Ä–∞—É",
    "–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "–∂–µ–∑“õ–∞–∑“ì–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "–∞–∫—Ç–∞—É": "–ê–∫—Ç–∞—É",
    "–æ–Ω–ª–∞–π–Ω": "–û–Ω–ª–∞–π–Ω",
    "online": "–û–Ω–ª–∞–π–Ω",
    "zoom": "–û–Ω–ª–∞–π–Ω (Zoom)",
    "—Ç–∞—à–∫–µ–Ω—Ç": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω",
}

EMOJI_RE = re.compile(
    "[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]",
    re.UNICODE,
)


# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def parse_date(text: str) -> Optional[datetime]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É. –ù–ï –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç +1 –≥–æ–¥ –∫ –ø—Ä–æ—à–µ–¥—à–∏–º –¥–∞—Ç–∞–º:
    –µ—Å–ª–∏ –≥–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω –∏ –¥–∞—Ç–∞ –ø—Ä–æ—à–ª–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
    """
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try:
            return datetime(year, month, day)
        except Exception:
            return None

    # –î–î-–î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month:
            return make_dt(year, month, int(m.group(2)))

    # –î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î –ú–µ—Å[—Å–æ–∫—Ä] [–ì–ì–ì–ì]
    m = re.search(
        r"(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?",
        t,
    )
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î.–ú–ú[.–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12:
            return make_dt(year, month, int(m.group(1)))

    return None


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: "—è–Ω–≤–∞—Ä—è",
        2: "—Ñ–µ–≤—Ä–∞–ª—è",
        3: "–º–∞—Ä—Ç–∞",
        4: "–∞–ø—Ä–µ–ª—è",
        5: "–º–∞—è",
        6: "–∏—é–Ω—è",
        7: "–∏—é–ª—è",
        8: "–∞–≤–≥—É—Å—Ç–∞",
        9: "—Å–µ–Ω—Ç—è–±—Ä—è",
        10: "–æ–∫—Ç—è–±—Ä—è",
        11: "–Ω–æ—è–±—Ä—è",
        12: "–¥–µ–∫–∞–±—Ä—è",
    }
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s


def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    known = [
        "Narxoz",
        "Nazarbayev",
        "KBTU",
        "–ö–ë–¢–£",
        "Astana Hub",
        "IT Park",
        "MOST IT Hub",
        "Holiday Inn",
        "Esentai",
        "Yandex",
        "Smart Point",
        "Almaty Arena",
    ]
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf"{re.escape(v)}[^\n,.]*", text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at = re.search(r"@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)", text)
    if at:
        return at.group(1).strip()[:60]
    return None


def is_real_event(text: str) -> bool:
    t = text.lower()
    return (any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS))


def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)


def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)


def dedup_title(title: str) -> str:
    """'Data Community BirthdayData Community Birthday' ‚Üí 'Data Community Birthday'"""
    for i in range(10, len(title) // 2 + 1):
        if title[i:].startswith(title[:i]):
            return title[:i].strip(" .,‚Äì-")
    return title


def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    # 16:00–û–Ω–ª–∞–π–Ω -> 16:00 –û–Ω–ª–∞–π–Ω
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-z–ê-–Ø–∞-—è–Å—ë])", r"\1 ", s)
    # "–§–µ–≤,16:00" -> "–§–µ–≤, 16:00"
    s = re.sub(r"([–∞-—è—ë–ê-–Ø–Å]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)

    # ‚úÖ –ì–û–†–û–î+–¢–ï–ö–°–¢ –±–µ–∑ –ø—Ä–æ–±–µ–ª–∞: "–ê–ª–º–∞—Ç—ãIT" -> "–ê–ª–º–∞—Ç—ã IT"
    city_tokens = sorted(set(KZ_CITIES.values()), key=len, reverse=True)
    for city in city_tokens:
        s = re.sub(rf"({re.escape(city)})(?=[A-Za-z–ê-–Ø–Å–∞-—è—ë])", r"\1 ", s)

    # –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r"\s{2,}", " ", s)
    return s


def strip_leading_city_from_title(title: str) -> str:
    """
    –£–±–∏—Ä–∞–µ—Ç –≥–æ—Ä–æ–¥/–æ–Ω–ª–∞–π–Ω –≤ –Ω–∞—á–∞–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞:
    "–ê–ª–º–∞—Ç—ã IT-—ç–∫–æ—Å–∏—Å—Ç–µ–º–∞" -> "IT-—ç–∫–æ—Å–∏—Å—Ç–µ–º–∞"
    "–ê—Ç—ã—Ä–∞—É –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ..." -> "–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ..."
    "–û–Ω–ª–∞–π–Ω –í–Ω–µ–¥—Ä–µ–Ω–∏–µ..." -> "–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..."
    """
    t = normalize_glued_text(strip_emoji(title)).strip()

    city_tokens = sorted(
        set(KZ_CITIES.values()) | {"–û–Ω–ª–∞–π–Ω", "Online", "online", "Zoom", "ZOOM", "zoom", "–û–Ω–ª–∞–π–Ω (Zoom)"},
        key=len,
        reverse=True,
    )

    for city in city_tokens:
        t = re.sub(
            rf"^\s*{re.escape(city)}\s*([\-‚Äì‚Äî:‚Ä¢,])?\s*",
            "",
            t,
            flags=re.IGNORECASE,
        )

    return t.strip(" -‚Äì‚Äî‚Ä¢,:.").strip()


def strip_leading_datetime_from_title(title: str) -> str:
    """
    –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π: –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏ ‚Äî —Å—Ä–µ–∑–∞–µ–º.
    –ü—Ä–∏–º–µ—Ä—ã:
    "24 –§–µ–≤, 16:00 –û–Ω–ª–∞–π–Ω –í–Ω–µ–¥—Ä–µ–Ω–∏–µ..." -> "–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..."
    "24 —Ñ–µ–≤—Ä–∞–ª—è 16:00–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..." -> "–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..."
    """
    t = strip_emoji(title).strip()

    # —É–±–∏—Ä–∞–µ–º –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è/–≥–æ—Ä–æ–¥ –≤–Ω—É—Ç—Ä–∏
    t = normalize_glued_text(t)

    # 24 —Ñ–µ–≤, 16:00 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[–ê-–Ø–∞-—è–Å—ëA-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24 —Ñ–µ–≤—Ä–∞–ª—è 2026 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[–∞-—è—ë]{3,}(?:\s+\d{4})?\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24.02.2026 ...
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)

    # ‚úÖ —É–±–∏—Ä–∞–µ–º –≥–æ—Ä–æ–¥/–æ–Ω–ª–∞–π–Ω –≤ –Ω–∞—á–∞–ª–µ (–µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è)
    t = strip_leading_city_from_title(t)

    return t.strip(" -‚Äì‚Ä¢.,").strip()


def clean_title_deterministic(raw_title: str) -> Optional[str]:
    s = strip_leading_datetime_from_title(raw_title)
    s = dedup_title(s)
    s = re.sub(r"\s{2,}", " ", s).strip()

    if len(s) < 5:
        return None
    if looks_like_description(s):
        return None

    # –æ–±—Ä–µ–∑–∞–µ–º —Ö–≤–æ—Å—Ç –ø–æ—Å–ª–µ –º–∞—Ä–∫–µ—Ä–æ–≤ –æ–ø–∏—Å–∞–Ω–∏—è
    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx != -1 and idx > 12:
            s = s[:idx].strip(" -‚Äì‚Ä¢.,")
            break

    s = s.strip()
    if len(s) < 5:
        return None
    return s[:120]


# ‚îÄ‚îÄ‚îÄ Parse glued line: "09 –§–µ–≤, 17:00–®—ã–º–∫–µ–Ω—Ç –ù–∞–∑–≤–∞–Ω–∏–µ" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_GLUE_RE = re.compile(
    r"^(\d{1,2})\s+"                      # day
    r"([–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,})"             # month
    r"[,\s]+"
    r"(\d{1,2}:\d{2})"                    # time
    r"\s*"
    r"(?:(–û–Ω–ª–∞–π–Ω|online|zoom|–ê–ª–º–∞—Ç—ã|–ê—Å—Ç–∞–Ω–∞|–®—ã–º–∫–µ–Ω—Ç|–ñ–µ–∑–∫–∞–∑–≥–∞–Ω|–ñ–µ–∑“õ–∞–∑“ì–∞–Ω|–ö–∞—Ä–∞–≥–∞–Ω–¥–∞|–ö–æ—Å—Ç–∞–Ω–∞–π|–ü–∞–≤–ª–æ–¥–∞—Ä|–°–µ–º–µ–π|–ê—Ç—ã—Ä–∞—É|–ê–∫—Ç–∞—É|–ê–∫—Ç–æ–±–µ|–¢–∞—Ä–∞–∑|–ö—ã–∑—ã–ª–æ—Ä–¥–∞)\s*)?"
    r"(.+)$",                             # title rest
    re.IGNORECASE,
)


def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m:
        return None

    day_s, month_s = m.group(1), m.group(2).lower()
    time_str = m.group(3)
    possible_city = (m.group(4) or "").strip()
    title_raw = m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    try:
        dt = datetime(datetime.now().year, month_num, int(day_s))
    except Exception:
        return None

    if not is_future(dt):
        return None

    city = None
    if possible_city:
        city = KZ_CITIES.get(possible_city.lower(), possible_city)

    title_raw = dedup_title(title_raw)

    if len(title_raw) < 5:
        return None

    return {
        "dt": dt,
        "time_str": time_str,
        "city": city,
        "title_raw": title_raw[:300],
        "date_formatted": format_date(dt, time_str),
    }


# ‚îÄ‚îÄ‚îÄ Formatting post (NO date in title) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def make_post(event: Dict) -> str:
    title = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get("location", "")
    venue = event.get("venue", "")

    # –≤–∞–∂–Ω–æ: –¥–∞—Ç–∞/–≥–æ—Ä–æ–¥ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —É–±–∏—Ä–∞–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞
    title = strip_leading_datetime_from_title(title)

    lines = [f"üéØ <b>{title}</b>"]

    if location in ("–û–Ω–ª–∞–π–Ω", "–û–Ω–ª–∞–π–Ω (Zoom)"):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue:
        lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {date_str}")
    lines.append(f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ Bot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = set()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={"User-Agent": "Mozilla/5.0"})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    # ‚îÄ‚îÄ Digest parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(
                r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?"
                r"|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*"
                r"(?:\s+\d{4})?)",
                line,
                re.IGNORECASE,
            )
            if not dm:
                i += 1
                continue

            date_raw = dm.group(0)
            rest = line[dm.end():].strip()

            tm = re.search(r"(?:–≤\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm:
                rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -‚Äì‚Ä¢")

            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"):
                    link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith("http"):
                            link = "https://" + link
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt):
                    title_raw = nxt

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ—à–µ–¥—à–µ–µ (–¥–∞–π–¥–∂–µ—Å—Ç): {title_raw[:40]}")
                i += 1
                continue

            ctx = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_location(ctx) or extract_location(text)

            title_clean = clean_title_deterministic(title_raw) or dedup_title(title_raw[:120])
            if not title_clean:
                i += 1
                continue

            events.append(
                {
                    "title": title_clean,
                    "date": format_date(dt, time_str),
                    "location": location or "",
                    "venue": extract_venue(ctx),
                    "link": link or post_link,
                    "source": source,
                    "image_url": image_url,
                }
            )
            i += 1
        return events

    # ‚îÄ‚îÄ Telegram channels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td:
                    continue

                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30:
                    continue

                le = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"

                if post_link in self.posted:
                    continue
                self.posted.add(post_link)

                # image
                image_url = None
                img_div = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if img_div:
                    sm = re.search(r"url\('([^']+)'\)", img_div.get("style", ""))
                    if sm:
                        image_url = sm.group(1)

                # digest?
                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    logger.info(f"üìã –î–∞–π–¥–∂–µ—Å—Ç {channel['name']}: {len(evs)}")
                    continue

                if not is_real_event(text):
                    continue

                # first non-empty line
                first_line = ""
                for ln in text.strip().split("\n"):
                    cl = strip_emoji(ln).strip()
                    if len(cl) > 10:
                        first_line = cl
                        break

                # glued detect (date + time and immediately letters)
                has_glue = bool(
                    re.search(r"\d{1,2}\s+[–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,}[,\s]+\d{1,2}:\d{2}[–ê-–Ø–ÅA-Za-z]", first_line)
                )

                if has_glue:
                    glued = parse_glued_line(first_line)
                    if not glued:
                        logger.info(f"‚è≠Ô∏è Glue: –ø—Ä–æ—à–µ–¥—à–∞—è/–Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è: {first_line[:60]}")
                        continue

                    title = clean_title_deterministic(glued["title_raw"])
                    if not title:
                        logger.info(f"‚è≠Ô∏è Glue: –ø–ª–æ—Ö–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {glued['title_raw'][:60]}")
                        continue

                    all_events.append(
                        {
                            "title": title,
                            "date": glued["date_formatted"],
                            "location": glued["city"] or extract_location(text) or "",
                            "venue": extract_venue(text),
                            "link": post_link,
                            "source": channel["name"],
                            "image_url": image_url,
                        }
                    )
                    continue

                # normal posts
                dt = parse_date(text)
                if not is_future(dt):
                    logger.info(f"‚è≠Ô∏è –ü—Ä–æ—à–µ–¥—à–µ–µ/–Ω–µ—Ç –¥–∞—Ç—ã: {text[:50].strip()}")
                    continue

                # heuristic title: first strong line
                title_candidate = None
                for ln in text.split("\n"):
                    ln = strip_emoji(ln).strip()
                    if len(ln) > 10 and not re.match(r"^\d{1,2}\s+[–∞-—è—ë]", ln.lower()):
                        title_candidate = ln
                        break

                title = clean_title_deterministic(title_candidate or "")
                if not title:
                    logger.info(f"‚è≠Ô∏è –ù–µ—Ç/–ø–ª–æ—Ö–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {text[:50].strip()}")
                    continue

                tm2 = re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append(
                    {
                        "title": title,
                        "date": format_date(dt, time_str),
                        "location": extract_location(text) or "",
                        "venue": extract_venue(text),
                        "link": post_link,
                        "source": channel["name"],
                        "image_url": image_url,
                    }
                )

            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue

        return all_events

    # ‚îÄ‚îÄ Sites ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site["url"])
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        events = []

        for link in soup.find_all("a", href=True)[:80]:
            try:
                href = link.get("href", "")
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue
                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)
                if href.rstrip("/") == site["url"].rstrip("/"):
                    continue
                if href in self.posted:
                    continue
                if is_site_trash(title_raw):
                    continue
                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt):
                    continue

                image_url = None
                img = link.find("img", src=True) or (parent.find("img", src=True) if parent else None)
                if img:
                    src = img.get("src", "")
                    if src and not src.startswith("http"):
                        from urllib.parse import urljoin
                        src = urljoin(site["url"], src)
                    image_url = src or None

                self.posted.add(href)

                title_clean = clean_title_deterministic(title_raw) or strip_emoji(dedup_title(title_raw))[:120]

                events.append(
                    {
                        "title": title_clean,
                        "date": format_date(dt),
                        "location": extract_location(context) or "",
                        "venue": extract_venue(context),
                        "link": href,
                        "source": site["name"],
                        "image_url": image_url,
                    }
                )

                if len(events) >= 5:
                    break
            except Exception:
                continue

        return events

    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)
            if evs:
                logger.info(f"‚úÖ {site['name']}: {len(evs)}")

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)
            if evs:
                logger.info(f"‚úÖ {ch['name']}: {len(evs)}")

        return all_events


# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")

        posted = 0
        for event in unique[:15]:
            text = make_post(event)
            if not text:
                continue
            try:
                if event.get("image_url"):
                    try:
                        await bot.send_photo(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            photo=event["image_url"],
                            caption=text,
                            parse_mode="HTML",
                        )
                    except Exception:
                        await bot.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )
                else:
                    await bot.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                posted += 1
                logger.info(f"‚úÖ ({posted}) {event.get('title','')[:50]}")
                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"‚ùå send error: {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted}")

    finally:
        await bot_obj.close()


if __name__ == "__main__":
    asyncio.run(main())
