import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path

import io
import numpy as np
import cv2
import pytesseract
from PIL import Image





STATE_DIR = Path("state")
POSTED_FILE = STATE_DIR / "load_posted.json"

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))




def strip_intro_phrases(text: str) -> str:
    patterns = [
        r"^–≤\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–∫–∞–∂–¥(—É—é|—ã–π|–æ–µ)?\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–≤\s+\w+\s+",  # –≤ —è–Ω–≤–∞—Ä–µ, –≤ –º–∞—Ä—Ç–µ –∏ —Ç.–ø.
        r"^–ø—Ä–∏–≥–ª–∞—à–∞–µ–º\s+",
        r"^—Å–æ—Å—Ç–æ–∏—Ç—Å—è\s+",
        r"^–±—É–¥–µ—Ç\s+",
    ]

    s = text.strip()

    for p in patterns:
        s = re.sub(p, "", s, flags=re.IGNORECASE)

    return s.strip(" -‚Äì‚Ä¢,")




def remove_city_from_title(title: str) -> str:
    for city_key in KZ_CITIES.keys():
        pattern = re.compile(rf"{city_key}", re.IGNORECASE)
        title = pattern.sub("", title)

    title = re.sub(r"\s{2,}", " ", title)
    return title.strip(" -‚Äì‚Ä¢,")





def fix_glued_words(text: str) -> str:
    # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ + –ª–∞—Ç–∏–Ω–∏—Ü–∞
    text = re.sub(r'([–∞-—è—ë])([A-Z])', r'\1 \2', text)
    text = re.sub(r'([a-z])([–ê-–Ø–Å])', r'\1 \2', text)
    text = re.sub(r'([–∞-—è—ë])([–ê-–Ø–Å])', r'\1 \2', text)
    return text



def extract_city_from_title(title: str) -> Optional[str]:
    lower = title.lower()
    for key, value in KZ_CITIES.items():
        if key in lower:
            return value
    return None



def is_clean_photo(url: str) -> bool:
    url = url.lower()

    blacklist = [
        "banner",
        "poster",
        "event",
        "flyer",
        "afisha",
        "1080x",
        "square",
        "card",
    ]

    return not any(word in url for word in blacklist)





def remove_weekday_from_start(text: str) -> str:
    lower = text.lower()

    for key in WEEK_DAYS.keys():
        if lower.startswith(key + " "):
            return text[len(key):].strip(" -‚Äì‚Ä¢, ")

        if lower.startswith("–≤ " + key + " "):
            return text[len("–≤ " + key):].strip(" -‚Äì‚Ä¢, ")

        if lower.startswith("–∫–∞–∂–¥—É—é " + key + " "):
            return text[len("–∫–∞–∂–¥—É—é " + key):].strip(" -‚Äì‚Ä¢, ")

    return text



def generate_universal_description(full_text: str, title: str) -> str:
    text = strip_emoji(full_text)
    text = normalize_glued_text(text)

    if title:
        text = re.sub(re.escape(title), "", text, flags=re.IGNORECASE)

    # —É–¥–∞–ª—è–µ–º —Å—Å—ã–ª–∫–∏
    text = re.sub(r"http\S+", "", text)

    paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 50]

    for p in paragraphs:

        low = p.lower()

    # UI –º—É—Å–æ—Ä
        if any(w in low for w in [
            "—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å", "telegram", "facebook",
            "whatsapp", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"
        ]):
            continue

    # üö´ –∞–¥—Ä–µ—Å
        if any(x in low for x in [
            "—É–ª.", "—É–ª–∏—Ü–∞", "–ø—Ä.", "–ø—Ä–æ—Å–ø–µ–∫—Ç",
            "—ç—Ç–∞–∂", "–æ—Ñ–∏—Å", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü",
            "–∑–¥–∞–Ω–∏–µ", "—Ä–∞–π–æ–Ω", "–æ—Å—Ç–∞–Ω–æ–≤"
        ]):
            continue

        # üö´ –¥–∞—Ç–∞/–≤—Ä–µ–º—è
        if re.search(r"\d{1,2}:\d{2}", p):
            continue

        if re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]+", p):
            continue

        words = p.split()
        if len(words) > 15:
            if len(words) > 30:
                return " ".join(words[:30]) + "..."
            return p

    return ""



def generate_fallback_description(title: str) -> str:
    t = title.lower()

    if "career" in t:
        return "–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤ –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –æ –≤—ã–±–æ—Ä–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –∫–∞—Ä—å–µ—Ä–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö."

    if "movie" in t:
        return "–ö–∏–Ω–æ-–≤—Å—Ç—Ä–µ—á–∞ —Å –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ IT."

    if "ai" in t or "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in t:
        return "–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ, –ø–æ—Å–≤—è—â—ë–Ω–Ω–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –≤ –±–∏–∑–Ω–µ—Å–µ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö."

    if "meetup" in t:
        return "–ù–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–∞—è –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±–º–µ–Ω–∞ –æ–ø—ã—Ç–æ–º –∏ –Ω–µ—Ç–≤–æ—Ä–∫–∏–Ω–≥–∞."

    if "—Ñ–æ—Ä—É–º" in t or "conference" in t:
        return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ —Å —É—á–∞—Å—Ç–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ç–µ–º."

    return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π."



def extract_program_block(full_text: str) -> str:
    text = strip_emoji(full_text)
    lines = text.split("\n")

    trigger_words = [
        "—á—Ç–æ —Ç–µ–±—è –∂–¥",
        "—á—Ç–æ –≤–∞—Å –∂–¥",
        "–≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ",
        "–ø—Ä–æ–≥—Ä–∞–º–º–∞",
        "–≤—ã —É–∑–Ω–∞–µ—Ç–µ",
    ]

    start_index = None

    for i, line in enumerate(lines):
        low = line.lower()
        if any(word in low for word in trigger_words):
            start_index = i
            break

    if start_index is None:
        return ""

    collected = []

    for line in lines[start_index + 1:]:
        clean = line.strip()

        if not clean:
            continue

        # —Å—Ç–æ–ø –µ—Å–ª–∏ –¥–æ—à–ª–∏ –¥–æ –¥–∞—Ç—ã / –∞–¥—Ä–µ—Å–∞
        if any(x in clean.lower() for x in ["üìç", "‚è∞", "http", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ"]):
            break

        if re.search(r"\d{1,2}:\d{2}", clean):
            break

        if len(clean) < 5:
            continue

        collected.append(clean)

        if len(collected) >= 4:
            break

    if not collected:
        return ""

    result = "\n".join(collected)

    words = result.split()
    if len(words) > 40:
        result = " ".join(words[:40]) + "..."

    return result.strip()




def smart_cut_title(text: str) -> str:
    """
    –û—Å—Ç–∞–≤–ª—è–µ—Ç –ª–æ–≥–∏—á–µ—Å–∫–∏ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—É—é —á–∞—Å—Ç—å —Ç–µ–∫—Å—Ç–∞:
    - —Ä–µ–∂–µ—Ç –ø–æ –ø–µ—Ä–≤–æ–º—É –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
    - —É–±–∏—Ä–∞–µ—Ç –¥–∞—Ç—É/–≤—Ä–µ–º—è
    - –Ω–µ –¥–∞—ë—Ç –∑–∞–≥–æ–ª–æ–≤–∫—É –±—ã—Ç—å —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–º
    """

    text = normalize_glued_text(text)

    # –£–±–∏—Ä–∞–µ–º –¥–∞—Ç—É –≤ –Ω–∞—á–∞–ª–µ
    text = strip_leading_datetime_from_title(text)

    # –£–±–∏—Ä–∞–µ–º –≤—Ä–µ–º—è 16:00
    text = re.sub(r"\b\d{1,2}:\d{2}\b", "", text)

    # –£–±–∏—Ä–∞–µ–º –¥–∞—Ç—ã –≤–∏–¥–∞ 26 —Ñ–µ–≤—Ä–∞–ª—è 2026
    text = re.sub(r"\b\d{1,2}\s+[–∞-—è—ë]+(?:\s+\d{4})?\b", "", text, flags=re.IGNORECASE)

    text = re.sub(r"\s{2,}", " ", text).strip()

    # üî• —Ä–µ–∂–µ–º –ø–æ –ø–µ—Ä–≤–æ–º—É –∑–∞–≤–µ—Ä—à—ë–Ω–Ω–æ–º—É –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—é
    sentence_match = re.match(r"(.+?[.!?])\s", text)
    if sentence_match:
        text = sentence_match.group(1)

    # –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    if len(text) > 160:
        text = text[:160].rsplit(" ", 1)[0] + "..."

    return text.strip(" -‚Äì‚Ä¢,")



































#       OCR

DATE_REGEX = re.compile(
    r"\b\d{1,2}[:.]\d{2}\b|"           # 16:00
    r"\b\d{1,2}\s*(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)\b|"
    r"\b\d{4}\b|"
    r"\b(january|february|march|april|may|june|july|august|september|october|november|december)\b",
    re.IGNORECASE
)


async def smart_crop_text_zones(session, image_url: str):
    try:
        async with session.get(image_url, timeout=20) as resp:
            if resp.status != 200:
                return None
            data = await resp.read()

        pil_img = Image.open(io.BytesIO(data)).convert("RGB")
        img = np.array(pil_img)

        h, w, _ = img.shape
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        ocr = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)

        crop_top = 0
        crop_bottom = h

        detected_top = []
        detected_bottom = []

        for i, text in enumerate(ocr["text"]):
            text = text.strip()
            if not text:
                continue

            if DATE_REGEX.search(text):
                y = ocr["top"][i]
                bh = ocr["height"][i]

                # –≤–µ—Ä—Ö–Ω—è—è –∑–æ–Ω–∞
                if y < h * 0.45:
                    detected_top.append(y + bh)

                # –Ω–∏–∂–Ω—è—è –∑–æ–Ω–∞
                if y > h * 0.55:
                    detected_bottom.append(y)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –≤—ã—á–∏—Å–ª—è–µ–º –∑–æ–Ω—ã –æ–±—Ä–µ–∑–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        if detected_top:
            crop_top = max(detected_top) + 30  # –∑–∞–ø–∞—Å –≤–Ω–∏–∑

        if detected_bottom:
            crop_bottom = min(detected_bottom) - 30  # –∑–∞–ø–∞—Å –≤–≤–µ—Ä—Ö

        # –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ–≥–æ –æ–±—Ä–µ–∑–∞–Ω–∏—è
        if crop_bottom - crop_top < h * 0.45:
            return None

        cropped = img[crop_top:crop_bottom, 0:w]

        # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –ø—É—Å—Ç–æ
        if cropped.size == 0:
            return None

        final_img = Image.fromarray(cropped)
        buffer = io.BytesIO()
        final_img.save(buffer, format="JPEG", quality=95)
        buffer.seek(0)

        return buffer

    except Exception as e:
        print("OCR crop error:", e)
        return None












def normalize_link(link: str) -> str:
    if not link:
        return ""

    link = link.strip()

    # —É–±–∏—Ä–∞–µ–º /s/ –∏–∑ telegram preview
    link = link.replace("https://t.me/s/", "https://t.me/")

    # —É–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ?...
    link = link.split("?")[0]

    # —É–±–∏—Ä–∞–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â–∏–π /
    link = link.rstrip("/")

    return link






def load_posted() -> set:
    if POSTED_FILE.exists():
        try:
            with open(POSTED_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()




def save_posted(posted: set):
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)

        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(list(posted), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è posted_links: {e}")



URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1,
    "—Ñ–µ–≤—Ä–∞–ª—è": 2,
    "–º–∞—Ä—Ç–∞": 3,
    "–∞–ø—Ä–µ–ª—è": 4,
    "–º–∞—è": 5,
    "–∏—é–Ω—è": 6,
    "–∏—é–ª—è": 7,
    "–∞–≤–≥—É—Å—Ç–∞": 8,
    "—Å–µ–Ω—Ç—è–±—Ä—è": 9,
    "–æ–∫—Ç—è–±—Ä—è": 10,
    "–Ω–æ—è–±—Ä—è": 11,
    "–¥–µ–∫–∞–±—Ä—è": 12,
}
MONTHS_SHORT = {
    "—è–Ω–≤": 1,
    "—Ñ–µ–≤": 2,
    "–º–∞—Ä": 3,
    "–∞–ø—Ä": 4,
    "–º–∞–π": 5,
    "–∏—é–Ω": 6,
    "–∏—é–ª": 7,
    "–∞–≤–≥": 8,
    "—Å–µ–Ω": 9,
    "–æ–∫—Ç": 10,
    "–Ω–æ—è": 11,
    "–¥–µ–∫": 12,
}

EVENT_WORDS = [
    "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è",
    "conference",
    "—Ñ–æ—Ä—É–º",
    "forum",
    "summit",
    "—Å–∞–º–º–∏—Ç",
    "meetup",
    "–º–∏—Ç–∞–ø",
    "—Ö–∞–∫–∞—Ç–æ–Ω",
    "hackathon",
    "–≤–æ—Ä–∫—à–æ–ø",
    "workshop",
    "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å",
    "masterclass",
    "–≤–µ–±–∏–Ω–∞—Ä",
    "webinar",
    "—Å–µ–º–∏–Ω–∞—Ä",
    "pitch",
    "–ø–∏—Ç—á",
    "demo day",
    "–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä",
    "accelerator",
    "bootcamp",
    "–±—É—Ç–∫–µ–º–ø",
    "–≤—ã—Å—Ç–∞–≤–∫–∞",
    "–∫–æ–Ω–∫—É—Ä—Å",
    "competition",
    "—Ç—Ä–µ–Ω–∏–Ω–≥",
    "training",
    "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ",
    "–∏–≤–µ–Ω—Ç",
    "event",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º",
    "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è",
    "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è",
]
NOT_EVENT_WORDS = [
    "research",
    "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª",
    "–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥",
    "–º–ª–Ω $",
    "–º–ª—Ä–¥ $",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω",
    "—É–≤–æ–ª–µ–Ω",
    "–æ—Ç—á–µ—Ç",
    "–≤—ã—Ä—É—á–∫–∞",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞",
    "–±–∏—Ä–∂–∞",
    "–∞–∫—Ü–∏–∏",
    "—Ç–æ–∫–∞–µ–≤",
    "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ",
]
SITE_STOP_WORDS = [
    "–∫–æ–Ω—Ç–∞–∫—Ç—ã",
    "–æ –Ω–∞—Å",
    "–ø–æ–ª–∏—Ç–∏–∫–∞",
    "–≤–æ–π—Ç–∏",
    "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞",
    "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è",
    "–ø–æ–∏—Å–∫",
    "–≥–ª–∞–≤–Ω–∞—è",
    "–º–µ–Ω—é",
    "–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏",
    "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ",
    "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ",
    "privacy",
    "terms",
    "cookie",
]
DESCRIPTION_SIGNALS = [
    "—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏",
    "–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤",
    "–≤—ã —É–∑–Ω–∞–µ—Ç–µ",
    "–º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º",
    "–Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏",
    "–≤ —Ä–∞–º–∫–∞—Ö",
    "—Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤—Å—Ç—Ä–µ—á–∞",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º –≤–∞—Å",
    "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Å—Å—ã–ª–∫–µ",
    "—Å–≤–æ–±–æ–¥–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ",
    "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç –≤–∞—Å –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ",
    "–≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–π—Ç–∏",
]

KZ_CITIES = {
    # –ê–ª–º–∞—Ç—ã
    "–∞–ª–º–∞—Ç—ã": "–ê–ª–º–∞—Ç—ã",
    "almaty": "–ê–ª–º–∞—Ç—ã",

    # –ê—Å—Ç–∞–Ω–∞
    "–∞—Å—Ç–∞–Ω–∞": "–ê—Å—Ç–∞–Ω–∞",
    "astana": "–ê—Å—Ç–∞–Ω–∞",
    "nur-sultan": "–ê—Å—Ç–∞–Ω–∞",
    "nursultan": "–ê—Å—Ç–∞–Ω–∞",
    "–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω": "–ê—Å—Ç–∞–Ω–∞",

    # –®—ã–º–∫–µ–Ω—Ç
    "—à—ã–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç",
    "shymkent": "–®—ã–º–∫–µ–Ω—Ç",

    # –£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫
    "—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",
    "ust-kamenogorsk": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",
    "oskemen": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",

    # –ö—ã–∑—ã–ª–æ—Ä–¥–∞
    "–∫—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞",
    "kyzylorda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞",

    # –ê–∫—Ç–æ–±–µ
    "–∞–∫—Ç–æ–±–µ": "–ê–∫—Ç–æ–±–µ",
    "aktobe": "–ê–∫—Ç–æ–±–µ",

    # –¢–∞—Ä–∞–∑
    "—Ç–∞—Ä–∞–∑": "–¢–∞—Ä–∞–∑",
    "taraz": "–¢–∞—Ä–∞–∑",

    # –ü–∞–≤–ª–æ–¥–∞—Ä
    "–ø–∞–≤–ª–æ–¥–∞—Ä": "–ü–∞–≤–ª–æ–¥–∞—Ä",
    "pavlodar": "–ü–∞–≤–ª–æ–¥–∞—Ä",

    # –ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫
    "–ø–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫",
    "petropavlovsk": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫",

    # –°–µ–º–µ–π
    "—Å–µ–º–µ–π": "–°–µ–º–µ–π",
    "semey": "–°–µ–º–µ–π",

    # –ê—Ç—ã—Ä–∞—É
    "–∞—Ç—ã—Ä–∞—É": "–ê—Ç—ã—Ä–∞—É",
    "atyrau": "–ê—Ç—ã—Ä–∞—É",

    # –ñ–µ–∑“õ–∞–∑“ì–∞–Ω
    "–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "–∂–µ–∑“õ–∞–∑“ì–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "zhezkazgan": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "jezqazgan": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",

    # –ê–∫—Ç–∞—É
    "–∞–∫—Ç–∞—É": "–ê–∫—Ç–∞—É",
    "aktau": "–ê–∫—Ç–∞—É",

    # –û–Ω–ª–∞–π–Ω
    "–æ–Ω–ª–∞–π–Ω": "–û–Ω–ª–∞–π–Ω",
    "online": "–û–Ω–ª–∞–π–Ω",
    "zoom": "–û–Ω–ª–∞–π–Ω (Zoom)",

    # –¢–∞—à–∫–µ–Ω—Ç
    "—Ç–∞—à–∫–µ–Ω—Ç": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω",
    "tashkent": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω",
}

WEEK_DAYS = {
    # –ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫
    "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫—É": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–æ–º": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "monday": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",

    # –í—Ç–æ—Ä–Ω–∏–∫
    "–≤—Ç–æ—Ä–Ω–∏–∫": "–í—Ç–æ—Ä–Ω–∏–∫",
    "–≤—Ç–æ—Ä–Ω–∏–∫–∞": "–í—Ç–æ—Ä–Ω–∏–∫",
    "–≤—Ç–æ—Ä–Ω–∏–∫—É": "–í—Ç–æ—Ä–Ω–∏–∫",
    "tuesday": "–í—Ç–æ—Ä–Ω–∏–∫",

    # –°—Ä–µ–¥–∞
    "—Å—Ä–µ–¥–∞": "–°—Ä–µ–¥–∞",
    "—Å—Ä–µ–¥—É": "–°—Ä–µ–¥–∞",
    "—Å—Ä–µ–¥—ã": "–°—Ä–µ–¥–∞",
    "—Å—Ä–µ–¥–µ": "–°—Ä–µ–¥–∞",
    "wednesday": "–°—Ä–µ–¥–∞",

    # –ß–µ—Ç–≤–µ—Ä–≥
    "—á–µ—Ç–≤–µ—Ä–≥": "–ß–µ—Ç–≤–µ—Ä–≥",
    "—á–µ—Ç–≤–µ—Ä–≥–∞": "–ß–µ—Ç–≤–µ—Ä–≥",
    "—á–µ—Ç–≤–µ—Ä–≥—É": "–ß–µ—Ç–≤–µ—Ä–≥",
    "thursday": "–ß–µ—Ç–≤–µ—Ä–≥",

    # –ü—è—Ç–Ω–∏—Ü–∞
    "–ø—è—Ç–Ω–∏—Ü–∞": "–ü—è—Ç–Ω–∏—Ü–∞",
    "–ø—è—Ç–Ω–∏—Ü—É": "–ü—è—Ç–Ω–∏—Ü–∞",
    "–ø—è—Ç–Ω–∏—Ü—ã": "–ü—è—Ç–Ω–∏—Ü–∞",
    "–ø—è—Ç–Ω–∏—Ü–µ": "–ü—è—Ç–Ω–∏—Ü–∞",
    "friday": "–ü—è—Ç–Ω–∏—Ü–∞",

    # –°—É–±–±–æ—Ç–∞
    "—Å—É–±–±–æ—Ç–∞": "–°—É–±–±–æ—Ç–∞",
    "—Å—É–±–±–æ—Ç—É": "–°—É–±–±–æ—Ç–∞",
    "—Å—É–±–±–æ—Ç—ã": "–°—É–±–±–æ—Ç–∞",
    "saturday": "–°—É–±–±–æ—Ç–∞",

    # –í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ
    "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
    "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—è": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
    "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—é": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
    "sunday": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
}

EMOJI_RE = re.compile(
    "[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]",
    re.UNICODE,
)


# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def parse_date(text: str) -> Optional[datetime]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É. –ù–ï –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç +1 –≥–æ–¥ –∫ –ø—Ä–æ—à–µ–¥—à–∏–º –¥–∞—Ç–∞–º:
    –µ—Å–ª–∏ –≥–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω –∏ –¥–∞—Ç–∞ –ø—Ä–æ—à–ª–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
    """
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try:
            return datetime(year, month, day)
        except Exception:
            return None

    # –î–î-–î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month:
            return make_dt(year, month, int(m.group(2)))

    # –î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î –ú–µ—Å[—Å–æ–∫—Ä] [–ì–ì–ì–ì]
    m = re.search(
        r"(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?",
        t,
    )
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î.–ú–ú[.–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12:
            return make_dt(year, month, int(m.group(1)))

    return None


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: "—è–Ω–≤–∞—Ä—è",
        2: "—Ñ–µ–≤—Ä–∞–ª—è",
        3: "–º–∞—Ä—Ç–∞",
        4: "–∞–ø—Ä–µ–ª—è",
        5: "–º–∞—è",
        6: "–∏—é–Ω—è",
        7: "–∏—é–ª—è",
        8: "–∞–≤–≥—É—Å—Ç–∞",
        9: "—Å–µ–Ω—Ç—è–±—Ä—è",
        10: "–æ–∫—Ç—è–±—Ä—è",
        11: "–Ω–æ—è–±—Ä—è",
        12: "–¥–µ–∫–∞–±—Ä—è",
    }
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s


def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    known = [
        "Narxoz",
        "Nazarbayev",
        "KBTU",
        "–ö–ë–¢–£",
        "Astana Hub",
        "IT Park",
        "MOST IT Hub",
        "Holiday Inn",
        "Esentai",
        "Yandex",
        "Smart Point",
        "Almaty Arena",
    ]
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf"{re.escape(v)}[^\n,.]*", text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at = re.search(r"@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)", text)
    if at:
        return at.group(1).strip()[:60]
    return None


def is_real_event(text: str) -> bool:
    t = text.lower()
    return (any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS))


def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)


def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)


def dedup_title(title: str) -> str:
    half = len(title) // 2
    for i in range(10, half):
        if title[i:].strip() == title[:len(title)-i].strip():
            return title[:len(title)-i].strip()
    return title


def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    # 16:00–û–Ω–ª–∞–π–Ω -> 16:00 –û–Ω–ª–∞–π–Ω
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-z–ê-–Ø–∞-—è–Å—ë])", r"\1 ", s)
    # "–§–µ–≤,16:00" -> "–§–µ–≤, 16:00"
    s = re.sub(r"([–∞-—è—ë–ê-–Ø–Å]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)
    # –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r"\s{2,}", " ", s)
    return s


def strip_leading_datetime_from_title(title: str) -> str:
    """
    –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π: –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏ ‚Äî —Å—Ä–µ–∑–∞–µ–º.
    –ü—Ä–∏–º–µ—Ä—ã:
    "24 –§–µ–≤, 16:00 –û–Ω–ª–∞–π–Ω –í–Ω–µ–¥—Ä–µ–Ω–∏–µ..." -> "–û–Ω–ª–∞–π–Ω –í–Ω–µ–¥—Ä–µ–Ω–∏–µ..."
    "24 —Ñ–µ–≤—Ä–∞–ª—è 16:00–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..." -> "–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..."
    """
    t = strip_emoji(title).strip()

    # —É–±–∏—Ä–∞–µ–º –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –≤–Ω—É—Ç—Ä–∏
    t = normalize_glued_text(t)

    # 24 —Ñ–µ–≤, 16:00 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[–ê-–Ø–∞-—è–Å—ëA-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24 —Ñ–µ–≤—Ä–∞–ª—è 2026 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[–∞-—è—ë]{3,}(?:\s+\d{4})?\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24.02.2026 ...
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)

    return t.strip(" -‚Äì‚Ä¢.,").strip()










def clean_title_deterministic(raw_title: str) -> Optional[str]:

    s = strip_leading_datetime_from_title(raw_title)
    s = remove_weekday_from_start(s)
    s = strip_intro_phrases(s)
    s = fix_glued_words(s)
    s = dedup_title(s)
    s = remove_city_from_title(s)

    # –æ–±—Ä–µ–∑–∞–µ–º —Ö–≤–æ—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è
    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx != -1 and idx > 12:
            s = s[:idx].strip(" -‚Äì‚Ä¢.,")
            break

    s = re.sub(r"\s{2,}", " ", s).strip()

    if len(s) < 5:
        return None

    if looks_like_description(s):
        return None

    return smart_cut_title(s)







# ‚îÄ‚îÄ‚îÄ Parse glued line: "09 –§–µ–≤, 17:00–®—ã–º–∫–µ–Ω—Ç –ù–∞–∑–≤–∞–Ω–∏–µ" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

city_pattern = "|".join([re.escape(v) for v in KZ_CITIES.values()])

_GLUE_RE = re.compile(
    rf"^(\d{{1,2}})\s+"
    rf"([–ê-–Ø–Å–∞-—è—ëA-Za-z]{{3,}})"
    rf"[,\\s]+"
    rf"(\d{{1,2}}:\d{{2}})"
    rf"\s*"
    rf"(?:(–û–Ω–ª–∞–π–Ω|online|zoom|{city_pattern})\s*)?"
    rf"(.+)$",
    re.IGNORECASE,
)


def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m:
        return None

    day_s, month_s = m.group(1), m.group(2).lower()
    time_str = m.group(3)
    possible_city = (m.group(4) or "").strip()
    title_raw = m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    try:
        dt = datetime(datetime.now().year, month_num, int(day_s))
    except Exception:
        return None

    if not is_future(dt):
        return None

    city = None
    if possible_city:
        city = KZ_CITIES.get(possible_city.lower(), possible_city)

    title_raw = dedup_title(title_raw)

    if len(title_raw) < 5:
        return None

    return {
        "dt": dt,
        "time_str": time_str,
        "city": city,
        "title_raw": title_raw[:300],
        "date_formatted": format_date(dt, time_str),
    }


# ‚îÄ‚îÄ‚îÄ Formatting post (NO date in title) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def make_post(event: Dict) -> str:
    title = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get("location", "")
    venue = event.get("venue", "")

    # –≤–∞–∂–Ω–æ: –¥–∞—Ç–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —É–±–∏—Ä–∞–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞
    title = strip_leading_datetime_from_title(title)

    lines = [f"üéØ <b>{title}</b>"]

# 1Ô∏è‚É£ –ø—Ä–æ–±—É–µ–º –≤—ã—Ç–∞—â–∏—Ç—å –±–ª–æ–∫ "–ß—Ç–æ —Ç–µ–±—è –∂–¥—ë—Ç"
    program_block = extract_program_block(event.get("full_text", ""))

    if program_block:
        description = program_block
    else:
        # 2Ô∏è‚É£ –ø—Ä–æ–±—É–µ–º –≤–∑—è—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π –∞–±–∑–∞—Ü
        description = generate_universal_description(
            event.get("full_text", ""),
            title
        )

    # 3Ô∏è‚É£ –µ—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–∏—á–µ–≥–æ –Ω–µ—Ç ‚Äî fallback
    if not description:
        description = generate_fallback_description(title)

    if description:
        lines.append("")
        lines.append(f"üìù {description}")



    if location in ("–û–Ω–ª–∞–π–Ω", "–û–Ω–ª–∞–π–Ω (Zoom)"):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue:
        lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {date_str}")
    lines.append(f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)















# ‚îÄ‚îÄ‚îÄ Bot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={"User-Agent": "Mozilla/5.0"})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""







    async def fetch_page_metadata(self, url: str) -> Dict:
        """
        –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π title –∏ description —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
        """

        html = await self.fetch(url)
        if not html:
            return {}

        soup = BeautifulSoup(html, "html.parser")

        result = {}

        # ‚îÄ‚îÄ title ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        og_title = soup.find("meta", property="og:title")
        if og_title and og_title.get("content"):
            result["title"] = og_title["content"].strip()

        if not result.get("title"):
            if soup.title and soup.title.string:
                result["title"] = soup.title.string.strip()

        # ‚îÄ‚îÄ description ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        og_desc = soup.find("meta", property="og:description")
        if og_desc and og_desc.get("content"):
            result["description"] = og_desc["content"].strip()

        if not result.get("description"):
            meta_desc = soup.find("meta", attrs={"name": "description"})
            if meta_desc and meta_desc.get("content"):
                result["description"] = meta_desc["content"].strip()

        return result
    # ‚îÄ‚îÄ Digest parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(
                r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?"
                r"|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*"
                r"(?:\s+\d{4})?)",
                line,
                re.IGNORECASE,
            )
            if not dm:
                i += 1
                continue

            date_raw = dm.group(0)
            rest = line[dm.end():].strip()

            tm = re.search(r"(?:–≤\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm:
                rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -‚Äì‚Ä¢")

            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"):
                    link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith("http"):
                            link = "https://" + link
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt):
                    title_raw = nxt

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ—à–µ–¥—à–µ–µ (–¥–∞–π–¥–∂–µ—Å—Ç): {title_raw[:40]}")
                i += 1
                continue

            ctx = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_location(ctx) or extract_location(text)

            title_clean = clean_title_deterministic(title_raw) or dedup_title(title_raw[:120])
            if not title_clean:
                i += 1
                continue

            events.append(
                {
                    "title": title_clean,
                    "date": format_date(dt, time_str),
                    "location": location or "",
                    "venue": extract_venue(ctx),
                    "link": link or post_link,
                    "source": source,
                    "image_url": image_url,
                }
            )
            i += 1
        return events

    # ‚îÄ‚îÄ Telegram channels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td:
                    continue

                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30:
                    continue

                le = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"
                norm_link = normalize_link(post_link)

# üî• –ò–©–ï–ú –ü–ï–†–í–û–ò–°–¢–û–ß–ù–ò–ö –í –¢–ï–ö–°–¢–ï
                external_link = None
                links_in_text = re.findall(r"(https?://[^\s]+)", text)

                for l in links_in_text:
                    clean_l = normalize_link(l)
                    if "t.me" not in clean_l:   # –∏—Å–∫–ª—é—á–∞–µ–º telegram
                        external_link = clean_l
                        break

# –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –≤–Ω–µ—à–Ω–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                final_link = external_link if external_link else norm_link

                # ‚ùó –ü–†–û–í–ï–†–ö–ê –¢–û–õ–¨–ö–û –ó–î–ï–°–¨
                if norm_link in self.posted:
                    continue
                
                # üî• –ò–©–ï–ú –ü–ï–†–í–û–ò–°–¢–û–ß–ù–ò–ö
                external_link = None

# 1Ô∏è‚É£ —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º <a href=...>
                for a in td.find_all("a", href=True):
                    href = normalize_link(a["href"])
                    if "t.me" not in href:
                        external_link = href
                        break

# 2Ô∏è‚É£ –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî fallback –Ω–∞ regex
                if not external_link:
                    links_in_text = re.findall(r"(https?://[^\s]+)", text)
                    for l in links_in_text:
                        clean_l = normalize_link(l)
                        if "t.me" not in clean_l:
                            external_link = clean_l
                            break

# 3Ô∏è‚É£ –≤—ã–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É
                final_link = external_link if external_link else norm_link

                image_url = None

# Telegram photo preview
                photo_wrap = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if photo_wrap:
                    style = photo_wrap.get("style", "")
                    match = re.search(r"url\('([^']+)'\)", style)
                    if match:
                        image_url = match.group(1)

# –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º img –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
                if not image_url:
                    img_tag = td.find("img")
                    if img_tag and img_tag.get("src"):
                        image_url = img_tag["src"]

                # –î–∞–π–¥–∂–µ—Å—Ç
                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    continue

                if not is_real_event(text):
                    continue

                dt = parse_date(text)
                if not is_future(dt):
                    continue

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_candidate = None
                for ln in text.split("\n"):
                    ln = strip_emoji(ln).strip()
                    if len(ln) > 10:
                        title_candidate = ln
                        break

                raw_title = title_candidate or ""

                city_from_title = extract_city_from_title(raw_title)

                title = clean_title_deterministic(raw_title)
                if not title:
                    continue

                tm2 = re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None
                    # üî• –ï—Å–ª–∏ –µ—Å—Ç—å –≤–Ω–µ—à–Ω–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
                if external_link:
                    meta = await self.fetch_page_metadata(external_link)

                    if meta.get("title") and len(meta["title"]) > 10:
                        title = clean_title_deterministic(meta["title"])

                    if meta.get("description"):
                        full_text = meta["description"]
                    else:
                        full_text = text
                else:
                    full_text = text
                    all_events.append(
                    {
                        "title": title,
                        "date": format_date(dt, time_str),
                        "location": extract_location(text) or city_from_title or "",
                        "venue": extract_venue(text),
                        "link": final_link,
                        "source": channel["name"],
                        "full_text": full_text,
                        "image_url": image_url,
                    }
                )

            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue

        return all_events
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)

        return all_events




   

    
    # ‚îÄ‚îÄ Sites ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_site(self, site: Dict) -> List[Dict]:

        html = await self.fetch(site["url"])
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        events = []

        for link in soup.find_all("a", href=True)[:80]:
            try:
                href = link.get("href", "")
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue

                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)

                # üî• –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏
                href = normalize_link(href)

                if href.rstrip("/") == normalize_link(site["url"]).rstrip("/"):
                    continue

                # üî• –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª—è —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Å—Å—ã–ª–∫—É
                if href in self.posted:
                    continue

                if is_site_trash(title_raw):
                    continue

                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt):
                    continue

                image_url = None


# –æ–±—ã—á–Ω—ã–π img
                image_url = None

                if parent:
                    imgs = parent.find_all("img")

                    for img in imgs:
                        src = img.get("src") or img.get("data-src")
                        if not src:
                            continue

                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)

                        # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ—Å—Ç–µ—Ä—ã
                        if is_clean_photo(src):
                            image_url = src
                            break

                # fallback ‚Äî background-image
                if not image_url and parent:
                    style = parent.get("style", "")
                    match = re.search(r"url\(['\"]?([^'\")]+)", style)

                    if match:
                        src = match.group(1)

                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)

                        if is_clean_photo(src):
                            image_url = src


                title_clean = (
                    clean_title_deterministic(title_raw)
                    or strip_emoji(dedup_title(title_raw))[:120]
                )

                events.append(
                    {
                        "title": title_clean,
                        "date": format_date(dt),
                        "location": extract_location(context) or "",
                        "venue": extract_venue(context),
                        "link": href,
                        "full_text": context,
                        "source": site["name"],
                        "image_url": image_url,
                    }
                )

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events
        
        




    






# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")

    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot_api = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")
        logger.info(f"üì¶ –£–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {len(bot_obj.posted)}")

        posted = 0

        for event in unique[:15]:

            norm_link = normalize_link(event.get("link", ""))

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if norm_link in bot_obj.posted:
                logger.info(f"‚è≠Ô∏è –£–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ—Å—å: {event.get('title')[:50]}")
                continue

            text = make_post(event)
            if not text:
                continue

            try:
                photo_to_send = None

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OCR –∞–≤—Ç–æ-–æ–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if event.get("image_url"):
                    session = await bot_obj.get_session()
                    photo_to_send = await smart_crop_text_zones(
                        session,
                        event["image_url"]
                    )

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –û—Ç–ø—Ä–∞–≤–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if photo_to_send:
                    await bot_api.send_photo(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        photo=photo_to_send,
                        caption=text,
                        parse_mode="HTML",
                    )
                else:
                    await bot_api.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ state —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                bot_obj.posted.add(norm_link)
                save_posted(bot_obj.posted)

                posted += 1
                logger.info(f"‚úÖ ({posted}) {event.get('title','')[:50]}")

                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –Ω–æ–≤—ã—Ö: {posted}")

    finally:
        await bot_obj.close()


if __name__ == "__main__":
    asyncio.run(main())
