import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path


STATE_DIR = Path("state")
POSTED_FILE = STATE_DIR / "load_posted.json"

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))

def strip_intro_phrases(text: str) -> str:
    patterns = [
        r"^–≤\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–∫–∞–∂–¥(—É—é|—ã–π|–æ–µ)?\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–≤\s+\w+\s+",  
        r"^–ø—Ä–∏–≥–ª–∞—à–∞–µ–º\s+",
        r"^—Å–æ—Å—Ç–æ–∏—Ç—Å—è\s+",
        r"^–±—É–¥–µ—Ç\s+",
    ]
    s = text.strip()
    for p in patterns:
        s = re.sub(p, "", s, flags=re.IGNORECASE)
    return s.strip(" -‚Äì‚Ä¢,")

def remove_city_from_title(title: str) -> str:
    for city_key in KZ_CITIES.keys():
        title = re.sub(rf"^[^\w–∞-—è–ê-–Ø—ë–Åa-zA-Z]*{city_key}\b", "", title, flags=re.IGNORECASE)
        title = re.sub(rf"\b{city_key}\b", "", title, flags=re.IGNORECASE)
    title = re.sub(r"\s{2,}", " ", title)
    title = re.sub(r"^[,\-\s‚Ä¢:!]+", "", title) 
    return title.strip(" -‚Äì‚Ä¢:,!")

def fix_glued_words(text: str) -> str:
    text = re.sub(r'(\d{1,2}:\d{2})([–ê-–Ø–∞-—è–Å—ëA-Za-z])', r'\1 \2', text)
    text = re.sub(r'([–ê-–Ø–∞-—è–Å—ëA-Za-z])(\d{1,2}:\d{2})', r'\1 \2', text)
    text = re.sub(r'([!?,.])([–ê-–Ø–∞-—è–Å—ëA-Za-z])', r'\1 \2', text)
    text = re.sub(r'([–∞-—è—ë–ê-–Ø–Å])([A-Za-z])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z])([–∞-—è—ë–ê-–Ø–Å])', r'\1 \2', text)
    text = re.sub(r'([–∞-—è—ë])([–ê-–Ø–Å])', r'\1 \2', text)
    text = re.sub(r'\b(–≤|–Ω–∞|–≤–æ)\s+\1\b', r'\1', text, flags=re.IGNORECASE)
    return text

def extract_city_from_title(title: str) -> Optional[str]:
    lower = title.lower()
    for key, value in KZ_CITIES.items():
        if key in lower:
            return value
    return None

def is_clean_photo(url: str) -> bool:
    url = url.lower()
    blacklist = [
        "icon", "logo", "avatar", "svg", "button", "background", "footer"
    ]
    return not any(word in url for word in blacklist)

def remove_weekday_from_start(text: str) -> str:
    lower = text.lower()
    for key in WEEK_DAYS.keys():
        if lower.startswith(key + " "):
            return text[len(key):].strip(" -‚Äì‚Ä¢, ")
        if lower.startswith("–≤ " + key + " "):
            return text[len("–≤ " + key):].strip(" -‚Äì‚Ä¢, ")
        if lower.startswith("–∫–∞–∂–¥—É—é " + key + " "):
            return text[len("–∫–∞–∂–¥—É—é " + key):].strip(" -‚Äì‚Ä¢, ")
    return text

def generate_universal_description(full_text: str, title: str) -> str:
    text = strip_emoji(full_text)
    text = normalize_glued_text(text)

    if title:
        text = re.sub(re.escape(title), "", text, flags=re.IGNORECASE)

    text = re.sub(r"http\S+", "", text)
    text = re.sub(r'\b–≤\s+–≤\b', '–≤', text, flags=re.IGNORECASE)
    
    paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 50]

    for p in paragraphs:
        low = p.lower()
        if any(w in low for w in ["—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å", "telegram", "facebook", "whatsapp", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"]):
            continue
        if any(x in low for x in ["—É–ª.", "—É–ª–∏—Ü–∞", "–ø—Ä.", "–ø—Ä–æ—Å–ø–µ–∫—Ç", "—ç—Ç–∞–∂", "–æ—Ñ–∏—Å", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü", "–∑–¥–∞–Ω–∏–µ", "—Ä–∞–π–æ–Ω", "–æ—Å—Ç–∞–Ω–æ–≤"]):
            continue
        if re.search(r"\d{1,2}:\d{2}", p):
            continue
        if re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]+", p):
            continue

        words = p.split()
        if len(words) > 12:  
            if len(words) > 40:
                return " ".join(words[:40]) + "..."
            return p
    return ""

def generate_fallback_description(title: str) -> str:
    t = title.lower()
    if "career" in t: return "–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤ –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –æ –≤—ã–±–æ—Ä–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –∫–∞—Ä—å–µ—Ä–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö."
    if "movie" in t: return "–ö–∏–Ω–æ-–≤—Å—Ç—Ä–µ—á–∞ —Å –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ IT."
    if "ai" in t or "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in t: return "–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ, –ø–æ—Å–≤—è—â—ë–Ω–Ω–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –≤ –±–∏–∑–Ω–µ—Å–µ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö."
    if "meetup" in t: return "–ù–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–∞—è –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±–º–µ–Ω–∞ –æ–ø—ã—Ç–æ–º –∏ –Ω–µ—Ç–≤–æ—Ä–∫–∏–Ω–≥–∞."
    if "—Ñ–æ—Ä—É–º" in t or "conference" in t: return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ —Å —É—á–∞—Å—Ç–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ç–µ–º."
    if "battle" in t or "–±–∞—Ç–ª" in t: return "–°–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—Ç–∞–ø–æ–≤ —Å –ø–∏—Ç—á–∏–Ω–≥–æ–º –ø–µ—Ä–µ–¥ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–º –∂—é—Ä–∏ –∏ –ø—Ä–∏–∑–æ–≤—ã–º —Ñ–æ–Ω–¥–æ–º."
    if "—Ö–∞–∫–∞—Ç–æ–Ω" in t or "hackathon" in t: return "–ò–Ω—Ç–µ–Ω—Å–∏–≤–Ω–æ–µ —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–µ –ø–æ —Å–æ–∑–¥–∞–Ω–∏—é IT-–ø—Ä–æ–¥—É–∫—Ç–æ–≤ –≤ —Å–∂–∞—Ç—ã–µ —Å—Ä–æ–∫–∏."
    if "–ø–∏—Ç—á" in t or "pitch" in t: return "–ü–∏—Ç—á-—Å–µ—Å—Å–∏—è —Å –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–µ–π –ø—Ä–æ–µ–∫—Ç–æ–≤ –ø–µ—Ä–µ–¥ –∏–Ω–≤–µ—Å—Ç–æ—Ä–∞–º–∏ –∏ —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏."
    return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π."

def extract_program_block(full_text: str) -> str:
    text = strip_emoji(full_text)
    lines = text.split("\n")
    trigger_words = ["—á—Ç–æ —Ç–µ–±—è –∂–¥", "—á—Ç–æ –≤–∞—Å –∂–¥", "–≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ", "–ø—Ä–æ–≥—Ä–∞–º–º–∞", "–≤—ã —É–∑–Ω–∞–µ—Ç–µ"]
    start_index = None

    for i, line in enumerate(lines):
        low = line.lower()
        if any(word in low for word in trigger_words):
            start_index = i
            break

    if start_index is None:
        return ""

    collected = []
    for line in lines[start_index + 1:]:
        clean = line.strip()
        if not clean:
            continue
        if any(x in clean.lower() for x in ["üìç", "‚è∞", "http", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ"]):
            break
        if re.search(r"\d{1,2}:\d{2}", clean):
            break
        if len(clean) < 5:
            continue
        collected.append(clean)
        if len(collected) >= 4:
            break

    if not collected:
        return ""

    result = "\n".join(collected)
    words = result.split()
    if len(words) > 40:
        result = " ".join(words[:40]) + "..."
    return result.strip()

def normalize_link(link: str) -> str:
    if not link: return ""
    link = link.strip()
    link = link.replace("https://t.me/s/", "https://t.me/")
    link = link.split("?")[0]
    link = link.rstrip("/")
    return link

def load_posted() -> set:
    if POSTED_FILE.exists():
        try:
            with open(POSTED_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_posted(posted: set):
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)
        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(list(posted), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è posted_links: {e}")

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {"—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4, "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8, "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12}
MONTHS_SHORT = {"—è–Ω–≤": 1, "—Ñ–µ–≤": 2, "–º–∞—Ä": 3, "–∞–ø—Ä": 4, "–º–∞–π": 5, "–∏—é–Ω": 6, "–∏—é–ª": 7, "–∞–≤–≥": 8, "—Å–µ–Ω": 9, "–æ–∫—Ç": 10, "–Ω–æ—è": 11, "–¥–µ–∫": 12}

EVENT_WORDS = [
    "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è", "conference", "—Ñ–æ—Ä—É–º", "forum", "summit", "—Å–∞–º–º–∏—Ç", "meetup", "–º–∏—Ç–∞–ø",
    "—Ö–∞–∫–∞—Ç–æ–Ω", "hackathon", "–≤–æ—Ä–∫—à–æ–ø", "workshop", "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å", "masterclass", "–≤–µ–±–∏–Ω–∞—Ä",
    "webinar", "—Å–µ–º–∏–Ω–∞—Ä", "pitch", "–ø–∏—Ç—á", "demo day", "–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä", "accelerator",
    "bootcamp", "–±—É—Ç–∫–µ–º–ø", "–≤—ã—Å—Ç–∞–≤–∫–∞", "–∫–æ–Ω–∫—É—Ä—Å", "competition", "—Ç—Ä–µ–Ω–∏–Ω–≥", "training",
    "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ", "–∏–≤–µ–Ω—Ç", "event", "–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç", "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º", "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è",
    "battle", "–±–∞—Ç–ª",
]

NOT_EVENT_WORDS = [
    "research", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ", "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª", "–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥", "–º–ª–Ω $", "–º–ª—Ä–¥ $",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–±–∏—Ä–∂–∞", "–∞–∫—Ü–∏–∏", "—Ç–æ–∫–∞–µ–≤", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–Ω–∞–∑–Ω–∞—á–µ–Ω", "—É–≤–æ–ª–µ–Ω", "–≤—ã—Ä—É—á–∫–∞",
    "–≥–æ—Å–¥–æ—Ö–æ–¥", "–∑–∞–Ω—è—Ç–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è", "–∏–Ω—Å–ø–µ–∫—Ç–æ—Ä", "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö", "–≥–æ—Å–æ—Ä–≥–∞–Ω", 
    "–∞–∫–∏–º–∞—Ç", "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤", "–Ω–∞–ª–æ–≥–æ–≤", "–±—É—Ö–≥–∞–ª—Ç–µ—Ä", "–∫–∞–¥—Ä–æ–≤–æ–π —Å–ª—É–∂–±", "–ø–∞–ª–∞—Ç –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π", 
    "–∞—Ç–∞–º–µ–∫–µ–Ω", "–≥–æ—Å–∑–∞–∫—É–ø", "—Å—É–±—Å–∏–¥–∏", "—Å–µ–ª—å—Å–∫–æ–≥–æ —Ö–æ–∑—è–π—Å—Ç–≤", "–∑–∫–æ", "–≤–∫–æ", "—é–∫–æ", "—Å“õ–æ",
    "–ø–µ–Ω—Å–∏–æ–Ω–Ω", "–∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç", "–¥–µ–ø—É—Ç–∞—Ç", "–º–∞—Å–ª–∏—Ö–∞—Ç", "–º–∞–∂–∏–ª–∏—Å", "–Ω–∞–ª–æ–≥–æ–æ–±–ª–æ–∂–µ–Ω", "—à—Ç—Ä–∞—Ñ", 
    "–ø—Ä–æ–≤–µ—Ä–∫", "–∏–Ω—Å–ø–µ–∫—Ü–∏", "—Å–¥–∞—á–∞ –æ—Ç—á–µ—Ç", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ä–∞–∑—ä—è—Å–Ω–∏—Ç–µ–ª—å–Ω–∞—è",
    "–∫–¥–ø", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "—Ä–µ–≥–ª–∞–º–µ–Ω—Ç", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä", 
    "–∫–æ–º–ø–ª–∞–µ–Ω—Å", "compliance", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤", "–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞", "—Ç–µ—Ö–Ω–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    "—Å–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã", "–º–∞–Ω–∏–∫—é—Ä", "–ø–æ–≤–∞—Ä", "–∫—É–ª–∏–Ω–∞—Ä", "—à–≤–µ—è", "—Ä–µ–º–æ–Ω—Ç", "—Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫",
    "–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥", "—É—Ç—Ä–µ–Ω–Ω–∏–∫", "—à–∫–æ–ª—å–Ω–∞—è —è—Ä–º–∞—Ä–∫–∞",
    "machine learning", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é", "–º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
    "python", "javascript", "c++", "c#", "php", "golang", "ruby", "swift", "kotlin", "java",
    "html", "css", "—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥", "–±—ç–∫–µ–Ω–¥", "frontend", "backend",
    "—Å—Ç–∞—Ä—Ç –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "—è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "—è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", 
    "–Ω–∞—É—á–∏—Ç—å—Å—è –∫–æ–¥–∏—Ç—å", "—Å—Ç–∞–Ω—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º", "–ø—Ä–æ—Ñ–µ—Å—Å–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫", "–≤–æ–π—Ç–∏ –≤ it",
    "—Å –Ω—É–ª—è –¥–æ", "–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "–±–∞–∑–æ–≤—ã–π –∫—É—Ä—Å", "–¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤",
    "ai-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç", "ai-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏", "–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã ai", "chatgpt", "midjourney", 
    "—Ü–∏—Ñ—Ä–æ–≤–æ–π –ø–æ–º–æ—â–Ω–∏–∫", "—Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π", "–¥–ª—è –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è", "–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ai", 
    "–∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç–∏", "–Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –¥–ª—è", "–Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –¥–ª—è", "ai –¥–ª—è —Ä–∞–±–æ—Ç—ã", 
    "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç –∫–∞–∫"
]

SITE_STOP_WORDS = [
    "–∫–æ–Ω—Ç–∞–∫—Ç—ã", "–æ –Ω–∞—Å", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–≤–æ–π—Ç–∏", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞", "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è",
    "–ø–æ–∏—Å–∫", "–≥–ª–∞–≤–Ω–∞—è", "–º–µ–Ω—é", "–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ", "privacy", "terms", "cookie"
]

DESCRIPTION_SIGNALS = [
    "—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏", "–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤", "–≤—ã —É–∑–Ω–∞–µ—Ç–µ", "–º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º", "–Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏",
    "–≤ —Ä–∞–º–∫–∞—Ö", "—Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤—Å—Ç—Ä–µ—á–∞", "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º –≤–∞—Å", "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Å—Å—ã–ª–∫–µ",
    "—Å–≤–æ–±–æ–¥–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ", "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç –≤–∞—Å –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ", "–≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–π—Ç–∏", "–ø–æ–≥–æ–≤–æ—Ä–∏–º –æ", "–æ–±—Å—É–¥–∏–º", "—Ä–∞–∑–±–µ—Ä–µ–º" 
]

# üî• –°–ª–æ–≤–∞-–º–∞—Ä–∫–µ—Ä—ã –º—É—Å–æ—Ä–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–Ω–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è)
GARBAGE_DESCRIPTION_MARKERS = [
    # 2–ì–ò–°, –Ø–Ω–¥–µ–∫—Å.–ö–∞—Ä—Ç—ã –∏ –¥—Ä—É–≥–∏–µ –∞–≥—Ä–µ–≥–∞—Ç–æ—Ä—ã
    "–∞–¥—Ä–µ—Å–∞ —Å–æ –≤—Ö–æ–¥–∞–º–∏", "–æ—Ç–∑—ã–≤—ã, —Ñ–æ—Ç–æ", "–Ω–æ–º–µ—Ä–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤", "–≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã –∏ –∫–∞–∫ –¥–æ–µ—Ö–∞—Ç—å",
    "–ø—Ä–µ–º–∏—è 2–≥–∏—Å", "2–≥–∏—Å", "—è–Ω–¥–µ–∫—Å.–∫–∞—Ä—Ç—ã", "google maps",
    # –ú–µ–Ω—é —Ä–µ—Å—Ç–æ—Ä–∞–Ω–æ–≤/–∫–∞—Ñ–µ
    "—á–µ–∫ ", "–∑–∞–∫–∞–∑ –Ω–∞–≤—ã–Ω–æ—Å", "–ª–µ—Ç–Ω—è—è –≤–µ—Ä–∞–Ω–¥–∞", "–º–æ–∂–Ω–æ —Å –Ω–æ—É—Ç–±—É–∫–æ–º", "–¥–æ 30 –º–µ—Å—Ç",
    "–ø–æ–Ω—á–∏–∫–∏", "—Å—ç–Ω–¥–≤–∏—á–∏", "–∫–æ—Ñ–µ —Å —Å–æ–±–æ–π", "—á–∞–π —Å —Å–æ–±–æ–π", "—Å–º—É–∑–∏", "–º–æ–ª–æ—á–Ω—ã–µ –∫–æ–∫—Ç–µ–π–ª–∏",
    "—Å–≤–µ–∂–µ–≤—ã–∂–∞—Ç—ã–µ —Å–æ–∫–∏", "–≥–æ—Ä—è—á–∏–π —à–æ–∫–æ–ª–∞–¥", "–∫–∞–∫–∞–æ", "–ª–∏–º–æ–Ω–∞–¥", "–º–∞—Ç—á–∞", "–∞–π—Å-–∫–æ—Ñ–µ",
    # –ù–∞–≤–∏–≥–∞—Ü–∏—è —Å–∞–π—Ç–æ–≤
    "cookie", "–≤–æ–π—Ç–∏", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞", "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è", "privacy policy",
    "terms of service", "–≤—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã", "¬©",
    # SEO-–º—É—Å–æ—Ä
    "–¥–æ—Å—Ç—É–ø–Ω—ã–π –≤—Ö–æ–¥ –¥–ª—è –ª—é–¥–µ–π", "–¥–æ—Å—Ç–∞–≤–∫–∞", "—Å–∞–º–æ–≤—ã–≤–æ–∑", "–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –ø–∞—Ä–∫–æ–≤–∫–∞",
    "wi-fi", "–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä", "–¥–µ—Ç—Å–∫–∞—è –∫–æ–º–Ω–∞—Ç–∞",
    # –û–±—â–∏–π –º—É—Å–æ—Ä —Å –∫–∞—Ä—Ç–æ—á–µ–∫ –º–µ—Å—Ç
    "—Ä–µ–π—Ç–∏–Ω–≥", "–æ—Ü–µ–Ω–∫–∞", "–æ—Ç–∑—ã–≤", "—Ñ–æ—Ç–æ–≥—Ä–∞—Ñ–∏", "–ø–∞–Ω–æ—Ä–∞–º",
]

KZ_CITIES = {
    "–∞–ª–º–∞—Ç—ã": "–ê–ª–º–∞—Ç—ã", "almaty": "–ê–ª–º–∞—Ç—ã",
    "–∞—Å—Ç–∞–Ω–∞": "–ê—Å—Ç–∞–Ω–∞", "astana": "–ê—Å—Ç–∞–Ω–∞", "nur-sultan": "–ê—Å—Ç–∞–Ω–∞", "nursultan": "–ê—Å—Ç–∞–Ω–∞", "–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω": "–ê—Å—Ç–∞–Ω–∞", "–∞“õ–º–æ–ª–∞": "–ê—Å—Ç–∞–Ω–∞", "aqmola": "–ê—Å—Ç–∞–Ω–∞", "—Ü–µ–ª–∏–Ω–æ–≥—Ä–∞–¥": "–ê—Å—Ç–∞–Ω–∞", "tselinograd": "–ê—Å—Ç–∞–Ω–∞",
    "—à—ã–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç", "shymkent": "–®—ã–º–∫–µ–Ω—Ç", "—á–∏–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç", "chimkent": "–®—ã–º–∫–µ–Ω—Ç",
    "–∫–∞—Ä–∞–≥–∞–Ω–¥–∞": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "karaganda": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "“õ–∞—Ä–∞“ì–∞–Ω–¥—ã": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "qaraghandy": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "qaragandi": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞",
    "–∞–∫—Ç–æ–±–µ": "–ê–∫—Ç–æ–±–µ", "aktobe": "–ê–∫—Ç–æ–±–µ", "–∞“õ—Ç”©–±–µ": "–ê–∫—Ç–æ–±–µ", "aqtobe": "–ê–∫—Ç–æ–±–µ", "–∞–∫—Ç—é–±–∏–Ω—Å–∫": "–ê–∫—Ç–æ–±–µ", "aktyubinsk": "–ê–∫—Ç–æ–±–µ",
    "—Ç–∞—Ä–∞–∑": "–¢–∞—Ä–∞–∑", "taraz": "–¢–∞—Ä–∞–∑", "–∂–∞–º–±—ã–ª": "–¢–∞—Ä–∞–∑", "zhambyl": "–¢–∞—Ä–∞–∑", "–¥–∂–∞–º–±—É–ª": "–¢–∞—Ä–∞–∑", "djambul": "–¢–∞—Ä–∞–∑", "jambul": "–¢–∞—Ä–∞–∑",
    "–ø–∞–≤–ª–æ–¥–∞—Ä": "–ü–∞–≤–ª–æ–¥–∞—Ä", "pavlodar": "–ü–∞–≤–ª–æ–¥–∞—Ä",
    "—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "ust-kamenogorsk": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "”©—Å–∫–µ–º–µ–Ω": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "oskemen": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "ust-kamen": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",
    "—Å–µ–º–µ–π": "–°–µ–º–µ–π", "semey": "–°–µ–º–µ–π", "semei": "–°–µ–º–µ–π", "—Å–µ–º–∏–ø–∞–ª–∞—Ç–∏–Ω—Å–∫": "–°–µ–º–µ–π", "semipalatinsk": "–°–µ–º–µ–π",
    "–∞—Ç—ã—Ä–∞—É": "–ê—Ç—ã—Ä–∞—É", "atyrau": "–ê—Ç—ã—Ä–∞—É", "–≥—É—Ä—å–µ–≤": "–ê—Ç—ã—Ä–∞—É", "guriev": "–ê—Ç—ã—Ä–∞—É",
    "–∫–æ—Å—Ç–∞–Ω–∞–π": "–ö–æ—Å—Ç–∞–Ω–∞–π", "kostanay": "–ö–æ—Å—Ç–∞–Ω–∞–π", "“õ–æ—Å—Ç–∞–Ω–∞–π": "–ö–æ—Å—Ç–∞–Ω–∞–π", "qostanai": "–ö–æ—Å—Ç–∞–Ω–∞–π", "qostanay": "–ö–æ—Å—Ç–∞–Ω–∞–π", "–∫—É—Å—Ç–∞–Ω–∞–π": "–ö–æ—Å—Ç–∞–Ω–∞–π", "kustanai": "–ö–æ—Å—Ç–∞–Ω–∞–π", "kustanay": "–ö–æ—Å—Ç–∞–Ω–∞–π",
    "–∫—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "kyzylorda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "“õ—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "qyzylorda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "kzyil-orda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞",
    "—É—Ä–∞–ª—å—Å–∫": "–£—Ä–∞–ª—å—Å–∫", "uralsk": "–£—Ä–∞–ª—å—Å–∫", "–æ—Ä–∞–ª": "–£—Ä–∞–ª—å—Å–∫", "oral": "–£—Ä–∞–ª—å—Å–∫",
    "–ø–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "petropavlovsk": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "–ø–µ—Ç—Ä–æ–ø–∞–≤–ª": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "petropavl": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫",
    "—Ç—É—Ä–∫–µ—Å—Ç–∞–Ω": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω", "turkestan": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω", "—Ç“Ø—Ä–∫—ñ—Å—Ç–∞–Ω": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω", "turkistan": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω",
    "–∞–∫—Ç–∞—É": "–ê–∫—Ç–∞—É", "aktau": "–ê–∫—Ç–∞—É", "–∞“õ—Ç–∞—É": "–ê–∫—Ç–∞—É", "aqtau": "–ê–∫—Ç–∞—É", "—à–µ–≤—á–µ–Ω–∫–æ": "–ê–∫—Ç–∞—É", "shevchenko": "–ê–∫—Ç–∞—É",
    "—Ç–µ–º–∏—Ä—Ç–∞—É": "–¢–µ–º–∏—Ä—Ç–∞—É", "temirtau": "–¢–µ–º–∏—Ä—Ç–∞—É",
    "–∫–æ–∫—à–µ—Ç–∞—É": "–ö–æ–∫—à–µ—Ç–∞—É", "kokshetau": "–ö–æ–∫—à–µ—Ç–∞—É", "–∫”©–∫—à–µ—Ç–∞—É": "–ö–æ–∫—à–µ—Ç–∞—É", "qokshetau": "–ö–æ–∫—à–µ—Ç–∞—É", "–∫–æ–∫—á–µ—Ç–∞–≤": "–ö–æ–∫—à–µ—Ç–∞—É", "kokchetav": "–ö–æ–∫—à–µ—Ç–∞—É",
    "—Ç–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "taldykorgan": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "—Ç–∞–ª–¥—ã“õ–æ—Ä“ì–∞–Ω": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "taldyqorgan": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "taldikorgan": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω",
    "—ç–∫–∏–±–∞—Å—Ç—É–∑": "–≠–∫–∏–±–∞—Å—Ç—É–∑", "ekibastuz": "–≠–∫–∏–±–∞—Å—Ç—É–∑", "–µ–∫—ñ–±–∞—Å—Ç“±–∑": "–≠–∫–∏–±–∞—Å—Ç—É–∑",
    "—Ä—É–¥–Ω—ã–π": "–†—É–¥–Ω—ã–π", "rudny": "–†—É–¥–Ω—ã–π", "rudniy": "–†—É–¥–Ω—ã–π", "rudnyi": "–†—É–¥–Ω—ã–π",
    "–∂–∞–Ω–∞–æ–∑–µ–Ω": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "zhanaozen": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "–∂–∞“£–∞”©–∑–µ–Ω": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "janaozen": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "–Ω–æ–≤—ã–π —É–∑–µ–Ω—å": "–ñ–∞–Ω–∞–æ–∑–µ–Ω",
    "–∫–æ–Ω–∞–µ–≤": "–ö–æ–Ω–∞–µ–≤", "konaev": "–ö–æ–Ω–∞–µ–≤", "“õ–æ–Ω–∞–µ–≤": "–ö–æ–Ω–∞–µ–≤", "qonaev": "–ö–æ–Ω–∞–µ–≤", "qonayev": "–ö–æ–Ω–∞–µ–≤", "–∫–∞–ø—á–∞–≥–∞–π": "–ö–æ–Ω–∞–µ–≤", "kapchagay": "–ö–æ–Ω–∞–µ–≤", "“õ–∞–ø—à–∞“ì–∞–π": "–ö–æ–Ω–∞–µ–≤", "qapshaghay": "–ö–æ–Ω–∞–µ–≤",
    "–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "zhezkazgan": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "–∂–µ–∑“õ–∞–∑“ì–∞–Ω": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "jezqazgan": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "–¥–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "dzhezkazgan": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω",
    "–±–∞–ª—Ö–∞—à": "–ë–∞–ª—Ö–∞—à", "balkhash": "–ë–∞–ª—Ö–∞—à", "–±–∞–ª“õ–∞—à": "–ë–∞–ª—Ö–∞—à", "balqash": "–ë–∞–ª—Ö–∞—à",
    "—Å–∞—Ç–ø–∞–µ–≤": "–°–∞—Ç–ø–∞–µ–≤", "satpayev": "–°–∞—Ç–ø–∞–µ–≤", "—Å”ô—Ç–±–∞–µ–≤": "–°–∞—Ç–ø–∞–µ–≤", "satbayev": "–°–∞—Ç–ø–∞–µ–≤",
    "–∫–∞—Å–∫–µ–ª–µ–Ω": "–ö–∞—Å–∫–µ–ª–µ–Ω", "kaskelen": "–ö–∞—Å–∫–µ–ª–µ–Ω", "“õ–∞—Å–∫–µ–ª–µ“£": "–ö–∞—Å–∫–µ–ª–µ–Ω", "qaskelen": "–ö–∞—Å–∫–µ–ª–µ–Ω",
    "–∫—É–ª—å—Å–∞—Ä—ã": "–ö—É–ª—å—Å–∞—Ä—ã", "kulsary": "–ö—É–ª—å—Å–∞—Ä—ã", "“õ“±–ª—Å–∞—Ä—ã": "–ö—É–ª—å—Å–∞—Ä—ã", "qulsary": "–ö—É–ª—å—Å–∞—Ä—ã",
    "–æ–Ω–ª–∞–π–Ω": "–û–Ω–ª–∞–π–Ω", "online": "–û–Ω–ª–∞–π–Ω", "zoom": "–û–Ω–ª–∞–π–Ω (Zoom)", "–æ–Ω–ª–∞–π–Ω (zoom)": "–û–Ω–ª–∞–π–Ω (Zoom)",
    "—Ç–∞—à–∫–µ–Ω—Ç": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω", "tashkent": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω",
    "–±–∏—à–∫–µ–∫": "–ë–∏—à–∫–µ–∫, –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω", "bishkek": "–ë–∏—à–∫–µ–∫, –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω",
}

WEEK_DAYS = {
    "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫—É": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–æ–º": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "monday": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "–≤—Ç–æ—Ä–Ω–∏–∫": "–í—Ç–æ—Ä–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫–∞": "–í—Ç–æ—Ä–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫—É": "–í—Ç–æ—Ä–Ω–∏–∫", "tuesday": "–í—Ç–æ—Ä–Ω–∏–∫",
    "—Å—Ä–µ–¥–∞": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥—É": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥—ã": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥–µ": "–°—Ä–µ–¥–∞", "wednesday": "–°—Ä–µ–¥–∞",
    "—á–µ—Ç–≤–µ—Ä–≥": "–ß–µ—Ç–≤–µ—Ä–≥", "—á–µ—Ç–≤–µ—Ä–≥–∞": "–ß–µ—Ç–≤–µ—Ä–≥", "—á–µ—Ç–≤–µ—Ä–≥—É": "–ß–µ—Ç–≤–µ—Ä–≥", "thursday": "–ß–µ—Ç–≤–µ—Ä–≥",
    "–ø—è—Ç–Ω–∏—Ü–∞": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü—É": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü—ã": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü–µ": "–ü—è—Ç–Ω–∏—Ü–∞", "friday": "–ü—è—Ç–Ω–∏—Ü–∞",
    "—Å—É–±–±–æ—Ç–∞": "–°—É–±–±–æ—Ç–∞", "—Å—É–±–±–æ—Ç—É": "–°—É–±–±–æ—Ç–∞", "—Å—É–±–±–æ—Ç—ã": "–°—É–±–±–æ—Ç–∞", "saturday": "–°—É–±–±–æ—Ç–∞",
    "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—è": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—é": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "sunday": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
}

EMOJI_RE = re.compile("[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]", re.UNICODE)

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()

def is_future(dt: Optional[datetime]) -> bool:
    if not dt: return False
    return dt.date() > datetime.now().date()

def parse_date(text: str) -> Optional[datetime]:
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try: return datetime(year, month, day)
        except: return None

    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month: return make_dt(year, month, int(m.group(2)))

    m = re.search(r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(r"(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12: return make_dt(year, month, int(m.group(1)))

    return None

def format_date(dt: datetime, time_str: str = None) -> str:
    months = {1:"—è–Ω–≤–∞—Ä—è", 2:"—Ñ–µ–≤—Ä–∞–ª—è", 3:"–º–∞—Ä—Ç–∞", 4:"–∞–ø—Ä–µ–ª—è", 5:"–º–∞—è", 6:"–∏—é–Ω—è", 7:"–∏—é–ª—è", 8:"–∞–≤–≥—É—Å—Ç–∞", 9:"—Å–µ–Ω—Ç—è–±—Ä—è", 10:"–æ–∫—Ç—è–±—Ä—è", 11:"–Ω–æ—è–±—Ä—è", 12:"–¥–µ–∫–∞–±—Ä—è"}
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s

def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t: return value
    return None

def extract_venue(text: str) -> Optional[str]:
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–ª–æ—â–∞–¥–∫—É –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è ‚Äî –±–æ–ª–µ–µ –∞–∫–∫—É—Ä–∞—Ç–Ω–æ, –±–µ–∑ –æ–±—Ä–µ–∑–∞–Ω–Ω—ã—Ö —Ñ—Ä–∞–∑."""
    known = ["Narxoz", "Nazarbayev", "KBTU", "–ö–ë–¢–£", "Astana Hub", "IT Park", 
             "MOST IT Hub", "Holiday Inn", "Esentai", "Yandex", "Smart Point", 
             "Almaty Arena", "Technopark", "–¢–µ—Ö–Ω–æ–ø–∞—Ä–∫", "MOST Hub", "QazInnovations",
             "Rixos", "Hilton", "Marriott", "The Ritz", "Radisson"]
    for v in known:
        if v.lower() in text.lower():
            # –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –ø–ª–æ—â–∞–¥–∫–∏ –±–µ–∑ –¥–ª–∏–Ω–Ω–æ–≥–æ —Ö–≤–æ—Å—Ç–∞
            return v
    
    # –ò—â–µ–º —Ñ–æ—Ä–º–∞—Ç "üìç –ú–µ—Å—Ç–æ" –∏–ª–∏ "@ –ú–µ—Å—Ç–æ"
    at = re.search(r"(?:üìç|@)\s*([^@\n,]+?)(?:\s*(?:https?://|t\.me/)|\s*$)", text)
    if at:
        venue = at.group(1).strip()
        # –û–±—Ä–µ–∑–∞–µ–º –ø–æ –ø–µ—Ä–≤–æ–º—É –ø—Ä–µ–¥–ª–æ–≥—É/–∑–Ω–∞–∫—É –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ
        if len(venue) > 50:
            # –ë–µ—Ä—ë–º –¥–æ –ø–µ—Ä–≤–æ–π —Å–∫–æ–±–∫–∏ –∏–ª–∏ –∑–∞–ø—è—Ç–æ–π
            cut = re.split(r'[,;(]', venue)[0].strip()
            return cut[:50] if len(cut) > 50 else cut
        return venue
    return None

def is_real_event(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS)

def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)

def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)

def dedup_title(title: str) -> str:
    half = len(title) // 2
    for i in range(10, half):
        if title[i:].strip() == title[:len(title)-i].strip():
            return title[:len(title)-i].strip()
    return title

def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-z–ê-–Ø–∞-—è–Å—ë])", r"\1 ", s)
    s = re.sub(r"([–∞-—è—ë–ê-–Ø–Å]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)
    s = re.sub(r"\s{2,}", " ", s)
    return s

def strip_leading_datetime_from_title(title: str) -> str:
    t = strip_emoji(title).strip()
    t = normalize_glued_text(t)
    t = re.sub(r"^\s*\d{1,2}:\d{2}\s*", "", t)
    t = re.sub(r"^\s*\d{1,2}\s+[–ê-–Ø–∞-—è–Å—ëA-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^\s*\d{1,2}\s+[–∞-—è—ë]{3,}(?:\s+\d{4})?\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)
    return t.strip(" -‚Äì‚Ä¢.,").strip()

def remove_dates_and_times(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'\b\d{1,2}:\d{2}(?:-\d{1,2}:\d{2})?(?:\s*[aApP][mM])?\b', '', text)
    text = re.sub(r'\b\d{1,2}(?:-[–∞-—è]{1,2})?\s+(?:—è–Ω–≤[–∞-—è]*|—Ñ–µ–≤[–∞-—è]*|–º–∞—Ä[–∞-—è]*|–∞–ø—Ä[–∞-—è]*|–º–∞—è|–º–∞–π|–∏—é–Ω[–∞-—è]*|–∏—é–ª[–∞-—è]*|–∞–≤–≥[–∞-—è]*|—Å–µ–Ω[–∞-—è]*|–æ–∫—Ç[–∞-—è]*|–Ω–æ—è[–∞-—è]*|–¥–µ–∫[–∞-—è]*)\s*(?:,?\s*\d{4}(?:\s*–≥\.?)?)?\b', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\b(?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[a-z]*|jul[a-z]*|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)\s+\d{1,2}(?:st|nd|rd|th)?(?:,?\s*\d{4})?\b', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\b\d{1,2}\s+(?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[a-z]*|jul[a-z]*|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)\s*(?:,?\s*\d{4})?\b', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\b\d{1,2}[./-]\d{1,2}(?:[./-]\d{2,4})?\b', '', text)
    text = re.sub(r',\s*\|', ' |', text) 
    text = re.sub(r'\|\s*\|', '|', text)
    text = re.sub(r'\s{2,}', ' ', text)
    return text.strip(" -‚Äì‚Ä¢.,|")

def clean_title_deterministic(raw_title: str) -> Optional[str]:
    s = strip_leading_datetime_from_title(raw_title)
    s = remove_weekday_from_start(s)
    s = strip_intro_phrases(s)
    s = fix_glued_words(s)
    s = dedup_title(s)
    s = remove_city_from_title(s)

    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx != -1 and idx > 12:
            s = s[:idx].strip(" -‚Äì‚Ä¢.,")
            break

    s = re.sub(r'\b–≤\s+–≤\b', '–≤', s, flags=re.IGNORECASE)
    s = re.sub(r'\s+(–≤|–Ω–∞|—Å|–∏|–¥–ª—è|–æ—Ç|–∑–∞|–∫|–ø–æ|–∏–∑|—É|–æ|–æ–±|at|in|on|for|and|to|the)\s*$', '', s, flags=re.IGNORECASE)
    s = re.sub(r"\s{2,}", " ", s).strip(" -‚Äì‚Ä¢.,")
    
    if len(s) < 5 or looks_like_description(s): return None
    return s[:120]

city_pattern = "|".join([re.escape(v) for v in KZ_CITIES.values()])
_GLUE_RE = re.compile(
    rf"^(\d{{1,2}})\s+([–ê-–Ø–Å–∞-—è—ëA-Za-z]{{3,}})[,\s]+(\d{{1,2}}:\d{{2}})\s*(?:(–û–Ω–ª–∞–π–Ω|online|zoom|{city_pattern})\s*)?(.+)$",
    re.IGNORECASE
)

def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m: return None

    day_s, month_s, time_str = m.group(1), m.group(2).lower(), m.group(3)
    possible_city, title_raw = (m.group(4) or "").strip(), m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num: return None

    try: dt = datetime(datetime.now().year, month_num, int(day_s))
    except: return None
    if not is_future(dt): return None

    city = KZ_CITIES.get(possible_city.lower(), possible_city) if possible_city else None
    title_raw = dedup_title(title_raw)
    if len(title_raw) < 5: return None

    return {"dt": dt, "time_str": time_str, "city": city, "title_raw": title_raw[:300], "date_formatted": format_date(dt, time_str)}


# ‚îÄ‚îÄ‚îÄ üî• –ù–û–í–û–ï: –í–∞–ª–∏–¥–∞—Ü–∏—è –æ–ø–∏—Å–∞–Ω–∏—è ‚Äî –æ—Ç—Å–µ–∏–≤–∞–Ω–∏–µ –º—É—Å–æ—Ä–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def is_garbage_description(text: str) -> bool:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –º—É—Å–æ—Ä–Ω—ã–º –æ–ø–∏—Å–∞–Ω–∏–µ–º (2–ì–ò–°, –º–µ–Ω—é –∫–∞—Ñ–µ, –Ω–∞–≤–∏–≥–∞—Ü–∏—è —Å–∞–π—Ç–∞)."""
    if not text:
        return True
    low = text.lower()
    garbage_hits = sum(1 for marker in GARBAGE_DESCRIPTION_MARKERS if marker in low)
    # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ 2+ –º–∞—Ä–∫–µ—Ä–∞ –º—É—Å–æ—Ä–∞ ‚Äî —ç—Ç–æ —Ç–æ—á–Ω–æ –º—É—Å–æ—Ä
    if garbage_hits >= 2:
        return True
    # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Ä–∫–µ—Ä ‚Äî —Ç–æ–∂–µ –º—É—Å–æ—Ä
    if garbage_hits >= 1 and len(text) < 100:
        return True
    return False


def clean_deep_description(text: str, title: str) -> str:
    """–û—á–∏—â–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ —Å —Å–∞–π—Ç–∞: —É–±–∏—Ä–∞–µ—Ç –º—É—Å–æ—Ä, –¥—É–±–ª–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –ª–∏—à–Ω–∏–µ —Ñ—Ä–∞–∑—ã."""
    if not text or is_garbage_description(text):
        return ""
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª—å –∑–∞–≥–æ–ª–æ–≤–∫–∞ –∏–∑ –æ–ø–∏—Å–∞–Ω–∏—è
    if title:
        text = re.sub(re.escape(title), "", text, count=1, flags=re.IGNORECASE)
    
    # –£–±–∏—Ä–∞–µ–º URL-—ã
    text = re.sub(r"https?://\S+", "", text)
    
    # –£–±–∏—Ä–∞–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ—Ä–∞–∑—ã
    nav_phrases = ["—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ", "–ø–µ—Ä–µ–π—Ç–∏ –Ω–∞ —Å–∞–π—Ç", 
                   "–≤—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã", "–ø–æ–ª–∏—Ç–∏–∫–∞ –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏"]
    for phrase in nav_phrases:
        text = re.sub(re.escape(phrase), "", text, flags=re.IGNORECASE)
    
    # –ß–∏—Å—Ç–∏–º
    text = re.sub(r"\s{2,}", " ", text).strip()
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É: –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è (–¥–æ 60 —Å–ª–æ–≤)
    sentences = re.split(r'(?<=[.!?])\s+', text)
    result = []
    word_count = 0
    for sent in sentences:
        words = sent.split()
        if word_count + len(words) > 60:
            break
        result.append(sent)
        word_count += len(words)
        if len(result) >= 3:
            break
    
    cleaned = " ".join(result).strip()
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ ‚Äî –µ—Å–ª–∏ –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –æ—Å—Ç–∞–ª–æ—Å—å –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞
    if len(cleaned) < 30:
        return ""
    
    return cleaned


# ‚îÄ‚îÄ‚îÄ üî• –ù–û–í–û–ï: –£–º–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è (–Ω–µ –¥–µ–¥–ª–∞–π–Ω–∞!) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_event_date_smart(full_text: str, deep_text: str) -> Optional[Dict]:
    """
    –ò—â–µ—Ç –¥–∞—Ç—É –ü–†–û–í–ï–î–ï–ù–ò–Ø —Å–æ–±—ã—Ç–∏—è, –∞ –Ω–µ –¥–µ–¥–ª–∞–π–Ω –ø–æ–¥–∞—á–∏ –∑–∞—è–≤–æ–∫.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç {"dt": datetime, "time_str": str|None} –∏–ª–∏ None.
    """
    context = (full_text or "") + " " + (deep_text or "")
    context_lower = context.lower()
    
    # 1. –ò—â–µ–º –¥–∞—Ç—ã —Ä—è–¥–æ–º —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏ –ü–†–û–í–ï–î–ï–ù–ò–Ø
    event_markers = [
        r"(?:—Å–æ—Å—Ç–æ–∏—Ç—Å—è|–ø—Ä–æ–π–¥[–µ—ë]—Ç|–ø—Ä–æ–≤–µ–¥–µ–Ω–∏[–µ—è]|–¥–∞—Ç–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è|–Ω–∞—á–∞–ª–æ|—Å—Ç–∞—Ä—Ç(?:\s+—Å–æ–±—ã—Ç–∏—è)?|—Ñ–∏–Ω–∞–ª|–ø–∏—Ç—á–∏–Ω–≥|–¥–µ–º–æ[- ]?–¥–µ–Ω—å|demo\s*day)\s*[-‚Äî:]?\s*",
        r"(?:–∫–æ–≥–¥–∞|–¥–∞—Ç–∞)\s*[-‚Äî:]?\s*",
    ]
    
    for marker_pattern in event_markers:
        # –ò—â–µ–º –¥–∞—Ç—É –ø–æ—Å–ª–µ –º–∞—Ä–∫–µ—Ä–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è
        date_after = re.search(
            marker_pattern + r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?\s*(?:,?\s*(?:–≤\s+)?(\d{1,2}:\d{2}))?",
            context_lower
        )
        if date_after:
            day = int(date_after.group(1))
            month = MONTHS_RU.get(date_after.group(2), 0)
            year = int(date_after.group(3)) if date_after.group(3) else datetime.now().year
            time_str = date_after.group(4)
            if month:
                try:
                    dt = datetime(year, month, day)
                    if is_future(dt):
                        return {"dt": dt, "time_str": time_str}
                except:
                    pass
    
    # 2. –ò—â–µ–º –¥–∞—Ç—É —Ä—è–¥–æ–º —Å –º–∞—Ä–∫–µ—Ä–∞–º–∏ –î–ï–î–õ–ê–ô–ù–ê (—á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å—Ç–∏—Ç—å –∏—Ö)
    deadline_markers = ["–¥–µ–¥–ª–∞–π–Ω", "deadline", "–ø–æ–¥–∞—á–∞ –∑–∞—è–≤–æ–∫", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–æ", 
                        "registration deadline", "–ø—Ä–∏–µ–º –∑–∞—è–≤–æ–∫ –¥–æ", "–∑–∞—è–≤–∫–∏ –¥–æ",
                        "–∑–∞—è–≤–∫–∏ –ø—Ä–∏–Ω–∏–º–∞—é—Ç—Å—è –¥–æ", "—É—Å–ø–µ–π –¥–æ"]
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –¥–∞—Ç—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
    all_dates = []
    for m in re.finditer(r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?\s*(?:,?\s*(?:–≤\s+)?(\d{1,2}:\d{2}))?", context_lower):
        day = int(m.group(1))
        month = MONTHS_RU.get(m.group(2), 0)
        year = int(m.group(3)) if m.group(3) else datetime.now().year
        time_str = m.group(4)
        if not month:
            continue
        try:
            dt = datetime(year, month, day)
        except:
            continue
        if not is_future(dt):
            continue
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º: —ç—Ç–∞ –¥–∞—Ç–∞ ‚Äî –¥–µ–¥–ª–∞–π–Ω?
        # –ë–µ—Ä—ë–º –∫–æ–Ω—Ç–µ–∫—Å—Ç 60 —Å–∏–º–≤–æ–ª–æ–≤ –î–û –¥–∞—Ç—ã
        start_pos = max(0, m.start() - 60)
        before_context = context_lower[start_pos:m.start()]
        is_deadline = any(dl in before_context for dl in deadline_markers)
        
        all_dates.append({
            "dt": dt, 
            "time_str": time_str, 
            "is_deadline": is_deadline,
            "pos": m.start()
        })
    
    if not all_dates:
        return None
    
    # 3. –ï—Å–ª–∏ –µ—Å—Ç—å –¥–∞—Ç—ã –ù–ï-–¥–µ–¥–ª–∞–π–Ω—ã ‚Äî –±–µ—Ä—ë–º —Å–∞–º—É—é –ø–æ–∑–¥–Ω—é—é (–æ–±—ã—á–Ω–æ —ç—Ç–æ –¥–∞—Ç–∞ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è)
    non_deadline_dates = [d for d in all_dates if not d["is_deadline"]]
    if non_deadline_dates:
        # –ë–µ—Ä—ë–º —Å–∞–º—É—é –ø–æ–∑–¥–Ω—é—é –¥–∞—Ç—É (—Å–æ–±—ã—Ç–∏–µ –æ–±—ã—á–Ω–æ –ø–æ–∑–∂–µ –¥–µ–¥–ª–∞–π–Ω–∞)
        best = max(non_deadline_dates, key=lambda x: x["dt"])
        return {"dt": best["dt"], "time_str": best["time_str"]}
    
    # 4. –ï—Å–ª–∏ –≤—Å–µ –¥–∞—Ç—ã ‚Äî –¥–µ–¥–ª–∞–π–Ω—ã, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None (–ª—É—á—à–µ –æ—Å—Ç–∞–≤–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—É—é –¥–∞—Ç—É)
    return None


# ‚îÄ‚îÄ‚îÄ üî• –ù–û–í–û–ï: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ—á–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def extract_event_time(full_text: str, deep_text: str) -> Optional[str]:
    """–ò—â–µ—Ç –≤—Ä–µ–º—è –ù–ê–ß–ê–õ–ê —Å–æ–±—ã—Ç–∏—è (–Ω–µ –¥–µ–¥–ª–∞–π–Ω–∞)."""
    context = (full_text or "") + " " + (deep_text or "")
    context_lower = context.lower()
    
    # –ú–∞—Ä–∫–µ—Ä—ã –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞
    time_patterns = [
        r"(?:–Ω–∞—á–∞–ª–æ|—Å—Ç–∞—Ä—Ç|–≤)\s+(\d{1,2}:\d{2})",
        r"(\d{1,2}:\d{2})\s*[-‚Äì]\s*\d{1,2}:\d{2}",  # –¥–∏–∞–ø–∞–∑–æ–Ω: –±–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ
        r"–≤—Ä–µ–º—è[:\s]+(\d{1,2}:\d{2})",
    ]
    
    for pattern in time_patterns:
        m = re.search(pattern, context_lower)
        if m:
            time_str = m.group(1)
            # –§–∏–ª—å—Ç—Ä—É–µ–º: 23:59 ‚Äî —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –¥–µ–¥–ª–∞–π–Ω, –∞ –Ω–µ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
            hour = int(time_str.split(":")[0])
            if 7 <= hour <= 22:  # –†–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è
                return time_str
    
    return None


# ‚îÄ‚îÄ‚îÄ Formatting post ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def make_post(event: Dict) -> str:
    title = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get("location", "")
    venue = event.get("venue", "")
    
    # 1. –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title = remove_city_from_title(title)
    title = strip_leading_datetime_from_title(title)

    full_text_raw = event.get("full_text", "")
    deep_description = event.get("deep_description", "")

    # üî• 2. –£–ú–ù–´–ô –í–´–ë–û–† –î–ê–¢–´ –ü–†–û–í–ï–î–ï–ù–ò–Ø (–Ω–µ –¥–µ–¥–ª–∞–π–Ω–∞!)
    smart_date = extract_event_date_smart(full_text_raw, deep_description)
    if smart_date:
        # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ –≤–º–µ—Å—Ç–æ –≤—Ä–µ–º–µ–Ω–∏ –∏–∑ –¥–µ–¥–ª–∞–π–Ω–∞
        event_time = smart_date.get("time_str")
        if not event_time:
            event_time = extract_event_time(full_text_raw, deep_description)
        date_str = format_date(smart_date["dt"], event_time)
    else:
        # –ï—Å–ª–∏ —É–º–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –Ω–µ –Ω–∞—à—ë–ª –æ—Ç–¥–µ–ª—å–Ω—É—é –¥–∞—Ç—É, –ø—Ä–æ–±—É–µ–º —Ö–æ—Ç—è –±—ã –Ω–∞–π—Ç–∏ –≤—Ä–µ–º—è –Ω–∞—á–∞–ª–∞
        event_time = extract_event_time(full_text_raw, deep_description)
        if event_time:
            # –ï—Å–ª–∏ –≤ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –¥–∞—Ç–µ —É–∂–µ –µ—Å—Ç—å –≤—Ä–µ–º—è (—Ç–∏–ø–∞ 23:59 –¥–µ–¥–ª–∞–π–Ω) ‚Äî –∑–∞–º–µ–Ω—è–µ–º
            # –£–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ä–æ–µ –≤—Ä–µ–º—è –∏–∑ date_str –∏ —Å—Ç–∞–≤–∏–º –Ω–æ–≤–æ–µ
            date_str_no_time = re.sub(r',?\s*\d{1,2}:\d{2}', '', date_str).strip()
            date_str = f"{date_str_no_time}, {event_time}"

    # üî• 3. –ü–†–ò–û–†–ò–¢–ï–¢ –û–ü–ò–°–ê–ù–ò–Ø (—Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –º—É—Å–æ—Ä–∞!)
    description = ""
    
    # –ü—Ä–æ–±—É–µ–º deep_description (—Å —Å–∞–π—Ç–∞), –Ω–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º—É—Å–æ—Ä
    if deep_description and len(deep_description) > 50:
        cleaned_deep = clean_deep_description(deep_description, title)
        if cleaned_deep:
            description = cleaned_deep
    
    # –ï—Å–ª–∏ deep –Ω–µ –ø—Ä–æ–∫–∞—Ç–∏–ª ‚Äî –∏—â–µ–º –ø—Ä–æ–≥—Ä–∞–º–º—É –≤ –ø–æ–ª–Ω–æ–º —Ç–µ–∫—Å—Ç–µ
    if not description:
        program_block = extract_program_block(full_text_raw)
        if program_block and len(program_block) > 40:
            description = program_block
    
    # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –ø–æ–ª–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
    if not description:
        description = generate_universal_description(full_text_raw, title)
    
    # –ó–∞–≥–ª—É—à–∫–∞
    if not description:
        description = generate_fallback_description(title)

    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –≤ –æ–ø–∏—Å–∞–Ω–∏–∏
    if description:
        desc_clean = strip_emoji(description).strip()
        desc_prefix = desc_clean[:25]
        if len(desc_prefix) > 15:
            idx = title.lower().find(desc_prefix.lower())
            if idx > 3:
                title = title[:idx].strip(" -‚Äì‚Ä¢.,:;|")
                title = re.sub(r'\s+(–≤|–Ω–∞|—Å|–∏|–¥–ª—è|–æ—Ç|–∑–∞|–∫|–ø–æ|–∏–∑|—É|–æ|–æ–±|at|in|on|for|and|to|the)\s*$', '', title, flags=re.IGNORECASE)

    # 4. –§–ò–ù–ê–õ–¨–ù–ê–Ø –ó–ê–ß–ò–°–¢–ö–ê
    title = fix_glued_words(remove_dates_and_times(title))
    description = fix_glued_words(remove_dates_and_times(description))

    # 5. –°–ë–û–†–ö–ê –ü–û–°–¢–ê
    lines = [f"üéØ <b>{title.strip()}</b>"]

    if description:
        lines.append("")
        lines.append(f"üìù {description.strip()}")

    # –õ–æ–∫–∞—Ü–∏—è
    if location in ("–û–Ω–ª–∞–π–Ω", "–û–Ω–ª–∞–π–Ω (Zoom)"):
        lines.append(f"üåê {location}")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    # –ü–ª–æ—â–∞–¥–∫–∞
    if venue: 
        lines.append(f"üìç {venue}")

    # –î–∞—Ç–∞ –∏ —Å—Å—ã–ª–∫–∞
    lines.append(f"üìÖ {date_str}")
    lines.append(f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)


class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"}
            )
        return self.session

    async def close(self):
        if self.session: await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    async def fetch_event_details(self, url: str) -> Dict[str, str]:
        """
        üî• –£–õ–£–ß–®–ï–ù–û: –ü–æ–ª—É—á–∞–µ—Ç –æ–ø–∏—Å–∞–Ω–∏–µ –∏ —Ñ–æ—Ç–æ —Å —Å–∞–π—Ç–∞ —Å–æ–±—ã—Ç–∏—è.
        –¢–µ–ø–µ—Ä—å —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç –º—É—Å–æ—Ä (2–ì–ò–°, –º–µ–Ω—é –∫–∞—Ñ–µ –∏ —Ç.–¥.)
        """
        result = {"desc": "", "image": ""}
        if not url or not url.startswith("http") or "t.me" in url:
            return result

        try:
            html = await self.fetch(url)
            if not html: return result
            soup = BeautifulSoup(html, "html.parser")

            # 1. –§–æ—Ç–æ (og:image)
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"):
                img_url = og_image["content"]
                if not img_url.startswith("http"):
                    from urllib.parse import urljoin
                    img_url = urljoin(url, img_url)
                if is_clean_photo(img_url):
                    result["image"] = img_url

            # –û—á–∏—Å—Ç–∫–∞ HTML
            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "menu", "form"]):
                tag.decompose()

            # 2. –°–û–ë–ò–†–ê–ï–ú –û–ü–ò–°–ê–ù–ò–ï (—Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –º—É—Å–æ—Ä–∞)
            content_area = soup.find("main") or soup.find("article") or soup.body
            collected_chunks = []
            
            if content_area:
                for elem in content_area.find_all(['p', 'li']):
                    txt = elem.get_text(separator=" ", strip=True)
                    if len(txt) < 30:
                        continue
                    
                    low_txt = txt.lower()
                    
                    # üî• –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º—É—Å–æ—Ä–Ω—ã–µ –∞–±–∑–∞—Ü—ã
                    if any(marker in low_txt for marker in GARBAGE_DESCRIPTION_MARKERS):
                        continue
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏—é —Å–∞–π—Ç–∞
                    if any(bad in low_txt for bad in ["cookie", "–≤–æ–π—Ç–∏", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞", 
                                                       "–≤—Å–µ –ø—Ä–∞–≤–∞ –∑–∞—â–∏—â–µ–Ω—ã", "¬©", "privacy"]):
                        continue
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –º–µ–Ω—é/—Å–ø–∏—Å–∫–∏ —Ç–æ–≤–∞—Ä–æ–≤ (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω—ã –∑–∞–ø—è—Ç—ã–µ —á–µ—Ä–µ–∑ –∫–∞–∂–¥—ã–µ 1-2 —Å–ª–æ–≤–∞)
                    comma_ratio = txt.count(",") / max(len(txt.split()), 1)
                    if comma_ratio > 0.3:  # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∑–∞–ø—è—Ç—ã—Ö ‚Äî —ç—Ç–æ —Å–ø–∏—Å–æ–∫, –Ω–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                        continue
                    
                    collected_chunks.append(txt)
                    
                    if len(collected_chunks) >= 5:
                        break
            
            if collected_chunks:
                full_desc = " ".join(collected_chunks)
                full_desc = re.sub(r"\s{2,}", " ", full_desc)
                
                words = full_desc.split()
                if words:
                    result["desc"] = " ".join(words[:80]) + ("..." if len(words) > 80 else "")
                    
        except Exception as e:
            logger.error(f"fetch_event_details {url}: {e}")
            
        return result

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+\d{4})?)", line, re.IGNORECASE)
            if not dm:
                i += 1; continue

            date_raw = dm.group(0)
            rest = line[dm.end():].strip()
            tm = re.search(r"(?:–≤\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm: rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -‚Äì‚Ä¢")
            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"): link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith("http"): link = "https://" + link
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt): title_raw = nxt

            if len(title_raw) < 5:
                i += 1; continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                i += 1; continue

            ctx = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_location(ctx) or extract_location(text)
            title_clean = clean_title_deterministic(title_raw) or dedup_title(title_raw[:120])
            if not title_clean:
                i += 1; continue

            events.append({
                "title": title_clean, "date": format_date(dt, time_str), "location": location or "",
                "venue": extract_venue(ctx), "link": link or post_link, "source": source, "image_url": image_url
            })
            i += 1
        return events

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html: return []
        soup = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td: continue
                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30: continue

                le = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"
                norm_link = normalize_link(post_link)

                external_link = None
                links_in_text = re.findall(r"(https?://[^\s]+)", text)
                for l in links_in_text:
                    clean_l = normalize_link(l)
                    if "t.me" not in clean_l:
                        external_link = clean_l
                        break

                final_link = external_link if external_link else norm_link
                if norm_link in self.posted: continue

                external_link = None
                for a in td.find_all("a", href=True):
                    href = normalize_link(a["href"])
                    if "t.me" not in href:
                        external_link = href
                        break

                if not external_link:
                    links_in_text = re.findall(r"(https?://[^\s]+)", text)
                    for l in links_in_text:
                        clean_l = normalize_link(l)
                        if "t.me" not in clean_l:
                            external_link = clean_l
                            break

                final_link = external_link if external_link else norm_link
                image_url = None

                photo_wrap = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if photo_wrap:
                    style = photo_wrap.get("style", "")
                    match = re.search(r"url\('([^']+)'\)", style)
                    if match: image_url = match.group(1)

                if not image_url:
                    img_tag = td.find("img")
                    if img_tag and img_tag.get("src"): image_url = img_tag["src"]

                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    continue

                if not is_real_event(text): continue
                dt = parse_date(text)
                if not is_future(dt): continue

                title_candidate = None
                for ln in text.split("\n"):
                    ln = strip_emoji(ln).strip()
                    if len(ln) > 10:
                        title_candidate = ln
                        break

                raw_title = title_candidate or ""
                city_from_title = extract_city_from_title(raw_title)
                title = clean_title_deterministic(raw_title)
                if not title: continue

                tm2 = re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append({
                    "title": title, "date": format_date(dt, time_str), "location": extract_location(text) or city_from_title or "",
                    "venue": extract_venue(text), "link": final_link, "source": channel["name"], "full_text": text, "image_url": image_url
                })
            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue
        return all_events
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []
        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)

        return all_events

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site["url"])
        if not html: return []
        soup = BeautifulSoup(html, "html.parser")
        events = []

        for link in soup.find_all("a", href=True)[:80]:
            try:
                href = link.get("href", "")
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15: continue
                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)

                href = normalize_link(href)
                if href.rstrip("/") == normalize_link(site["url"]).rstrip("/"): continue
                if href in self.posted: continue
                if is_site_trash(title_raw): continue
                if not is_real_event(title_raw): continue

                parent = link.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt): continue
                image_url = None

                if parent:
                    imgs = parent.find_all("img")
                    for img in imgs:
                        src = img.get("src") or img.get("data-src")
                        if not src: continue
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src
                            break

                if not image_url and parent:
                    style = parent.get("style", "")
                    match = re.search(r"url\(['\"]?([^'\")]+)", style)
                    if match:
                        src = match.group(1)
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src

                title_clean = clean_title_deterministic(title_raw) or strip_emoji(dedup_title(title_raw))[:120]
                events.append({
                    "title": title_clean, "date": format_date(dt), "location": extract_location(context) or "",
                    "venue": extract_venue(context), "link": href, "full_text": context, "source": site["name"], "image_url": image_url
                })
                if len(events) >= 5: break
            except Exception:
                continue
        return events


# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot_api = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")
        logger.info(f"üì¶ –£–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {len(bot_obj.posted)}")

        posted = 0

        for event in unique[:15]:
            norm_link = normalize_link(event.get("link", ""))

            if norm_link in bot_obj.posted:
                logger.info(f"‚è≠Ô∏è –£–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ—Å—å: {event.get('title')[:50]}")
                continue

            # 1. –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ñ–æ—Ç–æ
            details = await bot_obj.fetch_event_details(norm_link)
            
            if isinstance(details, dict):
                if details.get("desc"):
                    event["deep_description"] = details["desc"]
                if details.get("image"):
                    event["image_url"] = details["image"]
            elif isinstance(details, str) and details:
                event["deep_description"] = details

            text = make_post(event)
            if not text:
                continue

            try:
                photo_url = event.get("image_url")
                
                if photo_url:
                    try:
                        session = await bot_obj.get_session()
                        async with session.get(photo_url, timeout=15) as resp:
                            if resp.status == 200:
                                photo_bytes = await resp.read()
                                await bot_api.send_photo(
                                    chat_id=CHANNEL_ID,
                                    message_thread_id=MESSAGE_THREAD_ID,
                                    photo=photo_bytes,
                                    caption=text,
                                    parse_mode="HTML",
                                )
                            else:
                                raise Exception("Bad HTTP status for image")
                    except Exception as img_e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–æ—Ç–æ, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç. –û—à–∏–±–∫–∞: {img_e}")
                        await bot_api.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )
                else:
                    await bot_api.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                bot_obj.posted.add(norm_link)
                save_posted(bot_obj.posted)

                posted += 1
                logger.info(f"‚úÖ ({posted}) {event.get('title','')[:50]}")

                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –Ω–æ–≤—ã—Ö: {posted}")

    finally:
        await bot_obj.close()

if __name__ == "__main__":
    asyncio.run(main())
