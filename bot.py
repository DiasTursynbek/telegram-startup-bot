import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN         = os.getenv('BOT_TOKEN')
CHANNEL_ID        = os.getenv('CHANNEL_ID', "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv('MESSAGE_THREAD_ID', '4'))
CLAUDE_API_KEY    = os.getenv('CLAUDE_API_KEY', '')

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz",                 "name": "ER10"},
    {"url": "https://kapital.kz",              "name": "Capital"},
    {"url": "https://forbes.kz",               "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media",         "name": "Kursiv kz"},
    {"url": "https://ma7.vc",                  "name": "MA7"},
    {"url": "https://tumarventures.com",        "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io",     "name": "White hill capital"},
    {"url": "https://bigsky.vc",               "name": "Big sky ventures"},
    {"url": "https://mostfund.vc",             "name": "Most ventures"},
    {"url": "https://axiomcapital.com",        "name": "Axiom capital"},
    {"url": "https://jastarventures.com",       "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz",         "name": "NURIS"},
    {"url": "https://tech.kz",                 "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz",  "name": "Digital Business KZ"},
    {"username": "vcinsightskz",       "name": "VC Insights KZ"},
    {"username": "tech_kz",            "name": "Tech KZ"},
    {"username": "startupalmaty",      "name": "Startup Almaty"},
    {"username": "astanahub_events",   "name": "Astana Hub Events"},
]

MONTHS_RU = {
    '—è–Ω–≤–∞—Ä—è':1,'—Ñ–µ–≤—Ä–∞–ª—è':2,'–º–∞—Ä—Ç–∞':3,'–∞–ø—Ä–µ–ª—è':4,
    '–º–∞—è':5,'–∏—é–Ω—è':6,'–∏—é–ª—è':7,'–∞–≤–≥—É—Å—Ç–∞':8,
    '—Å–µ–Ω—Ç—è–±—Ä—è':9,'–æ–∫—Ç—è–±—Ä—è':10,'–Ω–æ—è–±—Ä—è':11,'–¥–µ–∫–∞–±—Ä—è':12,
}
MONTHS_SHORT = {
    '—è–Ω–≤':1,'—Ñ–µ–≤':2,'–º–∞—Ä':3,'–∞–ø—Ä':4,
    '–º–∞–π':5,'–∏—é–Ω':6,'–∏—é–ª':7,'–∞–≤–≥':8,
    '—Å–µ–Ω':9,'–æ–∫—Ç':10,'–Ω–æ—è':11,'–¥–µ–∫':12,
}

EVENT_WORDS = [
    '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è','conference','—Ñ–æ—Ä—É–º','forum','summit','—Å–∞–º–º–∏—Ç',
    'meetup','–º–∏—Ç–∞–ø','—Ö–∞–∫–∞—Ç–æ–Ω','hackathon','–≤–æ—Ä–∫—à–æ–ø','workshop',
    '–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å','masterclass','–≤–µ–±–∏–Ω–∞—Ä','webinar','—Å–µ–º–∏–Ω–∞—Ä',
    'pitch','–ø–∏—Ç—á','demo day','–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä','accelerator',
    'bootcamp','–±—É—Ç–∫–µ–º–ø','–≤—ã—Å—Ç–∞–≤–∫–∞','–∫–æ–Ω–∫—É—Ä—Å','competition',
    '—Ç—Ä–µ–Ω–∏–Ω–≥','training','–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ','–∏–≤–µ–Ω—Ç','event',
    '–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç','–ø—Ä–∏–≥–ª–∞—à–∞–µ–º','–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è','—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è',
]
NOT_EVENT_WORDS = [
    'research','–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ','–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª','–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥',
    '–º–ª–Ω $','–º–ª—Ä–¥ $','–Ω–∞–∑–Ω–∞—á–µ–Ω','—É–≤–æ–ª–µ–Ω','–æ—Ç—á–µ—Ç','–≤—ã—Ä—É—á–∫–∞',
    '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞','–±–∏—Ä–∂–∞','–∞–∫—Ü–∏–∏','—Ç–æ–∫–∞–µ–≤','–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ',
]
SITE_STOP_WORDS = [
    '–∫–æ–Ω—Ç–∞–∫—Ç—ã','–æ –Ω–∞—Å','–ø–æ–ª–∏—Ç–∏–∫–∞','–≤–æ–π—Ç–∏','—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞',
    '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è','–ø–æ–∏—Å–∫','–≥–ª–∞–≤–Ω–∞—è','–º–µ–Ω—é','–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏',
    '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ','–ø–æ–¥—Ä–æ–±–Ω–µ–µ','—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ','privacy','terms','cookie',
]
# –ü—Ä–∏–∑–Ω–∞–∫–∏ –æ–ø–∏—Å–∞–Ω–∏—è ‚Äî –Ω–µ –ø–æ–¥—Ö–æ–¥—è—Ç –∫–∞–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫
DESCRIPTION_SIGNALS = [
    '—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏','–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤','–≤—ã —É–∑–Ω–∞–µ—Ç–µ','–º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º',
    '–Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏','–≤ —Ä–∞–º–∫–∞—Ö','—Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤—Å—Ç—Ä–µ—á–∞','–ø—Ä–∏–≥–ª–∞—à–∞–µ–º –≤–∞—Å',
    '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å','–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Å—Å—ã–ª–∫–µ','—Å–≤–æ–±–æ–¥–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ',
    '–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç –≤–∞—Å –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ',
]

KZ_CITIES = {
    '–∞–ª–º–∞—Ç—ã':'–ê–ª–º–∞—Ç—ã','–∞—Å—Ç–∞–Ω–∞':'–ê—Å—Ç–∞–Ω–∞','—à—ã–º–∫–µ–Ω—Ç':'–®—ã–º–∫–µ–Ω—Ç',
    '–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω':'–ê—Å—Ç–∞–Ω–∞','—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫':'–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫',
    '–∫—ã–∑—ã–ª–æ—Ä–¥–∞':'–ö—ã–∑—ã–ª–æ—Ä–¥–∞','–∞–∫—Ç–æ–±–µ':'–ê–∫—Ç–æ–±–µ','—Ç–∞—Ä–∞–∑':'–¢–∞—Ä–∞–∑',
    '–ø–∞–≤–ª–æ–¥–∞—Ä':'–ü–∞–≤–ª–æ–¥–∞—Ä','—Å–µ–º–µ–π':'–°–µ–º–µ–π','–∞—Ç—ã—Ä–∞—É':'–ê—Ç—ã—Ä–∞—É',
    '–∂–µ–∑–∫–∞–∑–≥–∞–Ω':'–ñ–µ–∑“õ–∞–∑“ì–∞–Ω','–∂–µ–∑“õ–∞–∑“ì–∞–Ω':'–ñ–µ–∑“õ–∞–∑“ì–∞–Ω','–∞–∫—Ç–∞—É':'–ê–∫—Ç–∞—É',
    '–æ–Ω–ª–∞–π–Ω':'–û–Ω–ª–∞–π–Ω','online':'–û–Ω–ª–∞–π–Ω','zoom':'–û–Ω–ª–∞–π–Ω (Zoom)',
    '—Ç–∞—à–∫–µ–Ω—Ç':'–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω',
}

EMOJI_RE = re.compile(
    '[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]',
    re.UNICODE,
)

# ‚îÄ‚îÄ‚îÄ –•–µ–ª–ø–µ—Ä—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub('', s).strip()


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def parse_date(text: str) -> Optional[datetime]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É. –ù–ï –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç +1 –≥–æ–¥ –∫ –ø—Ä–æ—à–µ–¥—à–∏–º –¥–∞—Ç–∞–º:
    –µ—Å–ª–∏ –≥–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω –∏ –¥–∞—Ç–∞ –ø—Ä–æ—à–ª–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
    –≠—Ç–æ —É–±–∏—Ä–∞–µ—Ç –ª–æ–∂–Ω—ã–µ –ø–æ—Å—Ç—ã —Ç–∏–ø–∞ '16 —Ñ–µ–≤—Ä–∞–ª—è 2027'.
    """
    t   = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try:
            return datetime(year, month, day)
        except Exception:
            return None

    # –î–î-–î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r'(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?', t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year  = int(m.group(4)) if m.group(4) else now.year
        if month:
            return make_dt(year, month, int(m.group(2)))

    # –î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r'(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?', t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î –ú–µ—Å[—Å–æ–∫—Ä] [–ì–ì–ì–ì]
    m = re.search(
        r'(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?', t
    )
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î.–ú–ú[.–ì–ì–ì–ì]
    m = re.search(r'(\d{1,2})\.(\d{2})(?:\.(\d{4}))?', t)
    if m:
        month = int(m.group(2))
        year  = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12:
            return make_dt(year, month, int(m.group(1)))

    return None


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1:'—è–Ω–≤–∞—Ä—è',2:'—Ñ–µ–≤—Ä–∞–ª—è',3:'–º–∞—Ä—Ç–∞',4:'–∞–ø—Ä–µ–ª—è',
        5:'–º–∞—è',6:'–∏—é–Ω—è',7:'–∏—é–ª—è',8:'–∞–≤–≥—É—Å—Ç–∞',
        9:'—Å–µ–Ω—Ç—è–±—Ä—è',10:'–æ–∫—Ç—è–±—Ä—è',11:'–Ω–æ—è–±—Ä—è',12:'–¥–µ–∫–∞–±—Ä—è',
    }
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s


def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    known = ['Narxoz','Nazarbayev','KBTU','–ö–ë–¢–£','Astana Hub',
             'IT Park','MOST IT Hub','Holiday Inn','Esentai',
             'Yandex','Smart Point','Almaty Arena']
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf'{v}[^\n,.]*', text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', text)
    if at:
        return at.group(1).strip()[:60]
    return None


def is_real_event(text: str) -> bool:
    t = text.lower()
    return (any(w in t for w in EVENT_WORDS)
            and not any(w in t for w in NOT_EVENT_WORDS))


def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)


def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)


def dedup_title(title: str) -> str:
    """'Data Community BirthdayData Community Birthday' ‚Üí 'Data Community Birthday'"""
    for i in range(10, len(title) // 2 + 1):
        if title[i:].startswith(title[:i]):
            return title[:i].strip(' .,‚Äì-')
    return title


# ‚îÄ‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ "09 –§–µ–≤, 17:00–®—ã–º–∫–µ–Ω—Ç –ù–∞–∑–≤–∞–Ω–∏–µ" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_GLUE_RE = re.compile(
    r'^(\d{1,2})\s+'                                              # –¥–µ–Ω—å
    r'([–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,})'                                    # –º–µ—Å—è—Ü
    r'[,\s]+'
    r'(\d{1,2}:\d{2})'                                            # –≤—Ä–µ–º—è
    r'([–ê-–Ø–Å][–∞-—è—ë]+(?:-[–ê-–Ø–Å–∞-—è—ë]+)?(?:\s[–ê-–Ø–Å][–∞-—è—ë]+)?)?'   # –≥–æ—Ä–æ–¥ (–æ–ø—Ü)
    r'\s*(.+)$'                                                   # –∑–∞–≥–æ–ª–æ–≤–æ–∫
)

def parse_glued_line(line: str) -> Optional[Dict]:
    line = strip_emoji(line).strip()
    m    = _GLUE_RE.match(line)
    if not m:
        return None

    day_s, month_s  = m.group(1), m.group(2).lower()
    time_str        = m.group(3)
    possible_city   = (m.group(4) or '').strip()
    title_raw       = m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    try:
        dt = datetime(datetime.now().year, month_num, int(day_s))
    except Exception:
        return None

    # –î–∞—Ç–∞ –ø—Ä–æ—à–ª–∞ ‚Äî –ù–ï –ø—Ä–∏–±–∞–≤–ª—è–µ–º –≥–æ–¥, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
    if not is_future(dt):
        return None

    city      = KZ_CITIES.get(possible_city.lower()) if possible_city else None
    title_raw = dedup_title(title_raw)

    if len(title_raw) < 5:
        return None

    return {
        'dt':             dt,
        'time_str':       time_str,
        'city':           city or (possible_city if possible_city else None),
        'title_raw':      title_raw[:300],
        'date_formatted': format_date(dt, time_str),
    }


# ‚îÄ‚îÄ‚îÄ Claude: —á–∏—Å—Ç–∏—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def claude_clean_title(raw_text: str, session: aiohttp.ClientSession) -> Optional[str]:
    """
    –ü—Ä–æ—Å–∏–º Claude –∏–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–ª–∏ None (–µ—Å–ª–∏ SKIP / –æ—à–∏–±–∫–∞).
    """
    if not CLAUDE_API_KEY:
        return None

    prompt = (
        "–ò–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ –∏–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è (1 —Å—Ç—Ä–æ–∫–∞).\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "- –¢–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ: –±–µ–∑ –¥–∞—Ç—ã, –≤—Ä–µ–º–µ–Ω–∏, –≥–æ—Ä–æ–¥–∞, —Å—Å—ã–ª–æ–∫, —Ö—ç—à—Ç–µ–≥–æ–≤\n"
        "- –ï—Å–ª–∏ —Å–ª–æ–≤–∞ —Å–ª–∏–ø–ª–∏—Å—å (–Ω–∞–ø—Ä. 'Birthday–î–∞—Ç–∞') ‚Äî —Ä–∞–∑–¥–µ–ª–∏ –ø—Ä–æ–±–µ–ª–æ–º\n"
        "- –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ –¥—É–±–ª–∏—Ä—É–µ—Ç—Å—è –¥–≤–∞–∂–¥—ã ‚Äî –æ—Å—Ç–∞–≤—å –æ–¥–Ω–æ\n"
        "- –ï—Å–ª–∏ —ç—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ –∞ –Ω–µ –Ω–∞–∑–≤–∞–Ω–∏–µ ‚Äî –æ—Ç–≤–µ—Ç—å: SKIP\n"
        "- –ï—Å–ª–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–∑–≤–ª–µ—á—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ ‚Äî –æ—Ç–≤–µ—Ç—å: SKIP\n"
        "- –¢–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ, –±–æ–ª—å—à–µ –Ω–∏—á–µ–≥–æ\n\n"
        f"–¢–µ–∫—Å—Ç:\n{raw_text[:800]}"
    )

    try:
        async with session.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key":           CLAUDE_API_KEY,
                "anthropic-version":   "2023-06-01",
                "content-type":        "application/json",
            },
            json={
                "model":      "claude-haiku-4-5-20251001",
                "max_tokens": 80,
                "messages":   [{"role": "user", "content": prompt}],
            },
            timeout=10,
        ) as resp:
            if resp.status != 200:
                return None
            data   = await resp.json()
            result = data["content"][0]["text"].strip()
            if result.upper() == "SKIP" or len(result) < 5:
                return None
            return result[:120]
    except Exception as e:
        logger.error(f"Claude API error: {e}")
        return None


# ‚îÄ‚îÄ‚îÄ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–∞ (—Å—Ç—Ä–æ–≥–æ 4-5 —Å—Ç—Ä–æ–∫) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def make_post(event: Dict) -> str:
    title    = (event.get('title') or '').strip()
    date_str = (event.get('date')  or '').strip()
    link     = (event.get('link')  or '').strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get('location', '')
    venue    = event.get('venue', '')

    lines = [f"üéØ <b>{title}</b>"]

    if location in ('–û–Ω–ª–∞–π–Ω', '–û–Ω–ª–∞–π–Ω (Zoom)'):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue:
        lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {date_str}")
    lines.append(f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ EventBot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class EventBot:
    def __init__(self):
        self.session = None
        self.posted  = set()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={'User-Agent': 'Mozilla/5.0'})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    # ‚îÄ‚îÄ –î–∞–π–¥–∂–µ—Å—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines  = text.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1; continue

            dm = re.match(
                r'^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?'
                r'|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*'
                r'(?:\s+\d{4})?)',
                line, re.IGNORECASE,
            )
            if not dm:
                i += 1; continue

            date_raw = dm.group(0)
            rest     = line[dm.end():].strip()

            tm = re.search(r'(?:–≤\s*)?(\d{1,2}:\d{2})', rest)
            time_str = tm.group(1) if tm else None
            if tm:
                rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(' -‚Äì‚Ä¢')

            link = None
            lm = re.search(r'((?:https?://|t\.me/)\S+)', line)
            if lm:
                link = lm.group(1)
                if not link.startswith('http'):
                    link = 'https://' + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), '').strip()
            else:
                for j in range(i+1, min(i+4, len(lines))):
                    lm2 = re.search(r'((?:https?://|t\.me/)\S+)', lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith('http'):
                            link = 'https://' + link
                        break

            if len(title_raw) < 5 and i+1 < len(lines):
                nxt = strip_emoji(lines[i+1]).strip()
                if len(nxt) > 5 and not re.match(r'^\d', nxt):
                    title_raw = nxt

            if len(title_raw) < 5:
                i += 1; continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ—à–µ–¥—à–µ–µ (–¥–∞–π–¥–∂–µ—Å—Ç): {title_raw[:40]}")
                i += 1; continue

            ctx      = line + ' ' + (lines[i+1] if i+1 < len(lines) else '')
            location = extract_location(ctx) or extract_location(text)

            events.append({
                'title':     dedup_title(title_raw[:120]),
                'date':      format_date(dt, time_str),
                'location':  location or '',
                'venue':     extract_venue(ctx),
                'link':      link or post_link,
                'source':    source,
                'image_url': image_url,
            })
            i += 1
        return events

    # ‚îÄ‚îÄ Telegram-–∫–∞–Ω–∞–ª ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html:
            return []

        soup       = BeautifulSoup(html, 'html.parser')
        all_events = []
        session    = await self.get_session()

        for msg in soup.find_all('div', class_='tgme_widget_message')[:20]:
            try:
                td = msg.find('div', class_='tgme_widget_message_text')
                if not td:
                    continue
                text = td.get_text(separator='\n', strip=True)
                if len(text) < 30:
                    continue

                le = msg.find('a', class_='tgme_widget_message_date')
                post_link = le['href'] if le else f"https://t.me/{channel['username']}"

                if post_link in self.posted:
                    continue
                self.posted.add(post_link)

                # –û–¥–Ω–∞ –∫–∞—Ä—Ç–∏–Ω–∫–∞
                image_url = None
                img_div = msg.find('a', class_='tgme_widget_message_photo_wrap')
                if img_div:
                    sm = re.search(r"url\('([^']+)'\)", img_div.get('style', ''))
                    if sm:
                        image_url = sm.group(1)

                # ‚îÄ‚îÄ –î–∞–π–¥–∂–µ—Å—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if re.search(r'\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}', text):
                    evs = self.parse_digest(text, post_link, channel['name'], image_url)
                    all_events.extend(evs)
                    logger.info(f"üìã –î–∞–π–¥–∂–µ—Å—Ç {channel['name']}: {len(evs)}")
                    continue

                if not is_real_event(text):
                    continue

                # –ü–µ—Ä–≤–∞—è –Ω–µ–ø—É—Å—Ç–∞—è —Å—Ç—Ä–æ–∫–∞
                first_line = ''
                for ln in text.strip().split('\n'):
                    cl = strip_emoji(ln).strip()
                    if len(cl) > 10:
                        first_line = cl
                        break

                # ‚îÄ‚îÄ –ü—Ä–∏–∫–ª–µ–µ–Ω–Ω–∞—è –¥–∞—Ç–∞? ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                has_glue = bool(re.search(
                    r'\d{1,2}\s+[–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,}[,\s]+\d{1,2}:\d{2}[–ê-–Ø–ÅA-Za-z]',
                    first_line,
                ))

                if has_glue:
                    glued = parse_glued_line(first_line)
                    if not glued:
                        # –î–∞—Ç–∞ –ø—Ä–æ—à–ª–∞ –∏–ª–∏ –Ω–µ —Ä–∞—Å–ø–∞—Ä—Å–∏–ª–∞—Å—å ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        logger.info(f"‚è≠Ô∏è –ü—Ä–∏–∫–ª–µ–µ–Ω–Ω–∞—è –¥–∞—Ç–∞: –ø—Ä–æ—à–µ–¥—à–∞—è/–Ω–µ –ø–∞—Ä—Å–∏—Ç—Å—è: {first_line[:60]}")
                        continue

                    # –ß–∏—Å—Ç–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ Claude (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –¥–ª—è –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω—ã—Ö)
                    title = None
                    if CLAUDE_API_KEY:
                        title = await claude_clean_title(text, session)
                    # Fallback –±–µ–∑ Claude
                    if not title:
                        title = dedup_title(glued['title_raw'])
                        if looks_like_description(title) or len(title) < 5:
                            logger.info(f"‚è≠Ô∏è –û–ø–∏—Å–∞–Ω–∏–µ –≤–º–µ—Å—Ç–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞: {title[:60]}")
                            continue

                    all_events.append({
                        'title':     title,
                        'date':      glued['date_formatted'],
                        'location':  glued['city'] or extract_location(text) or '',
                        'venue':     extract_venue(text),
                        'link':      post_link,
                        'source':    channel['name'],
                        'image_url': image_url,
                    })
                    continue

                # ‚îÄ‚îÄ –û–±—ã—á–Ω—ã–π –ø–æ—Å—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                dt = parse_date(text)
                if not is_future(dt):
                    logger.info(f"‚è≠Ô∏è –ü—Ä–æ—à–µ–¥—à–µ–µ/–Ω–µ—Ç –¥–∞—Ç—ã: {text[:50].strip()}")
                    continue

                title = None
                if CLAUDE_API_KEY:
                    title = await claude_clean_title(text, session)

                # Fallback –±–µ–∑ Claude
                if not title:
                    for ln in text.split('\n'):
                        ln = strip_emoji(ln).strip()
                        if (len(ln) > 10
                                and not re.match(r'^\d{1,2}\s+[–∞-—è—ë]', ln.lower())
                                and not looks_like_description(ln)):
                            title = dedup_title(ln[:120])
                            break

                if not title or looks_like_description(title):
                    logger.info(f"‚è≠Ô∏è –ù–µ—Ç/–ø–ª–æ—Ö–æ–π –∑–∞–≥–æ–ª–æ–≤–æ–∫: {text[:50].strip()}")
                    continue

                tm2      = re.search(r'\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})', text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append({
                    'title':     title,
                    'date':      format_date(dt, time_str),
                    'location':  extract_location(text) or '',
                    'venue':     extract_venue(text),
                    'link':      post_link,
                    'source':    channel['name'],
                    'image_url': image_url,
                })

            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue

        return all_events

    # ‚îÄ‚îÄ –°–∞–π—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site['url'])
        if not html:
            return []

        soup   = BeautifulSoup(html, 'html.parser')
        events = []

        for link in soup.find_all('a', href=True)[:80]:
            try:
                href      = link.get('href', '')
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                if href.rstrip('/') == site['url'].rstrip('/'):
                    continue
                if href in self.posted:
                    continue
                if is_site_trash(title_raw):
                    continue
                if not is_real_event(title_raw):
                    continue

                parent  = link.find_parent(['div','article','li','section'])
                context = parent.get_text(separator=' ', strip=True) if parent else title_raw
                dt      = parse_date(context)

                if not is_future(dt):
                    continue

                image_url = None
                img = (link.find('img', src=True)
                       or (parent.find('img', src=True) if parent else None))
                if img:
                    src = img.get('src', '')
                    if src and not src.startswith('http'):
                        from urllib.parse import urljoin
                        src = urljoin(site['url'], src)
                    image_url = src or None

                self.posted.add(href)
                events.append({
                    'title':     strip_emoji(dedup_title(title_raw))[:120],
                    'date':      format_date(dt),
                    'location':  extract_location(context) or '',
                    'venue':     extract_venue(context),
                    'link':      href,
                    'source':    site['name'],
                    'image_url': image_url,
                })

                if len(events) >= 5:
                    break
            except Exception:
                continue

        return events

    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)
            if evs:
                logger.info(f"‚úÖ {site['name']}: {len(evs)}")

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)
            if evs:
                logger.info(f"‚úÖ {ch['name']}: {len(evs)}")

        return all_events


# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot     = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = e['title'][:40].lower()
            if key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")

        posted = 0
        for event in unique[:15]:
            text = make_post(event)
            if not text:
                continue
            try:
                if event.get('image_url'):
                    try:
                        await bot.send_photo(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            photo=event['image_url'],
                            caption=text,
                            parse_mode='HTML',
                        )
                    except Exception:
                        await bot.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode='HTML',
                            disable_web_page_preview=True,
                        )
                else:
                    await bot.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode='HTML',
                        disable_web_page_preview=True,
                    )
                posted += 1
                logger.info(f"‚úÖ ({posted}) {event['title'][:50]}")
                await asyncio.sleep(2)
            except Exception as e:
                logger.error(f"‚ùå send error: {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted}")

    finally:
        await bot_obj.close()


if __name__ == '__main__':
    asyncio.run(main())
