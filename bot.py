import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path

STATE_DIR = Path("state")
POSTED_FILE = STATE_DIR / "load_posted.json"

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))






def normalize_link(link: str) -> str:
    if not link:
        return ""

    link = link.strip()

    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ /s/ Ğ¸Ğ· telegram preview
    link = link.replace("https://t.me/s/", "https://t.me/")

    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ ?...
    link = link.split("?")[0]

    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞ°ÑÑ‰Ğ¸Ğ¹ /
    link = link.rstrip("/")

    return link






def load_posted() -> set:
    if POSTED_FILE.exists():
        try:
            with open(POSTED_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()




def save_posted(posted: set):
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)

        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(list(posted), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ posted_links: {e}")



URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    "ÑĞ½Ğ²Ğ°Ñ€Ñ": 1,
    "Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ": 2,
    "Ğ¼Ğ°Ñ€Ñ‚Ğ°": 3,
    "Ğ°Ğ¿Ñ€ĞµĞ»Ñ": 4,
    "Ğ¼Ğ°Ñ": 5,
    "Ğ¸ÑĞ½Ñ": 6,
    "Ğ¸ÑĞ»Ñ": 7,
    "Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°": 8,
    "ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ": 9,
    "Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ": 10,
    "Ğ½Ğ¾ÑĞ±Ñ€Ñ": 11,
    "Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ": 12,
}
MONTHS_SHORT = {
    "ÑĞ½Ğ²": 1,
    "Ñ„ĞµĞ²": 2,
    "Ğ¼Ğ°Ñ€": 3,
    "Ğ°Ğ¿Ñ€": 4,
    "Ğ¼Ğ°Ğ¹": 5,
    "Ğ¸ÑĞ½": 6,
    "Ğ¸ÑĞ»": 7,
    "Ğ°Ğ²Ğ³": 8,
    "ÑĞµĞ½": 9,
    "Ğ¾ĞºÑ‚": 10,
    "Ğ½Ğ¾Ñ": 11,
    "Ğ´ĞµĞº": 12,
}

EVENT_WORDS = [
    "ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ",
    "conference",
    "Ñ„Ğ¾Ñ€ÑƒĞ¼",
    "forum",
    "summit",
    "ÑĞ°Ğ¼Ğ¼Ğ¸Ñ‚",
    "meetup",
    "Ğ¼Ğ¸Ñ‚Ğ°Ğ¿",
    "Ñ…Ğ°ĞºĞ°Ñ‚Ğ¾Ğ½",
    "hackathon",
    "Ğ²Ğ¾Ñ€ĞºÑˆĞ¾Ğ¿",
    "workshop",
    "Ğ¼Ğ°ÑÑ‚ĞµÑ€-ĞºĞ»Ğ°ÑÑ",
    "masterclass",
    "Ğ²ĞµĞ±Ğ¸Ğ½Ğ°Ñ€",
    "webinar",
    "ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€",
    "pitch",
    "Ğ¿Ğ¸Ñ‚Ñ‡",
    "demo day",
    "Ğ°ĞºÑĞµĞ»ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€",
    "accelerator",
    "bootcamp",
    "Ğ±ÑƒÑ‚ĞºĞµĞ¼Ğ¿",
    "Ğ²Ñ‹ÑÑ‚Ğ°Ğ²ĞºĞ°",
    "ĞºĞ¾Ğ½ĞºÑƒÑ€Ñ",
    "competition",
    "Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³",
    "training",
    "Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ",
    "Ğ¸Ğ²ĞµĞ½Ñ‚",
    "event",
    "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµÑ‚",
    "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµĞ¼",
    "Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞ¹ÑÑ",
    "Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ",
]
NOT_EVENT_WORDS = [
    "research",
    "Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾",
    "Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»",
    "Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞº Ñ€Ğ°ÑƒĞ½Ğ´",
    "Ğ¼Ğ»Ğ½ $",
    "Ğ¼Ğ»Ñ€Ğ´ $",
    "Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½",
    "ÑƒĞ²Ğ¾Ğ»ĞµĞ½",
    "Ğ¾Ñ‚Ñ‡ĞµÑ‚",
    "Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°",
    "ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°",
    "Ğ±Ğ¸Ñ€Ğ¶Ğ°",
    "Ğ°ĞºÑ†Ğ¸Ğ¸",
    "Ñ‚Ğ¾ĞºĞ°ĞµĞ²",
    "Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑĞ»Ğ¾",
]
SITE_STOP_WORDS = [
    "ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹",
    "Ğ¾ Ğ½Ğ°Ñ",
    "Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°",
    "Ğ²Ğ¾Ğ¹Ñ‚Ğ¸",
    "Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚Ğ°",
    "Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ",
    "Ğ¿Ğ¾Ğ¸ÑĞº",
    "Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ",
    "Ğ¼ĞµĞ½Ñ",
    "Ğ²ÑĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸",
    "Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»ĞµĞµ",
    "Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ",
    "ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ",
    "privacy",
    "terms",
    "cookie",
]
DESCRIPTION_SIGNALS = [
    "Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ¸",
    "Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ¾Ğ²",
    "Ğ²Ñ‹ ÑƒĞ·Ğ½Ğ°ĞµÑ‚Ğµ",
    "Ğ¼Ñ‹ Ñ€Ğ°ÑÑĞºĞ°Ğ¶ĞµĞ¼",
    "Ğ½Ğ° Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸",
    "Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ…",
    "ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ÑÑ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°",
    "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµĞ¼ Ğ²Ğ°Ñ",
    "Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞ¹Ñ‚ĞµÑÑŒ",
    "Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞµ",
    "ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ",
    "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ÑÑ‚ Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚ÑŒ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğµ",
    "Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ¿ĞµÑ€ĞµĞ¹Ñ‚Ğ¸",
]

KZ_CITIES = {
    "Ğ°Ğ»Ğ¼Ğ°Ñ‚Ñ‹": "ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹",
    "Ğ°ÑÑ‚Ğ°Ğ½Ğ°": "ĞÑÑ‚Ğ°Ğ½Ğ°",
    "ÑˆÑ‹Ğ¼ĞºĞµĞ½Ñ‚": "Ğ¨Ñ‹Ğ¼ĞºĞµĞ½Ñ‚",
    "Ğ½ÑƒÑ€-ÑÑƒĞ»Ñ‚Ğ°Ğ½": "ĞÑÑ‚Ğ°Ğ½Ğ°",
    "ÑƒÑÑ‚ÑŒ-ĞºĞ°Ğ¼ĞµĞ½Ğ¾Ğ³Ğ¾Ñ€ÑĞº": "Ğ£ÑÑ‚ÑŒ-ĞšĞ°Ğ¼ĞµĞ½Ğ¾Ğ³Ğ¾Ñ€ÑĞº",
    "ĞºÑ‹Ğ·Ñ‹Ğ»Ğ¾Ñ€Ğ´Ğ°": "ĞšÑ‹Ğ·Ñ‹Ğ»Ğ¾Ñ€Ğ´Ğ°",
    "Ğ°ĞºÑ‚Ğ¾Ğ±Ğµ": "ĞĞºÑ‚Ğ¾Ğ±Ğµ",
    "Ñ‚Ğ°Ñ€Ğ°Ğ·": "Ğ¢Ğ°Ñ€Ğ°Ğ·",
    "Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ´Ğ°Ñ€": "ĞŸĞ°Ğ²Ğ»Ğ¾Ğ´Ğ°Ñ€",
    "ÑĞµĞ¼ĞµĞ¹": "Ğ¡ĞµĞ¼ĞµĞ¹",
    "Ğ°Ñ‚Ñ‹Ñ€Ğ°Ñƒ": "ĞÑ‚Ñ‹Ñ€Ğ°Ñƒ",
    "Ğ¶ĞµĞ·ĞºĞ°Ğ·Ğ³Ğ°Ğ½": "Ğ–ĞµĞ·Ò›Ğ°Ğ·Ò“Ğ°Ğ½",
    "Ğ¶ĞµĞ·Ò›Ğ°Ğ·Ò“Ğ°Ğ½": "Ğ–ĞµĞ·Ò›Ğ°Ğ·Ò“Ğ°Ğ½",
    "Ğ°ĞºÑ‚Ğ°Ñƒ": "ĞĞºÑ‚Ğ°Ñƒ",
    "Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½": "ĞĞ½Ğ»Ğ°Ğ¹Ğ½",
    "online": "ĞĞ½Ğ»Ğ°Ğ¹Ğ½",
    "zoom": "ĞĞ½Ğ»Ğ°Ğ¹Ğ½ (Zoom)",
    "Ñ‚Ğ°ÑˆĞºĞµĞ½Ñ‚": "Ğ¢Ğ°ÑˆĞºĞµĞ½Ñ‚, Ğ£Ğ·Ğ±ĞµĞºĞ¸ÑÑ‚Ğ°Ğ½",
}

EMOJI_RE = re.compile(
    "[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]",
    re.UNICODE,
)


# â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def parse_date(text: str) -> Optional[datetime]:
    """
    ĞŸĞ°Ñ€ÑĞ¸Ñ‚ Ğ´Ğ°Ñ‚Ñƒ. ĞĞ• Ğ¿Ñ€Ğ¸Ğ±Ğ°Ğ²Ğ»ÑĞµÑ‚ +1 Ğ³Ğ¾Ğ´ Ğº Ğ¿Ñ€Ğ¾ÑˆĞµĞ´ÑˆĞ¸Ğ¼ Ğ´Ğ°Ñ‚Ğ°Ğ¼:
    ĞµÑĞ»Ğ¸ Ğ³Ğ¾Ğ´ Ğ½Ğµ ÑƒĞºĞ°Ğ·Ğ°Ğ½ Ğ¸ Ğ´Ğ°Ñ‚Ğ° Ğ¿Ñ€Ğ¾ÑˆĞ»Ğ° â€” Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ None.
    """
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try:
            return datetime(year, month, day)
        except Exception:
            return None

    # Ğ”Ğ”-Ğ”Ğ” ĞœĞµÑÑÑ† [Ğ“Ğ“Ğ“Ğ“]
    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([Ğ°-ÑÑ‘]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month:
            return make_dt(year, month, int(m.group(2)))

    # Ğ”Ğ” ĞœĞµÑÑÑ† [Ğ“Ğ“Ğ“Ğ“]
    m = re.search(r"(\d{1,2})\s+([Ğ°-ÑÑ‘]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # Ğ”Ğ” ĞœĞµÑ[ÑĞ¾ĞºÑ€] [Ğ“Ğ“Ğ“Ğ“]
    m = re.search(
        r"(\d{1,2})\s+(ÑĞ½Ğ²|Ñ„ĞµĞ²|Ğ¼Ğ°Ñ€|Ğ°Ğ¿Ñ€|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½|Ğ¸ÑĞ»|Ğ°Ğ²Ğ³|ÑĞµĞ½|Ğ¾ĞºÑ‚|Ğ½Ğ¾Ñ|Ğ´ĞµĞº)[Ğ°-Ñ]*(?:\s+(\d{4}))?",
        t,
    )
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # Ğ”Ğ”.ĞœĞœ[.Ğ“Ğ“Ğ“Ğ“]
    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12:
            return make_dt(year, month, int(m.group(1)))

    return None


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: "ÑĞ½Ğ²Ğ°Ñ€Ñ",
        2: "Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ",
        3: "Ğ¼Ğ°Ñ€Ñ‚Ğ°",
        4: "Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        5: "Ğ¼Ğ°Ñ",
        6: "Ğ¸ÑĞ½Ñ",
        7: "Ğ¸ÑĞ»Ñ",
        8: "Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        9: "ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ",
        10: "Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ",
        11: "Ğ½Ğ¾ÑĞ±Ñ€Ñ",
        12: "Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
    }
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s


def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    known = [
        "Narxoz",
        "Nazarbayev",
        "KBTU",
        "ĞšĞ‘Ğ¢Ğ£",
        "Astana Hub",
        "IT Park",
        "MOST IT Hub",
        "Holiday Inn",
        "Esentai",
        "Yandex",
        "Smart Point",
        "Almaty Arena",
    ]
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf"{re.escape(v)}[^\n,.]*", text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at = re.search(r"@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)", text)
    if at:
        return at.group(1).strip()[:60]
    return None


def is_real_event(text: str) -> bool:
    t = text.lower()
    return (any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS))


def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)


def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)


def dedup_title(title: str) -> str:
    """'Data Community BirthdayData Community Birthday' â†’ 'Data Community Birthday'"""
    for i in range(10, len(title) // 2 + 1):
        if title[i:].startswith(title[:i]):
            return title[:i].strip(" .,â€“-")
    return title


def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    # 16:00ĞĞ½Ğ»Ğ°Ğ¹Ğ½ -> 16:00 ĞĞ½Ğ»Ğ°Ğ¹Ğ½
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-zĞ-Ğ¯Ğ°-ÑĞÑ‘])", r"\1 ", s)
    # "Ğ¤ĞµĞ²,16:00" -> "Ğ¤ĞµĞ², 16:00"
    s = re.sub(r"([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)
    # Ğ´Ğ²Ğ¾Ğ¹Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±ĞµĞ»Ñ‹
    s = re.sub(r"\s{2,}", " ", s)
    return s


def strip_leading_datetime_from_title(title: str) -> str:
    """
    ĞĞ° Ğ²ÑÑĞºĞ¸Ğ¹ ÑĞ»ÑƒÑ‡Ğ°Ğ¹: ĞµÑĞ»Ğ¸ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ĞµÑ‚ÑÑ Ñ Ğ´Ğ°Ñ‚Ñ‹/Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸ â€” ÑÑ€ĞµĞ·Ğ°ĞµĞ¼.
    ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
    "24 Ğ¤ĞµĞ², 16:00 ĞĞ½Ğ»Ğ°Ğ¹Ğ½ Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ..." -> "ĞĞ½Ğ»Ğ°Ğ¹Ğ½ Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ..."
    "24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ 16:00Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ..." -> "Ğ’Ğ½ĞµĞ´Ñ€ĞµĞ½Ğ¸Ğµ..."
    """
    t = strip_emoji(title).strip()

    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¸ĞºĞ»ĞµĞµĞ½Ğ½Ğ¾Ğµ Ğ²Ñ€ĞµĞ¼Ñ Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸
    t = normalize_glued_text(t)

    # 24 Ñ„ĞµĞ², 16:00 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[Ğ-Ğ¯Ğ°-ÑĞÑ‘A-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24 Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ 2026 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[Ğ°-ÑÑ‘]{3,}(?:\s+\d{4})?\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24.02.2026 ...
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)

    return t.strip(" -â€“â€¢.,").strip()


def clean_title_deterministic(raw_title: str) -> Optional[str]:
    s = strip_leading_datetime_from_title(raw_title)
    s = dedup_title(s)
    s = re.sub(r"\s{2,}", " ", s).strip()

    if len(s) < 5:
        return None
    if looks_like_description(s):
        return None

    # Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ñ…Ğ²Ğ¾ÑÑ‚ Ğ¿Ğ¾ÑĞ»Ğµ Ğ¼Ğ°Ñ€ĞºĞµÑ€Ğ¾Ğ² Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ
    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx != -1 and idx > 12:
            s = s[:idx].strip(" -â€“â€¢.,")
            break

    s = s.strip()
    if len(s) < 5:
        return None
    return s[:120]


# â”€â”€â”€ Parse glued line: "09 Ğ¤ĞµĞ², 17:00Ğ¨Ñ‹Ğ¼ĞºĞµĞ½Ñ‚ ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_GLUE_RE = re.compile(
    r"^(\d{1,2})\s+"                      # day
    r"([Ğ-Ğ¯ĞĞ°-ÑÑ‘A-Za-z]{3,})"             # month
    r"[,\s]+"
    r"(\d{1,2}:\d{2})"                    # time
    r"\s*"
    r"(?:(ĞĞ½Ğ»Ğ°Ğ¹Ğ½|online|zoom|ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹|ĞÑÑ‚Ğ°Ğ½Ğ°|Ğ¨Ñ‹Ğ¼ĞºĞµĞ½Ñ‚|Ğ–ĞµĞ·ĞºĞ°Ğ·Ğ³Ğ°Ğ½|Ğ–ĞµĞ·Ò›Ğ°Ğ·Ò“Ğ°Ğ½|ĞšĞ°Ñ€Ğ°Ğ³Ğ°Ğ½Ğ´Ğ°|ĞšĞ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ¹|ĞŸĞ°Ğ²Ğ»Ğ¾Ğ´Ğ°Ñ€|Ğ¡ĞµĞ¼ĞµĞ¹|ĞÑ‚Ñ‹Ñ€Ğ°Ñƒ|ĞĞºÑ‚Ğ°Ñƒ|ĞĞºÑ‚Ğ¾Ğ±Ğµ|Ğ¢Ğ°Ñ€Ğ°Ğ·|ĞšÑ‹Ğ·Ñ‹Ğ»Ğ¾Ñ€Ğ´Ğ°)\s*)?"
    r"(.+)$",                             # title rest
    re.IGNORECASE,
)


def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m:
        return None

    day_s, month_s = m.group(1), m.group(2).lower()
    time_str = m.group(3)
    possible_city = (m.group(4) or "").strip()
    title_raw = m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    try:
        dt = datetime(datetime.now().year, month_num, int(day_s))
    except Exception:
        return None

    if not is_future(dt):
        return None

    city = None
    if possible_city:
        city = KZ_CITIES.get(possible_city.lower(), possible_city)

    title_raw = dedup_title(title_raw)

    if len(title_raw) < 5:
        return None

    return {
        "dt": dt,
        "time_str": time_str,
        "city": city,
        "title_raw": title_raw[:300],
        "date_formatted": format_date(dt, time_str),
    }


# â”€â”€â”€ Formatting post (NO date in title) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_post(event: Dict) -> str:
    title = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get("location", "")
    venue = event.get("venue", "")

    # Ğ²Ğ°Ğ¶Ğ½Ğ¾: Ğ´Ğ°Ñ‚Ğ° Ğ² Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞµ ÑƒĞ±Ğ¸Ñ€Ğ°ĞµÑ‚ÑÑ Ğ²ÑĞµĞ³Ğ´Ğ°
    title = strip_leading_datetime_from_title(title)

    lines = [f"ğŸ¯ <b>{title}</b>"]

    if location in ("ĞĞ½Ğ»Ğ°Ğ¹Ğ½", "ĞĞ½Ğ»Ğ°Ğ¹Ğ½ (Zoom)"):
        lines.append("ğŸŒ ĞĞ½Ğ»Ğ°Ğ¹Ğ½")
    elif location:
        lines.append(f"ğŸ‡°ğŸ‡¿ ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½, ğŸ™ {location}")
    else:
        lines.append("ğŸ‡°ğŸ‡¿ ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½")

    if venue:
        lines.append(f"ğŸ“ {venue}")

    lines.append(f"ğŸ“… {date_str}")
    lines.append(f"ğŸ”— <a href='{link}'>Ğ§Ğ¸Ñ‚Ğ°Ñ‚ÑŒ â†’</a>")

    return "\n".join(lines)


# â”€â”€â”€ Bot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={"User-Agent": "Mozilla/5.0"})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    # â”€â”€ Digest parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(
                r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?"
                r"|\d{1,2}\s+(?:ÑĞ½Ğ²|Ñ„ĞµĞ²|Ğ¼Ğ°Ñ€|Ğ°Ğ¿Ñ€|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½|Ğ¸ÑĞ»|Ğ°Ğ²Ğ³|ÑĞµĞ½|Ğ¾ĞºÑ‚|Ğ½Ğ¾Ñ|Ğ´ĞµĞº)[Ğ°-Ñ]*"
                r"(?:\s+\d{4})?)",
                line,
                re.IGNORECASE,
            )
            if not dm:
                i += 1
                continue

            date_raw = dm.group(0)
            rest = line[dm.end():].strip()

            tm = re.search(r"(?:Ğ²\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm:
                rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -â€“â€¢")

            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"):
                    link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith("http"):
                            link = "https://" + link
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt):
                    title_raw = nxt

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                logger.info(f"â­ï¸ ĞŸÑ€Ğ¾ÑˆĞµĞ´ÑˆĞµĞµ (Ğ´Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚): {title_raw[:40]}")
                i += 1
                continue

            ctx = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_location(ctx) or extract_location(text)

            title_clean = clean_title_deterministic(title_raw) or dedup_title(title_raw[:120])
            if not title_clean:
                i += 1
                continue

            events.append(
                {
                    "title": title_clean,
                    "date": format_date(dt, time_str),
                    "location": location or "",
                    "venue": extract_venue(ctx),
                    "link": link or post_link,
                    "source": source,
                    "image_url": image_url,
                }
            )
            i += 1
        return events

    # â”€â”€ Telegram channels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td:
                    continue

                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30:
                    continue

                le = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"
                norm_link = normalize_link(post_link)

# ğŸ”¥ Ğ˜Ğ©Ğ•Ğœ ĞŸĞ•Ğ Ğ’ĞĞ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜Ğš Ğ’ Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ•
                external_link = None
                links_in_text = re.findall(r"(https?://[^\s]+)", text)

                for l in links_in_text:
                    clean_l = normalize_link(l)
                    if "t.me" not in clean_l:   # Ğ¸ÑĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ telegram
                        external_link = clean_l
                        break

# ĞµÑĞ»Ğ¸ Ğ½Ğ°ÑˆĞ»Ğ¸ Ğ²Ğ½ĞµÑˆĞ½Ğ¸Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ ĞµĞ³Ğ¾
                final_link = external_link if external_link else norm_link

                # â— ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ—Ğ”Ğ•Ğ¡Ğ¬
                if norm_link in self.posted:
                    continue
                
                # ğŸ”¥ Ğ˜Ğ©Ğ•Ğœ ĞŸĞ•Ğ Ğ’ĞĞ˜Ğ¡Ğ¢ĞĞ§ĞĞ˜Ğš
                external_link = None

# 1ï¸âƒ£ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¸Ñ‰ĞµĞ¼ <a href=...>
                for a in td.find_all("a", href=True):
                    href = normalize_link(a["href"])
                    if "t.me" not in href:
                        external_link = href
                        break

# 2ï¸âƒ£ ĞµÑĞ»Ğ¸ Ğ½Ğµ Ğ½Ğ°ÑˆĞ»Ğ¸ â€” fallback Ğ½Ğ° regex
                if not external_link:
                    links_in_text = re.findall(r"(https?://[^\s]+)", text)
                    for l in links_in_text:
                        clean_l = normalize_link(l)
                        if "t.me" not in clean_l:
                            external_link = clean_l
                            break

# 3ï¸âƒ£ Ğ²Ñ‹Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ„Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ ÑÑÑ‹Ğ»ĞºÑƒ
                final_link = external_link if external_link else norm_link

                image_url = None

# Telegram photo preview
                photo_wrap = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if photo_wrap:
                    style = photo_wrap.get("style", "")
                    match = re.search(r"url\('([^']+)'\)", style)
                    if match:
                        image_url = match.group(1)

# ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚ â€” Ğ¿Ğ¾Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ img Ğ²Ğ½ÑƒÑ‚Ñ€Ğ¸ Ñ‚ĞµĞºÑÑ‚Ğ°
                if not image_url:
                    img_tag = td.find("img")
                    if img_tag and img_tag.get("src"):
                        image_url = img_tag["src"]

                # Ğ”Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚
                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:Ğ²\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    continue

                if not is_real_event(text):
                    continue

                dt = parse_date(text)
                if not is_future(dt):
                    continue

                # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
                title_candidate = None
                for ln in text.split("\n"):
                    ln = strip_emoji(ln).strip()
                    if len(ln) > 10:
                        title_candidate = ln
                        break

                title = clean_title_deterministic(title_candidate or "")
                if not title:
                    continue

                tm2 = re.search(r"\d{1,2}\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append(
                    {
                        "title": title,
                        "date": format_date(dt, time_str),
                        "location": extract_location(text) or "",
                        "venue": extract_venue(text),
                        "link": final_link,
                        "source": channel["name"],
                        "image_url": image_url,
                    }
                )

            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue

        return all_events
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"ğŸŒ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ {len(URLS)} ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)

        logger.info(f"ğŸ“± ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ {len(TELEGRAM_CHANNELS)} ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)

        return all_events

    # â”€â”€ Sites â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def parse_site(self, site: Dict) -> List[Dict]:

        html = await self.fetch(site["url"])
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        events = []

        for link in soup.find_all("a", href=True)[:80]:
            try:
                href = link.get("href", "")
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue

                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)

                # ğŸ”¥ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ÑÑÑ‹Ğ»ĞºĞ¸
                href = normalize_link(href)

                if href.rstrip("/") == normalize_link(site["url"]).rstrip("/"):
                    continue

                # ğŸ”¥ Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ğ´ÑƒĞ±Ğ»Ñ Ñ‡ĞµÑ€ĞµĞ· Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ ÑÑÑ‹Ğ»ĞºÑƒ
                if href in self.posted:
                    continue

                if is_site_trash(title_raw):
                    continue

                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt):
                    continue

                image_url = None


# Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ img
                img = parent.find("img") if parent else None
                if img:
                    src = img.get("src") or img.get("data-src")
                    if src:
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        image_url = src

# background-image fallback
                if not image_url and parent:
                    style = parent.get("style", "")
                    match = re.search(r"url\(['\"]?([^'\")]+)", style)
                    if match:
                        src = match.group(1)
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        image_url = src

                # â— Ğ’ĞĞ–ĞĞ: ĞĞ• Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ² self.posted Ğ·Ğ´ĞµÑÑŒ
                # Ğ”Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ Ğ¿Ñ€Ğ¾Ğ¸ÑÑ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ Ğ¢ĞĞ›Ğ¬ĞšĞ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ¹ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸

                title_clean = (
                    clean_title_deterministic(title_raw)
                    or strip_emoji(dedup_title(title_raw))[:120]
                )

                events.append(
                    {
                        "title": title_clean,
                        "date": format_date(dt),
                        "location": extract_location(context) or "",
                        "venue": extract_venue(context),
                        "link": href,
                        "source": site["name"],
                        "image_url": image_url,
                    }
                )

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events
        
        


# â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def main():
    logger.info("ğŸš€ Ğ¡Ñ‚Ğ°Ñ€Ñ‚...")

    if not BOT_TOKEN:
        logger.error("âŒ BOT_TOKEN Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½!")
        return

    bot_obj = EventBot()
    bot_api = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"ğŸ“Š Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹: {len(unique)}")
        logger.info(f"ğŸ“¦ Ğ£Ğ¶Ğµ Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾: {len(bot_obj.posted)}")

        posted = 0

        DEFAULT_IMAGE = "https://yourdomain.com/default-event.jpg"

        for event in unique[:15]:

            norm_link = normalize_link(event.get("link", ""))

            # ğŸ”¥ ĞŸĞ ĞĞ’Ğ•Ğ ĞšĞ Ğ”Ğ£Ğ‘Ğ›Ğ¯
            if norm_link in bot_obj.posted:
                logger.info(f"â­ï¸ Ğ£Ğ¶Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ¾ÑÑŒ: {event.get('title')[:50]}")
                continue

            # fallback ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ°
            image_url = event.get("image_url") or DEFAULT_IMAGE

            text = make_post(event)
            if not text:
                continue

            try:
                photo_sent = False

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ ĞºĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºÑƒ Ğ²Ñ€ÑƒÑ‡Ğ½ÑƒÑ â”€â”€â”€â”€â”€â”€â”€â”€â”€
                try:
                    session = await bot_obj.get_session()
                    async with session.get(image_url, timeout=15) as resp:
                        if resp.status == 200:
                            content_type = resp.headers.get("Content-Type", "")
                            if "image" in content_type:
                                photo_bytes = await resp.read()

                                await bot_api.send_photo(
                                    chat_id=CHANNEL_ID,
                                    message_thread_id=MESSAGE_THREAD_ID,
                                    photo=photo_bytes,
                                    caption=text,
                                    parse_mode="HTML",
                                )

                                photo_sent = True
                except Exception as img_error:
                    logger.warning(f"âš ï¸ Ğ¤Ğ¾Ñ‚Ğ¾ Ğ½Ğµ ÑĞºĞ°Ñ‡Ğ°Ğ»Ğ¾ÑÑŒ: {img_error}")

                # â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ğ•ÑĞ»Ğ¸ Ñ„Ğ¾Ñ‚Ğ¾ Ğ½Ğµ ÑƒĞ´Ğ°Ğ»Ğ¾ÑÑŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€
                if not photo_sent:
                    await bot_api.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                # âœ… ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸
                bot_obj.posted.add(norm_link)
                save_posted(bot_obj.posted)

                posted += 1
                logger.info(f"âœ… ({posted}) {event.get('title','')[:50]}")

                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"âŒ send error: {e}")

        logger.info(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! ĞĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾ Ğ½Ğ¾Ğ²Ñ‹Ñ…: {posted}")

    finally:
        await bot_obj.close()


if __name__ == "__main__":
    asyncio.run(main())
