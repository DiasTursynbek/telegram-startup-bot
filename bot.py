import asyncio
import logging
from datetime import datetime, time
from typing import List, Dict
import aiohttp
from bs4 import BeautifulSoup
from telegram import Update, InputMediaPhoto
from telegram.ext import (
    Application,
    CommandHandler,
    ContextTypes,
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# ========== –ù–ê–°–¢–†–û–ô–ö–ò - –ò–ó–ú–ï–ù–ò–¢–ï –ó–î–ï–°–¨ ==========
BOT_TOKEN = "8587519643:AAG-cWoQEV96ABp_dTIR5jDZyjbqjuUxewY"
CHANNEL_ID = "@startup_events_kz"

# –í—Ä–µ–º—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ (–ø–æ UTC)
MORNING_TIME = time(hour=4, minute=0)   # 09:00 –ê–ª–º–∞—Ç—ã
EVENING_TIME = time(hour=13, minute=0)  # 18:00 –ê–ª–º–∞—Ç—ã
#hour=18, #23:00
# ================================================

# –í—Å–µ —Å–∞–π—Ç—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
URLS = [
    # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –∏ –±–∏–∑–Ω–µ—Å-–ø–æ—Ä—Ç–∞–ª—ã
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub", "type": "events"},
    {"url": "https://er10.kz", "name": "ER10.kz", "type": "news"},
    {"url": "https://kapital.kz", "name": "Kapital.kz", "type": "news"},
    {"url": "https://forbes.kz", "name": "Forbes.kz", "type": "news"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv.media", "type": "news"},
    
    # –í–µ–Ω—á—É—Ä–Ω—ã–µ —Ñ–æ–Ω–¥—ã –∏ –∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä—ã
    {"url": "https://ma7.vc", "name": "MA7.vc", "type": "venture"},
    {"url": "https://tumarventures.com", "name": "Tumar Ventures", "type": "venture"},
    {"url": "https://whitehillcapital.io", "name": "White Hill Capital", "type": "venture"},
    {"url": "https://bigsky.vc", "name": "Big Sky Ventures", "type": "venture"},
    {"url": "https://mostfund.vc", "name": "MOST Fund", "type": "venture"},
    
    # –í–µ–Ω—á—É—Ä–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
    {"url": "https://axiomcapital.com", "name": "Axiom Capital", "type": "venture"},
    {"url": "https://jastarventures.com", "name": "Jastar Ventures", "type": "venture"},
    
    # –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS", "type": "university"},
]

# Telegram –∫–∞–Ω–∞–ª—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
TELEGRAM_CHANNELS = [
    # –û—Å–Ω–æ–≤–Ω—ã–µ —Å—Ç–∞—Ä—Ç–∞–ø-–∫–∞–Ω–∞–ª—ã –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞
    {"username": "startup_course_com", "name": "Startup Course"},  # –° —Ñ–æ—Ç–æ
    {"username": "astanahub_events", "name": "Astana Hub Events"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    
    # –°—Ç–∞—Ä—Ç–∞–ø-—Å–æ–æ–±—â–µ—Å—Ç–≤–∞
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "tech_kz", "name": "Tech Kazakhstan"},
    {"username": "startups_kz", "name": "Startups KZ"},
    {"username": "kazakhstartups", "name": "Kazakhstan Startups"},
    
    # –ë–∏–∑–Ω–µ—Å –∏ –∏–Ω–Ω–æ–≤–∞—Ü–∏–∏
    {"username": "innovation_kz", "name": "Innovation KZ"},
    {"username": "business_kz_official", "name": "Business KZ"},
    {"username": "qazaqstartup", "name": "Qazaq Startup"},
    
    # –í–µ–Ω—á—É—Ä –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏
    {"username": "venture_kz", "name": "Venture Kazakhstan"},
    {"username": "investkz", "name": "Invest Kazakhstan"},
    
    # IT –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏
    {"username": "it_kz_official", "name": "IT Kazakhstan"},
    {"username": "devkz", "name": "Dev KZ"},
]


class UniversalParser:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
    
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        self.session = None
        self.posted_events = set()
        
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Å–æ–±—ã—Ç–∏–π
        self.event_keywords = [
            # –†—É—Å—Å–∫–∏–µ
            '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Å–æ–±—ã—Ç–∏–µ', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', '—Ñ–æ—Ä—É–º', '–≤—Å—Ç—Ä–µ—á–∞',
            '—Å—Ç–∞—Ä—Ç–∞–ø', '–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏—è', '–≤—ã—Å—Ç–∞–≤–∫–∞', '–≤–æ—Ä–∫—à–æ–ø', 'workshop',
            'pitch', 'demo day', '—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon', 'meetup', '–º–∏—Ç–∞–ø',
            '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', '–≤–µ–Ω—á—É—Ä', 'startup', '–∫–æ–Ω–∫—É—Ä—Å',
            '—Å–µ–º–∏–Ω–∞—Ä', '–≤–µ–±–∏–Ω–∞—Ä', '—Ç—Ä–µ–Ω–∏–Ω–≥', '–æ–±—É—á–µ–Ω–∏–µ', '–∫—É—Ä—Å',
            # English
            'event', 'conference', 'forum', 'meeting', 'pitch', 'demo',
            'hackathon', 'competition', 'accelerator', 'investment',
            'networking', 'summit', 'workshop', 'webinar', 'training'
        ]
    
    async def get_session(self):
        if self.session is None or self.session.closed:
            self.session = aiohttp.ClientSession(headers=self.headers)
        return self.session
    
    async def close(self):
        if self.session and not self.session.closed:
            await self.session.close()
    
    async def fetch_url(self, url: str) -> str:
        try:
            session = await self.get_session()
            async with session.get(url, timeout=15) as response:
                if response.status == 200:
                    return await response.text()
                return ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
            return ""
    
    async def extract_image_from_page(self, url: str, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø–µ—Ä–≤–æ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            # –ò—â–µ–º Open Graph –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            og_image = soup.find('meta', property='og:image')
            if og_image and og_image.get('content'):
                img_url = og_image['content']
                if not img_url.startswith('http'):
                    from urllib.parse import urljoin
                    img_url = urljoin(url, img_url)
                return img_url
            
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –∫–æ–Ω—Ç–µ–Ω—Ç–µ
            images = soup.find_all('img', src=True)
            for img in images:
                src = img.get('src', '')
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–∫–æ–Ω–∫–∏, –ª–æ–≥–æ—Ç–∏–ø—ã, –º–∞–ª–µ–Ω—å–∫–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                if any(skip in src.lower() for skip in ['icon', 'logo', 'avatar', 'emoji']):
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä—ã –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
                width = img.get('width')
                height = img.get('height')
                if width and height:
                    try:
                        if int(width) < 200 or int(height) < 200:
                            continue
                    except:
                        pass
                
                if not src.startswith('http'):
                    from urllib.parse import urljoin
                    src = urljoin(url, src)
                
                return src
            
        except Exception as e:
            logger.debug(f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
        
        return None
    
    async def parse_astana_hub(self, site: Dict) -> List[Dict]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Astana Hub"""
        html = await self.fetch_url(site['url'])
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        events = []
        
        event_links = soup.find_all('a', href=lambda x: x and '/ru/event/' in x and x != '/ru/event/')
        seen_titles = set()
        
        for link_elem in event_links[:5]:
            try:
                link = link_elem.get('href', '')
                if not link or link == '/ru/event/':
                    continue
                
                if not link.startswith('http'):
                    link = 'https://astanahub.com' + link
                
                title_elem = link_elem.find(['h3', 'h2', 'h4', 'div'])
                title = title_elem.get_text(strip=True) if title_elem else link_elem.get_text(strip=True)
                
                if not title or len(title) < 10 or title in seen_titles:
                    continue
                
                seen_titles.add(title)
                
                date = "–î–∞—Ç–∞ —É—Ç–æ—á–Ω—è–µ—Ç—Å—è"
                parent = link_elem.find_parent()
                if parent:
                    import re
                    date_text = parent.get_text()
                    date_match = re.search(r'(\d{1,2}\s+[–ê-–Ø–∞-—è]+,?\s+\d{2}:\d{2})', date_text)
                    if date_match:
                        date = date_match.group(1)
                
                location = "–û–Ω–ª–∞–π–Ω"
                if parent:
                    location_match = re.search(r'(–ê–ª–º–∞—Ç—ã|–ê—Å—Ç–∞–Ω–∞|–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫|–®—ã–º–∫–µ–Ω—Ç|–û–Ω–ª–∞–π–Ω)', parent.get_text())
                    if location_match:
                        location = location_match.group(1)
                
                # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = None
                img_elem = link_elem.find('img', src=True) or (parent.find('img', src=True) if parent else None)
                if img_elem:
                    image_url = img_elem['src']
                    if not image_url.startswith('http'):
                        image_url = 'https://astanahub.com' + image_url
                
                events.append({
                    'title': title[:200],
                    'date': date,
                    'location': location,
                    'link': link,
                    'source': site['name'],
                    'description': '–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –≤ —Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫–µ',
                    'image_url': image_url
                })
                
            except Exception as e:
                continue
        
        return events
    
    async def parse_generic_site(self, site: Dict) -> List[Dict]:
        """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ª—é–±–æ–≥–æ —Å–∞–π—Ç–∞"""
        html = await self.fetch_url(site['url'])
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        events = []
        
        # –°–¢–†–û–ì–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø - —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ (—Ç–æ —á—Ç–æ —Ç–æ—á–Ω–æ –ù–ï —è–≤–ª—è–µ—Ç—Å—è —Å–æ–±—ã—Ç–∏–µ–º/—Å—Ç–∞—Ä—Ç–∞–ø–æ–º)
        stop_words = [
            # –ù–∞–≤–∏–≥–∞—Ü–∏—è –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            '–ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ', 'portfolio', 'investment portfolio',
            '–æ –Ω–∞—Å', 'about', '–∫–æ–Ω—Ç–∞–∫—Ç—ã', 'contact',
            '–≥–ª–∞–≤–Ω–∞—è', 'home', '–∫–æ–º–∞–Ω–¥–∞', 'team', '–ø–æ–ª–∏—Ç–∏–∫–∞', 'privacy', 'terms',
            '–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', 'agreement', 'copyright',
            '–ø–æ–¥–ø–∏—Å–∫–∞', 'subscription', 'login', 'sign in', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è',
            
            # –§–∏–Ω–∞–Ω—Å—ã –∏ –±–∏—Ä–∂–∏ (–Ω–µ —Å–æ–±—ã—Ç–∏—è)
            '–∫—É—Ä—Å', '–¥–æ–ª–ª–∞—Ä', '—Ç–µ–Ω–≥–µ', '–≤–∞–ª—é—Ç', '–±–∏—Ä–∂–∞', '—Ü–µ–Ω–∞', '–∫–æ—Ç–∏—Ä–æ–≤–∫–∏',
            'exchange rate', 'currency', '–∞–∫—Ü–∏–∏ –∫–æ–º–ø–∞–Ω–∏–π', '–∏–Ω–¥–µ–∫—Å',
            
            # –ü–æ–ª–∏—Ç–∏–∫–∞ –∏ –≥–æ—Å–Ω–æ–≤–æ—Å—Ç–∏ (–Ω–µ —Å—Ç–∞—Ä—Ç–∞–ø—ã)
            '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å –æ–Ω–ª–∞–π–Ω', '–ø–ª–∞—Ç–µ–∂–µ–π', '—Ç–æ–∫–∞–µ–≤', '–Ω–∞–∑–∞—Ä–±–∞–µ–≤',
            '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–º–∏–Ω–∏—Å—Ç—Ä', '–¥–µ–ø—É—Ç–∞—Ç', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç',
            '“õ–∞–∑–∞“õ —Ç—ñ–ª—ñ', '–Ω–∞ –∫–∞–∑–∞—Ö—Å–∫–æ–º', '–ø–æ–≥–æ–¥–∞', '—Å–ø–æ—Ä—Ç',
            '–ø–æ—Å–æ–ª', 'ambassador', '—Ç–æ—Ä–≥–æ–≤–ª—è',
            
            # –®–∞–±–ª–æ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            'research', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ', 'see all', '—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Å–µ',
            '–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏', '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '–∞—Ä—Ö–∏–≤',
            '–ª–µ–Ω—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π', '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ñ–∏–Ω–∞–Ω—Å—ã', '–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–æ'
        ]
        
        # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ
        all_links = soup.find_all('a', href=True)
        
        for link_elem in all_links[:100]:  # –£–≤–µ–ª–∏—á–∏–ª –¥–æ 100
            try:
                href = link_elem.get('href', '')
                
                # –í–ê–ñ–ù–û: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É
                if not href or href in ['/', '#', 'javascript:void(0)', 'javascript:']:
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                if any(nav in href.lower() for nav in ['/about', '/contact', '/team', '/portfolio', '#']):
                    continue
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é —Å—Å—ã–ª–∫—É
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —ç—Ç–æ —Ç–∞ –∂–µ –≥–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
                base_domain = site['url'].rstrip('/')
                if href.rstrip('/') == base_domain:
                    continue
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å—Å—ã–ª–∫–∏
                title = link_elem.get_text(strip=True)
                
                # –ï—Å–ª–∏ —Ç–µ–∫—Å—Ç –∫–æ—Ä–æ—Ç–∫–∏–π, –∏—â–µ–º –≤ —Ä–æ–¥–∏—Ç–µ–ª–µ
                if not title or len(title) < 20:
                    parent = link_elem.find_parent(['article', 'div', 'h1', 'h2', 'h3'])
                    if parent:
                        title = parent.get_text(strip=True)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ –∏ –æ—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–µ
                if not title or len(title) < 15:
                    continue
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                title_lower = title.lower()
                if any(stop in title_lower for stop in stop_words):
                    continue
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –º—É—Å–æ—Ä–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                garbage_patterns = [
                    '–ª–µ–Ω—Ç–∞ –Ω–æ–≤–æ—Å—Ç–µ–π', '—ç–∫–æ–Ω–æ–º–∏–∫–∞', '—Ñ–∏–Ω–∞–Ω—Å—ã', '—Ç–æ–∫–∞–µ–≤', '–ø–æ—Å–æ–ª',
                    '“õ–∞–∑–∞“õ —Ç—ñ–ª—ñ', '–±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç—å', '—Å–æ–≥–ª–∞—à–µ–Ω–∏–µ', '–º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤',
                    '–º–∏–Ω–∏—Å—Ç—Ä', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ', '–ø–∞—Ä–ª–∞–º–µ–Ω—Ç', '–¥–µ–ø—É—Ç–∞—Ç',
                    '–ø–æ–≥–æ–¥–∞', '—Å–ø–æ—Ä—Ç', '–∫—É—Ä—Å –≤–∞–ª—é—Ç', '–±–∏—Ä–∂–∞'
                ]
                
                if any(garbage in title_lower for garbage in garbage_patterns):
                    continue
                
                # –ò—â–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Å—Å—ã–ª–∫–∏
                parent = link_elem.find_parent(['div', 'article', 'section'])
                parent_text = parent.get_text(strip=True).lower() if parent else ""
                combined_text = title_lower + " " + parent_text
                
                # –°–¢–†–û–ì–ê–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–Ø - –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –°–¢–ê–†–¢–ê–ü/–°–û–ë–´–¢–ò–ô–ù–´–• –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
                startup_keywords = [
                    '—Å—Ç–∞—Ä—Ç–∞–ø', 'startup', '—Å—Ç–∞—Ä—Ç–∞–ø–µ—Ä', '–ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å', 'entrepreneur',
                    'pitch', '–ø–∏—Ç—á', 'demo day', '–¥–µ–º–æ –¥–µ–Ω—å', '—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon',
                    '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', 'accelerator', '–∏–Ω–∫—É–±–∞—Ç–æ—Ä', 'incubator',
                    '–∏–Ω–≤–µ—Å—Ç–∏—Ü', 'investment', '–≤–µ–Ω—á—É—Ä', 'venture', '—Ñ–æ–Ω–¥', 'fund',
                    '–±–∏–∑–Ω–µ—Å-–∞–Ω–≥–µ–ª', 'angel investor', '—Ä–∞—É–Ω–¥', 'funding round'
                ]
                
                event_keywords = [
                    '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Å–æ–±—ã—Ç–∏–µ', 'event', 
                    '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'conference', '—Ñ–æ—Ä—É–º', 'forum', 'summit', '—Å–∞–º–º–∏—Ç',
                    '–≤—Å—Ç—Ä–µ—á–∞', '–≤—Å—Ç—Ä–µ—á–∏', 'meeting', 'networking', '–Ω–µ—Ç–≤–æ—Ä–∫–∏–Ω–≥',
                    'workshop', '–≤–æ—Ä–∫—à–æ–ø', '–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å', 'masterclass',
                    'meetup', '–º–∏—Ç–∞–ø', 'meetup', 
                    '–∫–æ–Ω–∫—É—Ä—Å', 'competition', 'contest',
                    '—Å–µ–º–∏–Ω–∞—Ä', 'seminar', 'webinar', '–≤–µ–±–∏–Ω–∞—Ä', 
                    '—Ç—Ä–µ–Ω–∏–Ω–≥', 'training', '–æ–±—É—á–µ–Ω–∏–µ', 'bootcamp', '–±—É—Ç–∫–µ–º–ø'
                ]
                
                # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∫–ª—é—á–µ–≤–æ–µ —Å–ª–æ–≤–æ –∏–∑ —Å—Ç–∞—Ä—Ç–∞–ø –ò–õ–ò —Å–æ–±—ã—Ç–∏–π
                has_startup_keyword = any(keyword in combined_text for keyword in startup_keywords)
                has_event_keyword = any(keyword in combined_text for keyword in event_keywords)
                
                if not (has_startup_keyword or has_event_keyword):
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –ø—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ —Å—Ç–∞—Ç—å—é/—Å–æ–±—ã—Ç–∏–µ (–Ω–µ –≥–ª–∞–≤–Ω–∞—è)
                # –û–±—ã—á–Ω–æ —É —Å—Ç–∞—Ç–µ–π –µ—Å—Ç—å ID, slug –∏–ª–∏ –¥–∞—Ç–∞ –≤ URL
                import re
                is_article_link = any([
                    re.search(r'/\d+/', href),  # –ï—Å—Ç—å —á–∏—Å–ª–æ –≤ URL (ID)
                    re.search(r'/20\d{2}/', href),  # –ï—Å—Ç—å –≥–æ–¥
                    len(href.split('/')) > 4,  # –ì–ª—É–±–æ–∫–∞—è –≤–ª–æ–∂–µ–Ω–Ω–æ—Å—Ç—å
                    re.search(r'/[a-z]+-[a-z]+', href),  # Slug —Å –¥–µ—Ñ–∏—Å–∞–º–∏
                ])
                
                if not is_article_link:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                if any(e['link'] == href for e in events):
                    continue
                
                # –ò—â–µ–º –¥–∞—Ç—É
                date = datetime.now().strftime("%d.%m.%Y")
                date_patterns = [
                    r'(\d{1,2}\s+[–ê-–Ø–∞-—è]+,?\s+\d{2}:\d{2})',
                    r'(\d{1,2}\s+[–ê-–Ø–∞-—è]+\s+\d{4})',
                    r'(\d{2}\.\d{2}\.\d{4})',
                    r'(\d{4}-\d{2}-\d{2})'
                ]
                
                for pattern in date_patterns:
                    date_match = re.search(pattern, combined_text)
                    if date_match:
                        date = date_match.group(1)
                        break
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è
                location = "–û–Ω–ª–∞–π–Ω"
                location_match = re.search(
                    r'(–ê–ª–º–∞—Ç—ã|–ê—Å—Ç–∞–Ω–∞|–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫|–®—ã–º–∫–µ–Ω—Ç|–ö–∞—Ä–∞–≥–∞–Ω–¥–∞|–ê–∫—Ç–æ–±–µ|–¢–∞—Ä–∞–∑|–û–Ω–ª–∞–π–Ω|Online)',
                    combined_text
                )
                if location_match:
                    location = location_match.group(1)
                
                # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = None
                img_elem = link_elem.find('img', src=True)
                if not img_elem and parent:
                    img_elem = parent.find('img', src=True)
                
                if img_elem:
                    image_url = img_elem['src']
                    if not image_url.startswith('http'):
                        from urllib.parse import urljoin
                        image_url = urljoin(site['url'], image_url)
                
                # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞
                description = title[:150] + '...' if len(title) > 150 else title
                
                events.append({
                    'title': title[:200],
                    'date': date,
                    'location': location,
                    'link': href,
                    'source': site['name'],
                    'description': description,
                    'image_url': image_url
                })
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 3 —Å–æ–±—ã—Ç–∏–π —Å –æ–¥–Ω–æ–≥–æ —Å–∞–π—Ç–∞
                if len(events) >= 3:
                    break
                    
            except Exception as e:
                continue
        
        return events
    
    async def parse_telegram_channel(self, channel: Dict, context: ContextTypes.DEFAULT_TYPE) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ Telegram –∫–∞–Ω–∞–ª–∞"""
        try:
            events = []
            username = channel['username']
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ –∫–∞–Ω–∞–ª–∞ (–º–∞–∫—Å–∏–º—É–º 10)
            try:
                # –ü—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–∞–Ω–∞–ª–µ
                chat = await context.bot.get_chat(f"@{username}")
                
                # –¢–µ–ª–µ–≥—Ä–∞–º API –Ω–µ –ø–æ–∑–≤–æ–ª—è–µ—Ç —á–∏—Ç–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é —á—É–∂–∏—Ö –∫–∞–Ω–∞–ª–æ–≤ –Ω–∞–ø—Ä—è–º—É—é —á–µ—Ä–µ–∑ –±–æ—Ç–∞
                # –ù–æ –º—ã –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—É–±–ª–∏—á–Ω—ã–π preview
                logger.info(f"‚úÖ –ö–∞–Ω–∞–ª @{username} –Ω–∞–π–¥–µ–Ω: {chat.title}")
                
                # –ê–õ–¨–¢–ï–†–ù–ê–¢–ò–í–ê: –ø–∞—Ä—Å–∏–º —á–µ—Ä–µ–∑ t.me preview
                preview_url = f"https://t.me/s/{username}"
                html = await self.fetch_url(preview_url)
                
                if not html:
                    return []
                
                soup = BeautifulSoup(html, 'html.parser')
                
                # –ò—â–µ–º –ø–æ—Å—Ç—ã –≤ –∫–∞–Ω–∞–ª–µ
                messages = soup.find_all('div', class_='tgme_widget_message')
                
                for msg in messages[:10]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 –ø–æ—Å—Ç–æ–≤
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
                        text_div = msg.find('div', class_='tgme_widget_message_text')
                        if not text_div:
                            continue
                        
                        text = text_div.get_text(strip=True)
                        
                        if not text or len(text) < 20:
                            continue
                        
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Å—Ç–∞—Ä—Ç–∞–ø/—Å–æ–±—ã—Ç–∏–π–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                        text_lower = text.lower()
                        
                        startup_keywords = [
                            '—Å—Ç–∞—Ä—Ç–∞–ø', 'startup', 'pitch', 'demo day', '—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon',
                            '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', 'accelerator', '–∏–Ω–≤–µ—Å—Ç–∏—Ü', 'investment', '–≤–µ–Ω—á—É—Ä', 'venture'
                        ]
                        
                        event_keywords = [
                            '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '—Å–æ–±—ã—Ç–∏–µ', 'event', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'conference',
                            '—Ñ–æ—Ä—É–º', 'forum', '–≤—Å—Ç—Ä–µ—á–∞', 'meeting', 'workshop', '–≤–æ—Ä–∫—à–æ–ø',
                            'meetup', '–º–∏—Ç–∞–ø', '—Å–µ–º–∏–Ω–∞—Ä', 'webinar', '–≤–µ–±–∏–Ω–∞—Ä'
                        ]
                        
                        has_startup = any(kw in text_lower for kw in startup_keywords)
                        has_event = any(kw in text_lower for kw in event_keywords)
                        
                        if not (has_startup or has_event):
                            continue
                        
                        # –ü–æ–ª—É—á–∞–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –ø–æ—Å—Ç
                        link_elem = msg.find('a', class_='tgme_widget_message_date')
                        post_link = link_elem['href'] if link_elem else f"https://t.me/{username}"
                        
                        # –ò—â–µ–º –¥–∞—Ç—É
                        date_elem = msg.find('time')
                        date = date_elem.get('datetime', datetime.now().strftime("%Y-%m-%d")) if date_elem else datetime.now().strftime("%Y-%m-%d")
                        
                        # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –¥–∞—Ç—É
                        try:
                            from datetime import datetime as dt
                            date_obj = dt.fromisoformat(date.replace('Z', '+00:00'))
                            date = date_obj.strftime("%d.%m.%Y")
                        except:
                            date = datetime.now().strftime("%d.%m.%Y")
                        
                        # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                        image_url = None
                        img_div = msg.find('a', class_='tgme_widget_message_photo_wrap')
                        if img_div:
                            style = img_div.get('style', '')
                            import re
                            img_match = re.search(r"url\('([^']+)'\)", style)
                            if img_match:
                                image_url = img_match.group(1)
                        
                        # –°–æ–∑–¥–∞–µ–º –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ
                        description = text[:150] + '...' if len(text) > 150 else text
                        
                        events.append({
                            'title': text[:100],
                            'date': date,
                            'location': 'Telegram',
                            'link': post_link,
                            'source': channel['name'],
                            'description': description,
                            'image_url': image_url
                        })
                        
                    except Exception as e:
                        logger.debug(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ—Å—Ç–∞: {e}")
                        continue
                
                return events
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å @{username}: {e}")
                return []
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Telegram –∫–∞–Ω–∞–ª–∞ {channel['name']}: {e}")
            return []
    
    async def parse_site(self, site: Dict, context: ContextTypes.DEFAULT_TYPE = None) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞"""
        try:
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è Astana Hub
            if 'astanahub.com' in site['url']:
                return await self.parse_astana_hub(site)
            
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
            return await self.parse_generic_site(site)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {site['name']}: {e}")
            return []
    
    async def get_all_events(self, context: ContextTypes.DEFAULT_TYPE = None) -> List[Dict]:
        """–°–æ–±—Ä–∞—Ç—å —Å–æ–±—ã—Ç–∏—è —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤"""
        all_events = []
        
        total_sources = len(URLS) + len(TELEGRAM_CHANNELS)
        logger.info(f"üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ {total_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ({len(URLS)} —Å–∞–π—Ç–æ–≤ + {len(TELEGRAM_CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤)...")
        
        # –ü–∞—Ä—Å–∏–º –≤—Å–µ —Å–∞–π—Ç—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        tasks = [self.parse_site(site, context) for site in URLS]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for i, result in enumerate(results):
            site_name = URLS[i]['name']
            if isinstance(result, Exception):
                logger.error(f"‚ùå {site_name}: –û—à–∏–±–∫–∞ - {result}")
            else:
                all_events.extend(result)
                if result:
                    logger.info(f"‚úÖ {site_name}: –Ω–∞–π–¥–µ–Ω–æ {len(result)} —Å–æ–±—ã—Ç–∏–π")
                else:
                    logger.info(f"‚ö†Ô∏è {site_name}: —Å–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        # –ü–∞—Ä—Å–∏–º Telegram –∫–∞–Ω–∞–ª—ã
        if context and TELEGRAM_CHANNELS:
            tg_tasks = [self.parse_telegram_channel(channel, context) for channel in TELEGRAM_CHANNELS]
            tg_results = await asyncio.gather(*tg_tasks, return_exceptions=True)
            
            for i, result in enumerate(tg_results):
                channel_name = TELEGRAM_CHANNELS[i]['name']
                if isinstance(result, Exception):
                    logger.error(f"‚ùå TG {channel_name}: –û—à–∏–±–∫–∞ - {result}")
                else:
                    all_events.extend(result)
                    if result:
                        logger.info(f"‚úÖ TG {channel_name}: –Ω–∞–π–¥–µ–Ω–æ {len(result)} —Å–æ–±—ã—Ç–∏–π")
                    else:
                        logger.info(f"‚ö†Ô∏è TG {channel_name}: —Å–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
        
        logger.info(f"üìä –í—Å–µ–≥–æ –Ω–∞–π–¥–µ–Ω–æ: {len(all_events)} —Å–æ–±—ã—Ç–∏–π –∏–∑ {total_sources} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
        
        return all_events


parser = UniversalParser()


async def post_to_channel(context: ContextTypes.DEFAULT_TYPE):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è –≤ –∫–∞–Ω–∞–ª"""
    logger.info("üì¢ –ù–∞—á–∏–Ω–∞—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é —Å–æ–±—ã—Ç–∏–π –≤ –∫–∞–Ω–∞–ª...")
    
    try:
        events = await parser.get_all_events(context)
        
        if not events:
            logger.warning("‚ö†Ô∏è –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–±—ã—Ç–∏–π –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏")
            return
        
        posted_count = 0
        for event in events[:20]:  # –ú–∞–∫—Å–∏–º—É–º 20 —Å–æ–±—ã—Ç–∏–π –∑–∞ —Ä–∞–∑
            event_id = f"{event['title']}_{event['source']}"
            
            if event_id in parser.posted_events:
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞—é: {event['title'][:40]}...")
                continue
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –∫–∞–∫ –Ω–∞ —Å–∫—Ä–∏–Ω—à–æ—Ç–µ
            caption = f"<b>{event['source']}</b>\n\n"
            caption += f"{event['description']}\n\n"
            caption += f"{event['link']}"
            
            try:
                # –ï—Å–ª–∏ –µ—Å—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å —Ñ–æ—Ç–æ
                if event.get('image_url'):
                    await context.bot.send_photo(
                        chat_id=CHANNEL_ID,
                        photo=event['image_url'],
                        caption=caption,
                        parse_mode='HTML'
                    )
                else:
                    # –ï—Å–ª–∏ –Ω–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è - –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç
                    await context.bot.send_message(
                        chat_id=CHANNEL_ID,
                        text=caption,
                        parse_mode='HTML',
                        disable_web_page_preview=False
                    )
                
                parser.posted_events.add(event_id)
                posted_count += 1
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ ({posted_count}): {event['title'][:40]}...")
                
                await asyncio.sleep(3)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –ø–æ—Å—Ç–∞–º–∏
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {e}")
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å –∫–∞—Ä—Ç–∏–Ω–∫–æ–π, –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ—ë
                try:
                    await context.bot.send_message(
                        chat_id=CHANNEL_ID,
                        text=caption,
                        parse_mode='HTML',
                        disable_web_page_preview=False
                    )
                    parser.posted_events.add(event_id)
                    posted_count += 1
                except:
                    pass
        
        if len(parser.posted_events) > 200:
            parser.posted_events = set(list(parser.posted_events)[-100:])
        
        logger.info(f"‚úÖ –ü—É–±–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted_count} —Å–æ–±—ã—Ç–∏–π")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ post_to_channel: {e}")


async def manual_post(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–†—É—á–Ω–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è (–∫–æ–º–∞–Ω–¥–∞ /post)"""
    await update.message.reply_text("üîÑ –ù–∞—á–∏–Ω–∞—é –ø—É–±–ª–∏–∫–∞—Ü–∏—é —Å–æ–±—ã—Ç–∏–π...")
    await post_to_channel(context)
    await update.message.reply_text("‚úÖ –ü—É–±–ª–∏–∫–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")


async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–ö–æ–º–∞–Ω–¥–∞ /start"""
    message = f"""
ü§ñ <b>–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–æ—Ç –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å–æ–±—ã—Ç–∏–π</b>

–Ø –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—É–±–ª–∏–∫—É—é –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è –≤ –∫–∞–Ω–∞–ª {CHANNEL_ID}

üìã <b>–ò—Å—Ç–æ—á–Ω–∏–∫–∏:</b>
üåê –°–∞–π—Ç–æ–≤: {len(URLS)}
‚Ä¢ Astana Hub, ER10.kz, Kapital.kz, Forbes.kz
‚Ä¢ MA7.vc, Tumar Ventures, White Hill Capital
‚Ä¢ Big Sky, MOST Fund, Axiom Capital, Jastar Ventures
‚Ä¢ NURIS –∏ –¥—Ä—É–≥–∏–µ

üì± Telegram –∫–∞–Ω–∞–ª–æ–≤: {len(TELEGRAM_CHANNELS)}
‚Ä¢ Astana Hub Events, Digital Business KZ
‚Ä¢ Startup Almaty, VC Insights KZ –∏ –¥—Ä—É–≥–∏–µ

‚è∞ <b>–†–∞—Å–ø–∏—Å–∞–Ω–∏–µ:</b>
‚Ä¢ –£—Ç—Ä–æ–º –≤ 08:00 (–ê–ª–º–∞—Ç—ã)
‚Ä¢ –í–µ—á–µ—Ä–æ–º –≤ 17:00 (–ê–ª–º–∞—Ç—ã)

<b>–ö–æ–º–∞–Ω–¥—ã:</b>
/post - –†—É—á–Ω–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è
/status - –°—Ç–∞—Ç—É—Å –±–æ—Ç–∞
    """
    
    await update.message.reply_text(message, parse_mode='HTML')


async def status(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """–°—Ç–∞—Ç—É—Å –±–æ—Ç–∞"""
    status_msg = f"""
üìä <b>–°—Ç–∞—Ç—É—Å –±–æ—Ç–∞</b>

‚úÖ –ë–æ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç
üìù –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ —Å–æ–±—ã—Ç–∏–π: {len(parser.posted_events)}
üì¢ –ö–∞–Ω–∞–ª: {CHANNEL_ID}
üåê –°–∞–π—Ç–æ–≤: {len(URLS)}
üì± Telegram –∫–∞–Ω–∞–ª–æ–≤: {len(TELEGRAM_CHANNELS)}

‚è∞ –°–ª–µ–¥—É—é—â–∞—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è:
‚Ä¢ –£—Ç—Ä–æ–º –≤ 08:00 (–ê–ª–º–∞—Ç—ã)
‚Ä¢ –í–µ—á–µ—Ä–æ–º –≤ 17:00 (–ê–ª–º–∞—Ç—ã)
    """
    
    await update.message.reply_text(status_msg, parse_mode='HTML')


def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    application = Application.builder().token(BOT_TOKEN).build()
    
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("post", manual_post))
    application.add_handler(CommandHandler("status", status))
    
    try:
        if application.job_queue is not None:
            application.job_queue.run_daily(
                post_to_channel,
                time=MORNING_TIME,
                name='morning_post'
            )
            
            application.job_queue.run_daily(
                post_to_channel,
                time=EVENING_TIME,
                name='evening_post'
            )
            
            logger.info("üöÄ –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –±–æ—Ç –∑–∞–ø—É—â–µ–Ω!")
            logger.info(f"üì¢ –ö–∞–Ω–∞–ª: {CHANNEL_ID}")
            logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤ + {len(TELEGRAM_CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤")
            logger.info(f"‚è∞ –ü—É–±–ª–∏–∫–∞—Ü–∏–∏: 08:00 –∏ 17:00 (–ê–ª–º–∞—Ç—ã)")
        else:
            logger.warning("‚ö†Ô∏è JobQueue –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
            logger.info("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω –≤ –†–£–ß–ù–û–ú —Ä–µ–∂–∏–º–µ")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞: {e}")
    
    application.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == '__main__':
    try:
        main()
    finally:
        asyncio.run(parser.close())