import os
import asyncio
import logging
import json
from datetime import datetime
from typing import List, Dict, Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re

logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID', "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv('MESSAGE_THREAD_ID', '4'))

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    'ÑĞ½Ğ²Ğ°Ñ€Ñ': 1, 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ': 2, 'Ğ¼Ğ°Ñ€Ñ‚Ğ°': 3, 'Ğ°Ğ¿Ñ€ĞµĞ»Ñ': 4,
    'Ğ¼Ğ°Ñ': 5, 'Ğ¸ÑĞ½Ñ': 6, 'Ğ¸ÑĞ»Ñ': 7, 'Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°': 8,
    'ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ': 9, 'Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ': 10, 'Ğ½Ğ¾ÑĞ±Ñ€Ñ': 11, 'Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ': 12,
}
MONTHS_SHORT = {
    'ÑĞ½Ğ²': 1, 'Ñ„ĞµĞ²': 2, 'Ğ¼Ğ°Ñ€': 3, 'Ğ°Ğ¿Ñ€': 4,
    'Ğ¼Ğ°Ğ¹': 5, 'Ğ¸ÑĞ½': 6, 'Ğ¸ÑĞ»': 7, 'Ğ°Ğ²Ğ³': 8,
    'ÑĞµĞ½': 9, 'Ğ¾ĞºÑ‚': 10, 'Ğ½Ğ¾Ñ': 11, 'Ğ´ĞµĞº': 12,
}

# ĞŸĞ¾ÑÑ‚ Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¢ĞĞ›Ğ¬ĞšĞ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… ÑĞ»Ğ¾Ğ²
EVENT_WORDS = [
    'ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ', 'conference', 'Ñ„Ğ¾Ñ€ÑƒĞ¼', 'forum', 'summit', 'ÑĞ°Ğ¼Ğ¼Ğ¸Ñ‚',
    'meetup', 'Ğ¼Ğ¸Ñ‚Ğ°Ğ¿', 'Ñ…Ğ°ĞºĞ°Ñ‚Ğ¾Ğ½', 'hackathon',
    'Ğ²Ğ¾Ñ€ĞºÑˆĞ¾Ğ¿', 'workshop', 'Ğ¼Ğ°ÑÑ‚ĞµÑ€-ĞºĞ»Ğ°ÑÑ', 'masterclass',
    'Ğ²ĞµĞ±Ğ¸Ğ½Ğ°Ñ€', 'webinar', 'ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€',
    'pitch', 'Ğ¿Ğ¸Ñ‚Ñ‡', 'demo day',
    'Ğ°ĞºÑĞµĞ»ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€', 'accelerator', 'bootcamp', 'Ğ±ÑƒÑ‚ĞºĞµĞ¼Ğ¿',
    'Ğ²Ñ‹ÑÑ‚Ğ°Ğ²ĞºĞ°', 'ĞºĞ¾Ğ½ĞºÑƒÑ€Ñ', 'competition',
    'Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³', 'training',
    'Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ', 'Ğ¸Ğ²ĞµĞ½Ñ‚', 'event',
    'Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµÑ‚', 'Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµĞ¼', 'Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞ¹ÑÑ', 'Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ',
]

# ĞŸĞ¾ÑÑ‚ Ğ’Ğ«Ğ‘Ğ ĞĞ¡Ğ«Ğ’ĞĞ•Ğœ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ Ñ…Ğ¾Ñ‚Ñ Ğ±Ñ‹ Ğ¾Ğ´Ğ½Ğ¾ Ğ¸Ğ· ÑÑ‚Ğ¸Ñ… ÑĞ»Ğ¾Ğ²
NOT_EVENT_WORDS = [
    'research', 'Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾', 'Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»', 'Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞº Ñ€Ğ°ÑƒĞ½Ğ´',
    'Ğ¼Ğ»Ğ½ $', 'Ğ¼Ğ»Ñ€Ğ´ $', 'Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½', 'ÑƒĞ²Ğ¾Ğ»ĞµĞ½', 'Ğ¾Ñ‚Ñ‡ĞµÑ‚', 'Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°',
    'ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°', 'Ğ±Ğ¸Ñ€Ğ¶Ğ°', 'Ğ°ĞºÑ†Ğ¸Ğ¸', 'Ñ‚Ğ¾ĞºĞ°ĞµĞ²', 'Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑĞ»Ğ¾',
]

# ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ÑƒÑĞ¾Ñ€ Ñ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²
SITE_STOP_WORDS = [
    'ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹', 'Ğ¾ Ğ½Ğ°Ñ', 'Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°', 'Ğ²Ğ¾Ğ¹Ñ‚Ğ¸', 'Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚Ğ°',
    'Ğ¿Ğ¾Ğ´Ğ¿Ğ¸ÑĞ°Ñ‚ÑŒÑÑ', 'Ğ¿Ğ¾Ğ¸ÑĞº', 'Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ', 'Ğ¼ĞµĞ½Ñ', 'Ğ²ÑĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸',
    'Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»ĞµĞµ', 'Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ', 'ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ', 'privacy', 'terms', 'cookie',
]

KZ_CITIES = {
    'Ğ°Ğ»Ğ¼Ğ°Ñ‚Ñ‹': 'ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹', 'Ğ°ÑÑ‚Ğ°Ğ½Ğ°': 'ĞÑÑ‚Ğ°Ğ½Ğ°', 'ÑˆÑ‹Ğ¼ĞºĞµĞ½Ñ‚': 'Ğ¨Ñ‹Ğ¼ĞºĞµĞ½Ñ‚',
    'Ğ½ÑƒÑ€-ÑÑƒĞ»Ñ‚Ğ°Ğ½': 'ĞÑÑ‚Ğ°Ğ½Ğ°', 'ÑƒÑÑ‚ÑŒ-ĞºĞ°Ğ¼ĞµĞ½Ğ¾Ğ³Ğ¾Ñ€ÑĞº': 'Ğ£ÑÑ‚ÑŒ-ĞšĞ°Ğ¼ĞµĞ½Ğ¾Ğ³Ğ¾Ñ€ÑĞº',
    'ĞºÑ‹Ğ·Ñ‹Ğ»Ğ¾Ñ€Ğ´Ğ°': 'ĞšÑ‹Ğ·Ñ‹Ğ»Ğ¾Ñ€Ğ´Ğ°', 'Ğ°ĞºÑ‚Ğ¾Ğ±Ğµ': 'ĞĞºÑ‚Ğ¾Ğ±Ğµ', 'Ñ‚Ğ°Ñ€Ğ°Ğ·': 'Ğ¢Ğ°Ñ€Ğ°Ğ·',
    'Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ´Ğ°Ñ€': 'ĞŸĞ°Ğ²Ğ»Ğ¾Ğ´Ğ°Ñ€', 'ÑĞµĞ¼ĞµĞ¹': 'Ğ¡ĞµĞ¼ĞµĞ¹', 'Ğ°Ñ‚Ñ‹Ñ€Ğ°Ñƒ': 'ĞÑ‚Ñ‹Ñ€Ğ°Ñƒ',
    'Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½': 'ĞĞ½Ğ»Ğ°Ğ¹Ğ½', 'online': 'ĞĞ½Ğ»Ğ°Ğ¹Ğ½', 'zoom': 'ĞĞ½Ğ»Ğ°Ğ¹Ğ½ (Zoom)',
    'Ñ‚Ğ°ÑˆĞºĞµĞ½Ñ‚': 'Ğ¢Ğ°ÑˆĞºĞµĞ½Ñ‚, Ğ£Ğ·Ğ±ĞµĞºĞ¸ÑÑ‚Ğ°Ğ½',
}

# Ğ­Ğ¼Ğ¾Ğ´Ğ·Ğ¸-Ñ€ĞµĞ³ÑƒĞ»ÑÑ€ĞºĞ° â€” ĞĞ• Ñ‚Ñ€Ğ¾Ğ³Ğ°ĞµÑ‚ ĞºĞ¸Ñ€Ğ¸Ğ»Ğ»Ğ¸Ñ†Ñƒ
EMOJI_RE = re.compile(
    '[\U00010000-\U0010ffff'
    '\u2600-\u27ff'
    '\u2300-\u23ff'
    '\u25a0-\u25ff'
    '\u2B00-\u2BFF'
    ']',
    re.UNICODE
)


def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub('', s).strip()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ”ĞĞ¢Ğ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_date(text: str) -> Optional[datetime]:
    t = text.lower()
    try:
        m = re.search(r'(\d{1,2})[-](\d{1,2})\s+([Ğ°-Ñ]+)(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(2))
            month = MONTHS_RU.get(m.group(3), 0)
            year = int(m.group(4)) if m.group(4) else datetime.now().year
            if month:
                return datetime(year, month, day)

        m = re.search(r'(\d{1,2})\s+([Ğ°-Ñ]+)(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = MONTHS_RU.get(m.group(2), 0)
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if month:
                dt = datetime(year, month, day)
                if not m.group(3) and dt.date() < datetime.now().date():
                    dt = datetime(year + 1, month, day)
                return dt

        m = re.search(r'(\d{1,2})\s+(ÑĞ½Ğ²|Ñ„ĞµĞ²|Ğ¼Ğ°Ñ€|Ğ°Ğ¿Ñ€|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½|Ğ¸ÑĞ»|Ğ°Ğ²Ğ³|ÑĞµĞ½|Ğ¾ĞºÑ‚|Ğ½Ğ¾Ñ|Ğ´ĞµĞº)[Ğ°-Ñ]*(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = MONTHS_SHORT.get(m.group(2)[:3], 0)
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if month:
                dt = datetime(year, month, day)
                if not m.group(3) and dt.date() < datetime.now().date():
                    dt = datetime(year + 1, month, day)
                return dt

        m = re.search(r'(\d{1,2})\.(\d{2})(?:\.(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = int(m.group(2))
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if 1 <= month <= 12:
                return datetime(year, month, day)
    except Exception:
        pass
    return None


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: 'ÑĞ½Ğ²Ğ°Ñ€Ñ', 2: 'Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ', 3: 'Ğ¼Ğ°Ñ€Ñ‚Ğ°', 4: 'Ğ°Ğ¿Ñ€ĞµĞ»Ñ',
        5: 'Ğ¼Ğ°Ñ', 6: 'Ğ¸ÑĞ½Ñ', 7: 'Ğ¸ÑĞ»Ñ', 8: 'Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°',
        9: 'ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ', 10: 'Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ', 11: 'Ğ½Ğ¾ÑĞ±Ñ€Ñ', 12: 'Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ',
    }
    result = f"{dt.day} {months[dt.month]} {dt.year}"
    if time_str:
        result += f", {time_str}"
    return result


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞœĞ•Ğ¡Ğ¢Ğ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    venues = [
        'Narxoz', 'Nazarbayev', 'KBTU', 'KĞ‘Ğ¢Ğ£', 'Astana Hub',
        'IT Park', 'MOST IT Hub', 'Holiday Inn', 'Esentai',
        'Yandex', 'Smart Point', 'Almaty Arena',
    ]
    for v in venues:
        if v.lower() in text.lower():
            m = re.search(rf'{v}[^\n,.]*', text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', text)
    if at_m:
        return at_m.group(1).strip()[:60]
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ—ĞĞ“ĞĞ›ĞĞ’ĞĞš
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_title(text: str) -> Optional[str]:
    lines = text.strip().split('\n')
    for line in lines:
        clean = strip_emoji(line).strip(' -\u2013\u2022\xb7.,')
        clean = re.sub(r'\s+', ' ', clean).strip()
        if len(clean) < 10:
            continue
        if 't.me/' in clean or 'http' in clean:
            continue

        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ñƒ+Ğ²Ñ€ĞµĞ¼Ñ+Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ ÑÑ‚Ñ€Ğ¾ĞºĞ¸:
        # ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹: "20 Ğ¤ĞµĞ², 15:00ĞĞºÑ‚Ğ°Ğ¹..." / "23 Ğ¤ĞµĞ², 17:30ĞšĞ¾ĞºÑˆĞµÑ‚Ğ°Ğ¹..."
        clean = re.sub(
            r'^\d{1,2}\s+[Ğ-Ğ¯ĞĞ°-ÑÑ‘]{3,}[,.]?\s*\d{1,2}:\d{2}\s*[Ğ-Ğ¯ĞA-Za-z\-]*\s*',
            '', clean
        ).strip()
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ "20 Ğ¤ĞµĞ², ..." Ğ±ĞµĞ· Ğ²Ñ€ĞµĞ¼ĞµĞ½Ğ¸
        clean = re.sub(
            r'^\d{1,2}\s+[Ğ-Ğ¯ĞĞ°-ÑÑ‘]{3,}[,.]?\s*[Ğ-Ğ¯ĞA-Za-z\-]*\s*',
            '', clean
        ).strip()
        # Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸: "Ğ¢ĞµĞºÑÑ‚ Ñ‚ĞµĞºÑÑ‚" ĞºĞ¾Ğ³Ğ´Ğ° Ğ¿ĞµÑ€Ğ²Ğ°Ñ Ñ‡Ğ°ÑÑ‚ÑŒ == Ğ²Ñ‚Ğ¾Ñ€Ğ°Ñ
        half = len(clean) // 2
        if half > 20 and clean[:half].strip() == clean[half:half*2].strip():
            clean = clean[:half].strip()

        # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ½Ğ°Ñ‡Ğ¸Ğ½Ğ°ÑÑ‰Ğ¸ĞµÑÑ Ñ Ñ†Ğ¸Ñ„Ñ€Ñ‹ (Ğ´Ğ°Ñ‚Ğ°)
        if re.match(r'^\d{1,2}[.\-:\s]', clean):
            continue

        # ĞŸÑ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ñ…Ğ¾Ğ¶Ğµ Ğ½Ğ° Ğ¸Ğ¼Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ° (Ğ˜Ğ¼Ñ Ğ¤Ğ°Ğ¼Ğ¸Ğ»Ğ¸Ñ Ğ±ĞµĞ· Ğ´Ñ€ÑƒĞ³Ğ¸Ñ… ÑĞ»Ğ¾Ğ²)
        if re.match(r'^[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+\s+[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+$', clean):
            continue

        if len(clean) < 10:
            continue
        return clean[:120]
    return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ¤Ğ˜Ğ›Ğ¬Ğ¢Ğ Ğ«
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def is_real_event(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS)


def is_site_trash(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in SITE_STOP_WORDS)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞŸĞĞœĞ¯Ğ¢Ğ¬ ĞĞ¨Ğ˜Ğ‘ĞĞš
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ERRORS_FILE = "grammar_errors.json"

def load_error_memory() -> List[str]:
    """Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ¸Ğ· Ñ„Ğ°Ğ¹Ğ»Ğ°."""
    if os.path.exists(ERRORS_FILE):
        try:
            with open(ERRORS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception:
            pass
    return []

def save_error_memory(errors: List[str]):
    """Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¸ÑÑ‚Ğ¾Ñ€Ğ¸Ñ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº (Ğ½Ğµ Ğ±Ğ¾Ğ»ĞµĞµ 50 Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ñ…)."""
    try:
        with open(ERRORS_FILE, 'w', encoding='utf-8') as f:
            json.dump(errors[-50:], f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸: {e}")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# AI Ğ˜Ğ¡ĞŸĞ ĞĞ’Ğ›Ğ•ĞĞ˜Ğ• Ğ¢Ğ•ĞšĞ¡Ğ¢Ğ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY', '')

async def ai_fix_text(title: str, session: aiohttp.ClientSession) -> tuple[str, List[str]]:
    """
    Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµÑ‚ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ñ‡ĞµÑ€ĞµĞ· Claude API.
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ (Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚, ÑĞ¿Ğ¸ÑĞ¾Ğº Ğ½Ğ¾Ğ²Ñ‹Ñ… Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ´Ğ»Ñ Ğ¿Ğ°Ğ¼ÑÑ‚Ğ¸).
    Ğ•ÑĞ»Ğ¸ API Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿ĞµĞ½ â€” Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ».
    """
    if not CLAUDE_API_KEY:
        return title, []

    error_memory = load_error_memory()
    memory_block = ""
    if error_memory:
        memory_block = "\n\nĞ Ğ°Ğ½ĞµĞµ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ (Ğ½Ğµ Ğ¿Ğ¾Ğ²Ñ‚Ğ¾Ñ€ÑĞ¹ Ğ¸Ñ…):\n" + "\n".join(
            f"- {e}" for e in error_memory[-20:]
        )

    prompt = f"""Ğ˜ÑĞ¿Ñ€Ğ°Ğ²ÑŒ Ğ³Ñ€Ğ°Ğ¼Ğ¼Ğ°Ñ‚Ğ¸ĞºÑƒ Ğ¸ Ğ¿ÑƒĞ½ĞºÑ‚ÑƒĞ°Ñ†Ğ¸Ñ Ğ² Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸. ĞŸÑ€Ğ°Ğ²Ğ¸Ğ»Ğ°:
1. Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ â€” Ğ½Ğµ Ğ¿ĞµÑ€ĞµÑ„Ñ€Ğ°Ğ·Ğ¸Ñ€ÑƒĞ¹
2. Ğ Ğ°ÑÑÑ‚Ğ°Ğ²ÑŒ Ğ·Ğ°Ğ¿ÑÑ‚Ñ‹Ğµ Ğ³Ğ´Ğµ Ğ½ÑƒĞ¶Ğ½Ğ¾
3. Ğ—Ğ°Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ğ±ÑƒĞºĞ²Ñ‹ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ
4. Ğ’ĞµÑ€Ğ½Ğ¸ JSON: {{"fixed": "Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚", "errors": ["Ğ¾ÑˆĞ¸Ğ±ĞºĞ°1", "Ğ¾ÑˆĞ¸Ğ±ĞºĞ°2"]}}
5. Ğ•ÑĞ»Ğ¸ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº Ğ½ĞµÑ‚ â€” Ğ²ĞµÑ€Ğ½Ğ¸ Ñ‚Ğ¾Ñ‚ Ğ¶Ğµ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ ÑĞ¿Ğ¸ÑĞ¾Ğº errors{memory_block}

Ğ—Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº: {title}"""

    try:
        async with session.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": CLAUDE_API_KEY,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json",
            },
            json={
                "model": "claude-haiku-4-5-20251001",
                "max_tokens": 200,
                "messages": [{"role": "user", "content": prompt}]
            },
            timeout=10
        ) as resp:
            if resp.status != 200:
                return title, []
            data = await resp.json()
            raw = data['content'][0]['text'].strip()
            raw = re.sub(r'^```json|```$', '', raw, flags=re.MULTILINE).strip()
            result = json.loads(raw)
            fixed = result.get('fixed', title)
            new_errors = result.get('errors', [])

            # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸ Ğ² Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒ
            if new_errors:
                memory = load_error_memory()
                memory.extend(new_errors)
                save_error_memory(memory)
                logger.info(f"ğŸ“ Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¾ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº: {len(new_errors)} â€” {new_errors}")

            return fixed, new_errors
    except Exception as e:
        logger.warning(f"AI ĞºĞ¾Ñ€Ñ€ĞµĞºÑ†Ğ¸Ñ Ğ½ĞµĞ´Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ğ°: {e}")
        return title, []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ĞŸĞĞ¡Ğ¢
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_post(event: Dict) -> str:
    """
    Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¿Ğ¾ÑÑ‚Ğ° (4â€“5 ÑÑ‚Ñ€Ğ¾Ğº):
    ğŸ¯ Ğ§Ñ‚Ğ¾: <Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ>
    ğŸŒ Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ° / ğŸ™ Ğ“Ğ¾Ñ€Ğ¾Ğ´ / ğŸ“ ĞœĞµÑÑ‚Ğ¾
    ğŸ“… Ğ”Ğ°Ñ‚Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ
    ğŸ”— Ğ¡ÑÑ‹Ğ»ĞºĞ° Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»
    """
    title = (event.get('title') or '').strip()
    if not title or len(title) < 5:
        return ""
    if not event.get('date'):
        return ""

    location = event.get('location', '')
    venue = event.get('venue', '')

    # Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ° 1 â€” Ñ‡Ñ‚Ğ¾ Ğ¿Ñ€Ğ¾Ñ…Ğ¾Ğ´Ğ¸Ñ‚
    lines = [f"ğŸ¯ <b>{title}</b>"]

    # Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ° 2 â€” ÑÑ‚Ñ€Ğ°Ğ½Ğ° + Ğ³Ğ¾Ñ€Ğ¾Ğ´
    if location in ('ĞĞ½Ğ»Ğ°Ğ¹Ğ½', 'ĞĞ½Ğ»Ğ°Ğ¹Ğ½ (Zoom)'):
        lines.append(f"ğŸŒ ĞĞ½Ğ»Ğ°Ğ¹Ğ½")
    elif location:
        lines.append(f"ğŸ‡°ğŸ‡¿ ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½, ğŸ™ {location}")
    else:
        lines.append(f"ğŸ‡°ğŸ‡¿ ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½")

    # Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ° 3 â€” Ğ¼ĞµÑÑ‚Ğ¾ (ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ)
    if venue:
        lines.append(f"ğŸ“ {venue}")

    # Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ° 4 â€” Ğ´Ğ°Ñ‚Ğ° Ğ¸ Ğ²Ñ€ĞµĞ¼Ñ
    lines.append(f"ğŸ“… {event['date']}")

    # Ğ¡Ñ‚Ñ€Ğ¾ĞºĞ° 5 â€” ÑÑÑ‹Ğ»ĞºĞ° Ğ½Ğ° Ğ¾Ñ€Ğ¸Ğ³Ğ¸Ğ½Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚ÑŒ
    lines.append(f"ğŸ”— <a href='{event['link']}'>Ğ§Ğ¸Ñ‚Ğ°Ñ‚ÑŒ â†’</a>")

    return "\n".join(lines)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ğ‘ĞĞ¢
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = set()

    async def get_session(self):
        if not self.session:
            self.session = aiohttp.ClientSession(headers={'User-Agent': 'Mozilla/5.0'})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            session = await self.get_session()
            async with session.get(url, timeout=15) as resp:
                return await resp.text() if resp.status == 200 else ""
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {url} - {e}")
            return ""

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            date_match = re.match(
                r'^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?'
                r'|\d{1,2}\s+(?:ÑĞ½Ğ²|Ñ„ĞµĞ²|Ğ¼Ğ°Ñ€|Ğ°Ğ¿Ñ€|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½|Ğ¸ÑĞ»|Ğ°Ğ²Ğ³|ÑĞµĞ½|Ğ¾ĞºÑ‚|Ğ½Ğ¾Ñ|Ğ´ĞµĞº)[Ğ°-Ñ]*'
                r'(?:\s+\d{4})?)',
                line, re.IGNORECASE
            )
            if not date_match:
                i += 1
                continue

            date_raw = date_match.group(0)
            rest = line[date_match.end():].strip()

            time_match = re.search(r'(?:Ğ²\s*)?(\d{1,2}:\d{2})', rest)
            time_str = time_match.group(1) if time_match else None
            if time_match:
                rest = (rest[:time_match.start()] + rest[time_match.end():]).strip()

            title_raw = strip_emoji(rest).strip(' -\u2013\u2022')

            link = None
            lm = re.search(r'((?:https?://|t\.me/)\S+)', line)
            if lm:
                link = lm.group(1)
                if not link.startswith('http'):
                    link = 'https://' + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), '').strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm = re.search(r'((?:https?://|t\.me/)\S+)', lines[j])
                    if lm:
                        link = lm.group(1)
                        if not link.startswith('http'):
                            link = 'https://' + link
                        break

            venue = None
            at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', title_raw)
            if at_m:
                venue = at_m.group(1).strip()
                title_raw = title_raw[:at_m.start()].strip()

            if len(title_raw) < 5 and i + 1 < len(lines):
                next_line = strip_emoji(lines[i + 1]).strip()
                if len(next_line) > 5 and not re.match(r'^\d', next_line):
                    title_raw = next_line

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                logger.info(f"\u23ed\ufe0f ĞŸÑ€Ğ¾ÑˆĞµĞ´ÑˆĞµĞµ: {title_raw[:40]} ({date_raw})")
                i += 1
                continue

            context = line + ' ' + (lines[i + 1] if i + 1 < len(lines) else '')
            location = extract_location(context) or extract_location(text)
            if not venue:
                venue = extract_venue(context)

            events.append({
                'title': title_raw[:120],
                'date': format_date(dt, time_str),
                'location': location or '',
                'venue': venue,
                'link': link or post_link,
                'source': source,
                'image_url': image_url,
            })
            i += 1
        return events

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        url = f"https://t.me/s/{channel['username']}"
        html = await self.fetch(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        all_events = []

        for msg in soup.find_all('div', class_='tgme_widget_message')[:20]:
            try:
                text_div = msg.find('div', class_='tgme_widget_message_text')
                if not text_div:
                    continue

                text = text_div.get_text(separator='\n', strip=True)
                if len(text) < 30:
                    continue

                link_elem = msg.find('a', class_='tgme_widget_message_date')
                post_link = link_elem['href'] if link_elem else f"https://t.me/{channel['username']}"

                if post_link in self.posted:
                    continue
                self.posted.add(post_link)

                image_url = None
                img_div = msg.find('a', class_='tgme_widget_message_photo_wrap')
                if img_div:
                    style = img_div.get('style', '')
                    m = re.search(r"url\('([^']+)'\)", style)
                    if m:
                        image_url = m.group(1)

                is_digest = bool(re.search(
                    r'\d{1,2}[.\-]\d{2}\s+(?:Ğ²\s+)?\d{1,2}:\d{2}', text
                ))

                if is_digest:
                    events = self.parse_digest(text, post_link, channel['name'], image_url)
                    all_events.extend(events)
                    logger.info(f"\U0001f4cb Ğ”Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚ {channel['name']}: {len(events)} ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹")
                    continue

                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ 1: ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ ÑĞ»Ğ¾Ğ²Ğ°
                if not is_real_event(text):
                    logger.info(f"\u23ed\ufe0f ĞĞµ Ğ¸Ğ²ĞµĞ½Ñ‚: {text[:50].strip()}")
                    continue

                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ 2: Ğ´Ğ°Ñ‚Ğ° Ğ² Ğ±ÑƒĞ´ÑƒÑ‰ĞµĞ¼
                dt = parse_date(text)
                if not is_future(dt):
                    logger.info(f"\u23ed\ufe0f {'ĞŸÑ€Ğ¾ÑˆĞµĞ´ÑˆĞµĞµ' if dt else 'ĞĞµÑ‚ Ğ´Ğ°Ñ‚Ñ‹'}: {text[:50].strip()}")
                    continue

                # Ğ¤Ğ¸Ğ»ÑŒÑ‚Ñ€ 3: Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾ Ğ¾Ñ‚ Ğ´Ğ°Ñ‚Ñ‹
                title = extract_title(text)
                if not title:
                    logger.info(f"\u23ed\ufe0f ĞĞµÑ‚ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°: {text[:50].strip()}")
                    continue
                # Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº â€” Ğ¸Ğ¼Ñ Ğ°Ğ²Ñ‚Ğ¾Ñ€Ğ° (Ğ˜Ğ¼Ñ Ğ¤Ğ°Ğ¼Ğ¸Ğ»Ğ¸Ñ), Ğ¸Ñ‰ĞµĞ¼ Ñ‚ĞµĞ¼Ñƒ Ğ² ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ñ… ÑÑ‚Ñ€Ğ¾ĞºĞ°Ñ…
                if re.match(r'^[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+\s+[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+$', title.strip()):
                    lines_all = text.strip().split('\n')
                    title = None
                    for ln in lines_all[1:]:
                        ln_c = strip_emoji(ln).strip()
                        if len(ln_c) > 15 and not re.match(r'^\d', ln_c) and 'http' not in ln_c:
                            if not re.match(r'^[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+\s+[Ğ-Ğ¯Ğ][Ğ°-ÑÑ‘]+$', ln_c):
                                title = ln_c[:120]
                                break
                if not title:
                    logger.info(f"\u23ed\ufe0f Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ°Ğ²Ñ‚Ğ¾Ñ€, Ğ½ĞµÑ‚ Ñ‚ĞµĞ¼Ñ‹: {text[:50].strip()}")
                    continue

                time_m = re.search(r'(?:Ğ²\s+)(\d{1,2}:\d{2})', text)
                time_str = time_m.group(1) if time_m else None

                all_events.append({
                    'title': title,
                    'date': format_date(dt, time_str),
                    'location': extract_location(text) or '',
                    'venue': extract_venue(text),
                    'link': post_link,
                    'source': channel['name'],
                    'image_url': image_url,
                })

            except Exception as e:
                logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ°: {e}")
                continue

        return all_events

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site['url'])
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        events = []

        for link in soup.find_all('a', href=True)[:80]:
            try:
                href = link.get('href', '')
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                if href.rstrip('/') == site['url'].rstrip('/'):
                    continue
                if href in self.posted:
                    continue

                # ĞĞ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ Ğ¼ÑƒÑĞ¾Ñ€
                if is_site_trash(title_raw):
                    continue

                # Ğ˜Ğ²ĞµĞ½Ñ‚-ÑĞ»Ğ¾Ğ²Ğ°
                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(['div', 'article', 'li', 'section'])
                context = parent.get_text(separator=' ', strip=True) if parent else title_raw
                dt = parse_date(context)

                # Ğ¢Ğ¾Ğ»ÑŒĞºĞ¾ Ğ±ÑƒĞ´ÑƒÑ‰Ğ¸Ğµ
                if not is_future(dt):
                    continue

                location = extract_location(context) or ''
                venue = extract_venue(context)

                image_url = None
                img = (link.find('img', src=True) or
                       (parent.find('img', src=True) if parent else None))
                if img:
                    src = img.get('src', '')
                    if src and not src.startswith('http'):
                        from urllib.parse import urljoin
                        src = urljoin(site['url'], src)
                    image_url = src or None

                self.posted.add(href)
                events.append({
                    'title': strip_emoji(title_raw)[:120],
                    'date': format_date(dt),
                    'location': location,
                    'venue': venue,
                    'link': href,
                    'source': site['name'],
                    'image_url': image_url,
                })

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events

    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"\U0001f310 ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ {len(URLS)} ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²...")
        for site in URLS:
            events = await self.parse_site(site)
            all_events.extend(events)
            if events:
                logger.info(f"\u2705 {site['name']}: {len(events)}")

        logger.info(f"\U0001f4f1 ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ {len(TELEGRAM_CHANNELS)} Telegram ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²...")
        for channel in TELEGRAM_CHANNELS:
            events = await self.parse_channel(channel)
            all_events.extend(events)
            if events:
                logger.info(f"\u2705 {channel['name']}: {len(events)}")

        return all_events


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# MAIN
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main():
    logger.info("\U0001f680 Ğ¡Ñ‚Ğ°Ñ€Ñ‚...")
    if not BOT_TOKEN:
        logger.error("\u274c BOT_TOKEN Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½!")
        return

    bot_obj = EventBot()
    bot = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = e['title'][:40].lower()
            if key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"\U0001f4ca Ğ£Ğ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹: {len(unique)}")

        posted = 0
        for event in unique[:15]:
            # AI Ğ¸ÑĞ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° (Ñ Ğ¿Ğ°Ğ¼ÑÑ‚ÑŒÑ Ğ¾ÑˆĞ¸Ğ±Ğ¾Ğº)
            if CLAUDE_API_KEY:
                fixed_title, _ = await ai_fix_text(event['title'], bot_obj.session or await bot_obj.get_session())
                event['title'] = fixed_title

            text = make_post(event)
            if not text:
                continue
            try:
                if event.get('image_url'):
                    try:
                        await bot.send_photo(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            photo=event['image_url'],
                            caption=text,
                            parse_mode='HTML'
                        )
                    except Exception:
                        await bot.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode='HTML',
                            disable_web_page_preview=True
                        )
                else:
                    await bot.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode='HTML',
                        disable_web_page_preview=True
                    )
                posted += 1
                logger.info(f"\u2705 ({posted}) {event['title'][:50]}")
                await asyncio.sleep(2)
            except Exception as e:
                logger.error(f"\u274c {e}")

        logger.info(f"\u2705 Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! ĞĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾: {posted}")

    finally:
        await bot_obj.close()


if __name__ == '__main__':
    asyncio.run(main())
