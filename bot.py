import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional, Tuple

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path

import io
import numpy as np
import cv2
import pytesseract
from PIL import Image


STATE_DIR = Path("state")
POSTED_FILE = STATE_DIR / "load_posted.json"

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))


# â”€â”€â”€ Ğ¡Ğ»Ğ¾Ğ²Ğ°Ñ€Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

MONTHS_RU = {
    "ÑĞ½Ğ²Ğ°Ñ€Ñ": 1, "Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ": 2, "Ğ¼Ğ°Ñ€Ñ‚Ğ°": 3, "Ğ°Ğ¿Ñ€ĞµĞ»Ñ": 4,
    "Ğ¼Ğ°Ñ": 5, "Ğ¸ÑĞ½Ñ": 6, "Ğ¸ÑĞ»Ñ": 7, "Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°": 8,
    "ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ": 9, "Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ": 10, "Ğ½Ğ¾ÑĞ±Ñ€Ñ": 11, "Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ": 12,
}
MONTHS_SHORT = {
    "ÑĞ½Ğ²": 1, "Ñ„ĞµĞ²": 2, "Ğ¼Ğ°Ñ€": 3, "Ğ°Ğ¿Ñ€": 4,
    "Ğ¼Ğ°Ğ¹": 5, "Ğ¸ÑĞ½": 6, "Ğ¸ÑĞ»": 7, "Ğ°Ğ²Ğ³": 8,
    "ÑĞµĞ½": 9, "Ğ¾ĞºÑ‚": 10, "Ğ½Ğ¾Ñ": 11, "Ğ´ĞµĞº": 12,
}

# âœ… Ğ”ĞĞ‘ĞĞ’Ğ›Ğ•Ğ ĞŸĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞº Ğ¸ Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ°
KZ_CITIES = {
    "Ğ°Ğ»Ğ¼Ğ°Ñ‚Ñ‹": "ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹",
    "Ğ°ÑÑ‚Ğ°Ğ½Ğ°": "ĞÑÑ‚Ğ°Ğ½Ğ°",
    "ÑˆÑ‹Ğ¼ĞºĞµĞ½Ñ‚": "Ğ¨Ñ‹Ğ¼ĞºĞµĞ½Ñ‚",
    "Ğ½ÑƒÑ€-ÑÑƒĞ»Ñ‚Ğ°Ğ½": "ĞÑÑ‚Ğ°Ğ½Ğ°",
    "Ğ¿ĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞº": "ĞŸĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞº",   # âœ… Ğ½Ğ¾Ğ²Ñ‹Ğ¹
    "Ğ¿ĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»": "ĞŸĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞº",       # âœ… ÑĞ¾ĞºÑ€Ğ°Ñ‰ĞµĞ½Ğ¸Ğµ
    "ÑƒÑÑ‚ÑŒ-ĞºĞ°Ğ¼ĞµĞ½Ğ¾Ğ³Ğ¾Ñ€ÑĞº": "Ğ£ÑÑ‚ÑŒ-ĞšĞ°Ğ¼ĞµĞ½Ğ¾Ğ³Ğ¾Ñ€ÑĞº",
    "ĞºÑ‹Ğ·Ñ‹Ğ»Ğ¾Ñ€Ğ´Ğ°": "ĞšÑ‹Ğ·Ñ‹Ğ»Ğ¾Ñ€Ğ´Ğ°",
    "Ğ°ĞºÑ‚Ğ¾Ğ±Ğµ": "ĞĞºÑ‚Ğ¾Ğ±Ğµ",
    "Ñ‚Ğ°Ñ€Ğ°Ğ·": "Ğ¢Ğ°Ñ€Ğ°Ğ·",
    "Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ´Ğ°Ñ€": "ĞŸĞ°Ğ²Ğ»Ğ¾Ğ´Ğ°Ñ€",
    "ÑĞµĞ¼ĞµĞ¹": "Ğ¡ĞµĞ¼ĞµĞ¹",
    "Ğ°Ñ‚Ñ‹Ñ€Ğ°Ñƒ": "ĞÑ‚Ñ‹Ñ€Ğ°Ñƒ",
    "Ğ¶ĞµĞ·ĞºĞ°Ğ·Ğ³Ğ°Ğ½": "Ğ–ĞµĞ·Ò›Ğ°Ğ·Ò“Ğ°Ğ½",
    "Ğ¶ĞµĞ·Ò›Ğ°Ğ·Ò“Ğ°Ğ½": "Ğ–ĞµĞ·Ò›Ğ°Ğ·Ò“Ğ°Ğ½",
    "Ğ°ĞºÑ‚Ğ°Ñƒ": "ĞĞºÑ‚Ğ°Ñƒ",
    "ĞºĞ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ¹": "ĞšĞ¾ÑÑ‚Ğ°Ğ½Ğ°Ğ¹",             # âœ… Ğ½Ğ¾Ğ²Ñ‹Ğ¹
    "ÑƒÑ€Ğ°Ğ»ÑŒÑĞº": "Ğ£Ñ€Ğ°Ğ»ÑŒÑĞº",               # âœ… Ğ½Ğ¾Ğ²Ñ‹Ğ¹
    "Ñ‚ĞµĞ¼Ğ¸Ñ€Ñ‚Ğ°Ñƒ": "Ğ¢ĞµĞ¼Ğ¸Ñ€Ñ‚Ğ°Ñƒ",             # âœ… Ğ½Ğ¾Ğ²Ñ‹Ğ¹
    "Ğ¾Ğ½Ğ»Ğ°Ğ¹Ğ½": "ĞĞ½Ğ»Ğ°Ğ¹Ğ½",
    "online": "ĞĞ½Ğ»Ğ°Ğ¹Ğ½",
    "zoom": "ĞĞ½Ğ»Ğ°Ğ¹Ğ½ (Zoom)",
    "Ñ‚Ğ°ÑˆĞºĞµĞ½Ñ‚": "Ğ¢Ğ°ÑˆĞºĞµĞ½Ñ‚, Ğ£Ğ·Ğ±ĞµĞºĞ¸ÑÑ‚Ğ°Ğ½",
}

EVENT_WORDS = [
    "ĞºĞ¾Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ†Ğ¸Ñ", "conference", "Ñ„Ğ¾Ñ€ÑƒĞ¼", "forum", "summit", "ÑĞ°Ğ¼Ğ¼Ğ¸Ñ‚",
    "meetup", "Ğ¼Ğ¸Ñ‚Ğ°Ğ¿", "Ñ…Ğ°ĞºĞ°Ñ‚Ğ¾Ğ½", "hackathon", "Ğ²Ğ¾Ñ€ĞºÑˆĞ¾Ğ¿", "workshop",
    "Ğ¼Ğ°ÑÑ‚ĞµÑ€-ĞºĞ»Ğ°ÑÑ", "masterclass", "Ğ²ĞµĞ±Ğ¸Ğ½Ğ°Ñ€", "webinar", "ÑĞµĞ¼Ğ¸Ğ½Ğ°Ñ€",
    "pitch", "Ğ¿Ğ¸Ñ‚Ñ‡", "demo day", "Ğ°ĞºÑĞµĞ»ĞµÑ€Ğ°Ñ‚Ğ¾Ñ€", "accelerator",
    "bootcamp", "Ğ±ÑƒÑ‚ĞºĞµĞ¼Ğ¿", "Ğ²Ñ‹ÑÑ‚Ğ°Ğ²ĞºĞ°", "ĞºĞ¾Ğ½ĞºÑƒÑ€Ñ", "competition",
    "Ñ‚Ñ€ĞµĞ½Ğ¸Ğ½Ğ³", "training", "Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğµ", "Ğ¸Ğ²ĞµĞ½Ñ‚", "event",
    "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµÑ‚", "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµĞ¼", "Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞ¹ÑÑ", "Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ",
    "career", "ĞºĞ°Ñ€ÑŒĞµÑ€",
]
NOT_EVENT_WORDS = [
    "Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ğ»Ğ¾", "Ğ¸Ğ½Ğ²ĞµÑÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ»", "Ğ¿Ñ€Ğ¸Ğ²Ğ»ĞµĞº Ñ€Ğ°ÑƒĞ½Ğ´",
    "Ğ¼Ğ»Ğ½ $", "Ğ¼Ğ»Ñ€Ğ´ $", "Ğ½Ğ°Ğ·Ğ½Ğ°Ñ‡ĞµĞ½", "ÑƒĞ²Ğ¾Ğ»ĞµĞ½", "Ğ¾Ñ‚Ñ‡ĞµÑ‚", "Ğ²Ñ‹Ñ€ÑƒÑ‡ĞºĞ°",
    "ĞºÑƒÑ€Ñ Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ğ°", "Ğ±Ğ¸Ñ€Ğ¶Ğ°", "Ñ‚Ğ¾ĞºĞ°ĞµĞ²", "Ğ¿Ñ€Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾ Ğ¿Ñ€Ğ¸Ğ½ÑĞ»Ğ¾",
]
SITE_STOP_WORDS = [
    "ĞºĞ¾Ğ½Ñ‚Ğ°ĞºÑ‚Ñ‹", "Ğ¾ Ğ½Ğ°Ñ", "Ğ¿Ğ¾Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ°", "Ğ²Ğ¾Ğ¹Ñ‚Ğ¸", "Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ°Ñ†Ğ¸Ñ Ğ°ĞºĞºĞ°ÑƒĞ½Ñ‚Ğ°",
    "Ğ¿Ğ¾Ğ¸ÑĞº", "Ğ³Ğ»Ğ°Ğ²Ğ½Ğ°Ñ", "Ğ¼ĞµĞ½Ñ", "Ğ²ÑĞµ Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸", "Ñ‡Ğ¸Ñ‚Ğ°Ñ‚ÑŒ Ğ´Ğ°Ğ»ĞµĞµ",
    "Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ", "ÑƒĞ·Ğ½Ğ°Ñ‚ÑŒ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ", "privacy", "terms", "cookie",
]
DESCRIPTION_SIGNALS = [
    "Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ¸", "Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ»ĞµĞ½Ğ¸Ğµ ÑĞ¿Ğ¸ĞºĞµÑ€Ğ¾Ğ²", "Ğ²Ñ‹ ÑƒĞ·Ğ½Ğ°ĞµÑ‚Ğµ", "Ğ¼Ñ‹ Ñ€Ğ°ÑÑĞºĞ°Ğ¶ĞµĞ¼",
    "Ğ½Ğ° Ğ¼ĞµÑ€Ğ¾Ğ¿Ñ€Ğ¸ÑÑ‚Ğ¸Ğ¸", "Ğ² Ñ€Ğ°Ğ¼ĞºĞ°Ñ…", "ÑĞ¾ÑÑ‚Ğ¾Ğ¸Ñ‚ÑÑ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ°", "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ĞµĞ¼ Ğ²Ğ°Ñ",
    "Ğ·Ğ°Ñ€ĞµĞ³Ğ¸ÑÑ‚Ñ€Ğ¸Ñ€ÑƒĞ¹Ñ‚ĞµÑÑŒ", "Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½ĞµĞµ Ğ¿Ğ¾ ÑÑÑ‹Ğ»ĞºĞµ", "ÑĞ²Ğ¾Ğ±Ğ¾Ğ´Ğ½Ğ¾Ğµ Ğ¾Ğ±Ñ‰ĞµĞ½Ğ¸Ğµ",
    "Ğ¿Ñ€Ğ¸Ğ³Ğ»Ğ°ÑˆĞ°ÑÑ‚ Ğ²Ğ°Ñ Ğ¿Ñ€Ğ¸Ğ½ÑÑ‚ÑŒ ÑƒÑ‡Ğ°ÑÑ‚Ğ¸Ğµ", "Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ñ‹ Ğ¿ĞµÑ€ĞµĞ¹Ñ‚Ğ¸",
    "ĞºĞ°Ğ¶Ğ´ÑƒÑ ÑÑ€ĞµĞ´Ñƒ", "ĞºĞ°Ğ¶Ğ´Ñ‹Ğ¹ Ñ‡ĞµÑ‚Ğ²ĞµÑ€Ğ³", "ĞºĞ°Ğ¶Ğ´ÑƒÑ Ğ¿ÑÑ‚Ğ½Ğ¸Ñ†Ñƒ",  # âœ… Ñ€ĞµĞ³ÑƒĞ»ÑÑ€Ğ½Ñ‹Ğµ ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ñ
]

EMOJI_RE = re.compile(
    "[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]",
    re.UNICODE,
)

DATE_REGEX = re.compile(
    r"\b\d{1,2}[:.]\d{2}\b|"
    r"\b\d{1,2}\s*(ÑĞ½Ğ²|Ñ„ĞµĞ²|Ğ¼Ğ°Ñ€|Ğ°Ğ¿Ñ€|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½|Ğ¸ÑĞ»|Ğ°Ğ²Ğ³|ÑĞµĞ½|Ğ¾ĞºÑ‚|Ğ½Ğ¾Ñ|Ğ´ĞµĞº)\b|"
    r"\b\d{4}\b|"
    r"\b(january|february|march|april|may|june|july|august|september|october|november|december)\b",
    re.IGNORECASE,
)


# â”€â”€â”€ Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def normalize_link(link: str) -> str:
    if not link:
        return ""
    link = link.strip()
    link = link.replace("https://t.me/s/", "https://t.me/")
    link = link.split("?")[0]
    link = link.rstrip("/")
    return link


def is_clean_photo(url: str) -> bool:
    url = url.lower()
    blacklist = ["banner", "poster", "event", "flyer", "afisha", "1080x", "square", "card"]
    return not any(word in url for word in blacklist)


# â”€â”€â”€ Ğ”ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def dedup_title(title: str) -> str:
    """
    Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµÑ‚ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° Ğ² Ñ€Ğ°Ğ·Ğ½Ñ‹Ñ… Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ…:
    1. 'TitleÂ«TitleÂ»ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶ĞµĞ½Ğ¸Ğµ'  â†’ 'Title'
    2. 'TitleTitle'               â†’ 'Title'
    3. 'Title â€” Title'            â†’ 'Title'
    """
    if not title:
        return title

    # âœ… ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½: TextÂ«TextÂ»Ğ§Ñ‚Ğ¾-Ñ‚Ğ¾ â†’ Ğ±ĞµÑ€Ñ‘Ğ¼ Ğ¿ĞµÑ€Ğ²Ñ‹Ğ¹ Text
    m = re.match(r'^(.+?)Â«.+?Â».*$', title)
    if m:
        candidate = m.group(1).strip(' -â€“â€¢.,')
        if len(candidate) >= 5:
            return candidate

    # âœ… ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½: "Word Word Word â€” Word Word Word" (Ñ‡ĞµÑ€ĞµĞ· Ñ‚Ğ¸Ñ€Ğµ)
    m = re.match(r'^(.{10,80})\s*[â€”â€“-]{1,2}\s*\1', title)
    if m:
        return m.group(1).strip()

    # âœ… ĞŸĞ°Ñ‚Ñ‚ĞµÑ€Ğ½: Ğ¿Ñ€ÑĞ¼Ğ¾Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ "AbcAbc" Ğ¸Ğ»Ğ¸ "Abc Abc"
    half = len(title) // 2
    for i in range(8, half + 1):
        chunk = title[:i]
        rest = title[i:].lstrip(' ')
        if rest.startswith(chunk):
            return chunk.strip(' .,â€“-')

    return title


# â”€â”€â”€ Ğ“Ğ¾Ñ€Ğ¾Ğ´ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_city_from_start(title: str) -> Tuple[str, Optional[str]]:
    """
    âœ… Ğ•ÑĞ»Ğ¸ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº ĞĞĞ§Ğ˜ĞĞĞ•Ğ¢Ğ¡Ğ¯ Ñ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ Ğ³Ğ¾Ñ€Ğ¾Ğ´Ğ° â€” Ğ²Ñ‹Ñ€ĞµĞ·Ğ°ĞµĞ¼ ĞµĞ³Ğ¾ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµĞ¼.
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚: (Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹_Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº, Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ»Ğ¸ None)

    ĞŸÑ€Ğ¸Ğ¼ĞµÑ€Ñ‹:
    'ĞŸĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞº Career Map' â†’ ('Career Map', 'ĞŸĞµÑ‚Ñ€Ğ¾Ğ¿Ğ°Ğ²Ğ»Ğ¾Ğ²ÑĞº')
    'ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹ Meetup 2026'       â†’ ('Meetup 2026', 'ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹')
    'Data Meetup ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹'       â†’ ('Data Meetup ĞĞ»Ğ¼Ğ°Ñ‚Ñ‹', None)  â† Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ½Ğµ Ğ² Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ
    """
    stripped = strip_emoji(title).strip()
    lower = stripped.lower()

    for key, value in sorted(KZ_CITIES.items(), key=lambda x: -len(x[0])):
        if lower.startswith(key):
            rest = stripped[len(key):].strip(' -â€“â€¢,')
            if len(rest) >= 5:
                return rest, value

    return stripped, None


def extract_city_from_text(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    known = [
        "Narxoz", "Nazarbayev", "KBTU", "ĞšĞ‘Ğ¢Ğ£", "Astana Hub",
        "IT Park", "MOST IT Hub", "Holiday Inn", "Esentai",
        "Yandex", "Smart Point", "Almaty Arena", "SKO Hub",  # âœ… Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½ SKO Hub
    ]
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf"{re.escape(v)}[^\n,.]*", text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at = re.search(r"@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)", text)
    if at:
        return at.group(1).strip()[:60]
    return None


def is_real_event(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS)


def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)


def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)


# â”€â”€â”€ ĞÑ‡Ğ¸ÑÑ‚ĞºĞ° Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fix_glued_words(text: str) -> str:
    text = re.sub(r'([Ğ°-ÑÑ‘])([A-Z])', r'\1 \2', text)
    text = re.sub(r'([a-z])([Ğ-Ğ¯Ğ])', r'\1 \2', text)
    text = re.sub(r'([Ğ°-ÑÑ‘])([Ğ-Ğ¯Ğ])', r'\1 \2', text)
    return text


def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-zĞ-Ğ¯Ğ°-ÑĞÑ‘])", r"\1 ", s)
    s = re.sub(r"([Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)
    s = re.sub(r"\s{2,}", " ", s)
    return s


def strip_leading_datetime_from_title(title: str) -> str:
    t = strip_emoji(title).strip()
    t = normalize_glued_text(t)
    t = re.sub(
        r"^\s*\d{1,2}\s+[Ğ-Ğ¯Ğ°-ÑĞÑ‘A-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*",
        "", t, flags=re.IGNORECASE,
    )
    t = re.sub(r"^\s*\d{1,2}\s+[Ğ°-ÑÑ‘]{3,}(?:\s+\d{4})?\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)
    return t.strip(" -â€“â€¢.,").strip()


def clean_title(raw_title: str) -> Optional[str]:
    """
    ĞŸĞ¾Ğ»Ğ½Ğ°Ñ Ğ¾Ñ‡Ğ¸ÑÑ‚ĞºĞ° Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°:
    1. Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´Ğ°Ñ‚Ñƒ/Ğ²Ñ€ĞµĞ¼Ñ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°
    2. Ğ˜ÑĞ¿Ñ€Ğ°Ğ²Ğ»ÑĞµĞ¼ ÑĞ»Ğ¸Ğ¿ÑˆĞ¸ĞµÑÑ ÑĞ»Ğ¾Ğ²Ğ°
    3. Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ (Ğ²ĞºĞ»ÑÑ‡Ğ°Ñ Â«ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸Â»)
    4. Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ Ñ…Ğ²Ğ¾ÑÑ‚Ñ‹-Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ñ
    """
    s = strip_leading_datetime_from_title(raw_title)
    s = fix_glued_words(s)
    s = dedup_title(s)  # âœ… Ñ‚ĞµĞ¿ĞµÑ€ÑŒ Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ°Ñ‚Ñ‹Ğ²Ğ°ĞµÑ‚ Ğ¸ Â«ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸Â»

    # ĞĞ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ñ…Ğ²Ğ¾ÑÑ‚-Ğ¾Ğ¿Ğ¸ÑĞ°Ğ½Ğ¸Ğµ
    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx > 12:
            s = s[:idx].strip(" -â€“â€¢.,")
            break

    s = re.sub(r"\s{2,}", " ", s).strip()

    if len(s) < 5 or looks_like_description(s):
        return None

    return s[:120]


# â”€â”€â”€ Ğ”Ğ°Ñ‚Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_date(text: str) -> Optional[datetime]:
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try:
            return datetime(year, month, day)
        except Exception:
            return None

    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([Ğ°-ÑÑ‘]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month:
            return make_dt(year, month, int(m.group(2)))

    m = re.search(r"(\d{1,2})\s+([Ğ°-ÑÑ‘]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(
        r"(\d{1,2})\s+(ÑĞ½Ğ²|Ñ„ĞµĞ²|Ğ¼Ğ°Ñ€|Ğ°Ğ¿Ñ€|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½|Ğ¸ÑĞ»|Ğ°Ğ²Ğ³|ÑĞµĞ½|Ğ¾ĞºÑ‚|Ğ½Ğ¾Ñ|Ğ´ĞµĞº)[Ğ°-Ñ]*(?:\s+(\d{4}))?", t
    )
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12:
            return make_dt(year, month, int(m.group(1)))

    return None


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: "ÑĞ½Ğ²Ğ°Ñ€Ñ", 2: "Ñ„ĞµĞ²Ñ€Ğ°Ğ»Ñ", 3: "Ğ¼Ğ°Ñ€Ñ‚Ğ°", 4: "Ğ°Ğ¿Ñ€ĞµĞ»Ñ",
        5: "Ğ¼Ğ°Ñ", 6: "Ğ¸ÑĞ½Ñ", 7: "Ğ¸ÑĞ»Ñ", 8: "Ğ°Ğ²Ğ³ÑƒÑÑ‚Ğ°",
        9: "ÑĞµĞ½Ñ‚ÑĞ±Ñ€Ñ", 10: "Ğ¾ĞºÑ‚ÑĞ±Ñ€Ñ", 11: "Ğ½Ğ¾ÑĞ±Ñ€Ñ", 12: "Ğ´ĞµĞºĞ°Ğ±Ñ€Ñ",
    }
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s


# â”€â”€â”€ Ğ¤Ğ¾Ñ€Ğ¼Ğ°Ñ‚ Ğ¿Ğ¾ÑÑ‚Ğ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_post(event: Dict) -> str:
    title    = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link     = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    title = strip_leading_datetime_from_title(title)
    if looks_like_description(title):
        return ""

    location = event.get("location", "")
    venue    = event.get("venue", "")

    lines = [f"ğŸ¯ <b>{title}</b>"]

    if location in ("ĞĞ½Ğ»Ğ°Ğ¹Ğ½", "ĞĞ½Ğ»Ğ°Ğ¹Ğ½ (Zoom)"):
        lines.append("ğŸŒ ĞĞ½Ğ»Ğ°Ğ¹Ğ½")
    elif location:
        lines.append(f"ğŸ‡°ğŸ‡¿ ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½, ğŸ™ {location}")
    else:
        lines.append("ğŸ‡°ğŸ‡¿ ĞšĞ°Ğ·Ğ°Ñ…ÑÑ‚Ğ°Ğ½")

    if venue:
        lines.append(f"ğŸ“ {venue}")

    lines.append(f"ğŸ“… {date_str}")
    lines.append(f"ğŸ”— <a href='{link}'>Ğ§Ğ¸Ñ‚Ğ°Ñ‚ÑŒ â†’</a>")

    return "\n".join(lines)


# â”€â”€â”€ ĞŸÑ€Ğ¸ĞºĞ»ĞµĞµĞ½Ğ½Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ° "09 Ğ¤ĞµĞ², 17:00Ğ¨Ñ‹Ğ¼ĞºĞµĞ½Ñ‚ĞĞ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ğµ" â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_GLUE_RE = re.compile(
    r"^(\d{1,2})\s+"
    r"([Ğ-Ğ¯ĞĞ°-ÑÑ‘A-Za-z]{3,})"
    r"[,\s]+"
    r"(\d{1,2}:\d{2})"
    r"\s*(.*?)$",
    re.IGNORECASE | re.DOTALL,
)


def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m:
        return None

    day_s, month_s = m.group(1), m.group(2).lower()
    time_str       = m.group(3)
    rest           = m.group(4).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    try:
        dt = datetime(datetime.now().year, month_num, int(day_s))
    except Exception:
        return None

    if not is_future(dt):
        return None

    # âœ… Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° rest
    rest_clean, city = extract_city_from_start(rest)
    rest_clean = dedup_title(rest_clean)

    if len(rest_clean) < 5:
        return None

    return {
        "dt":             dt,
        "time_str":       time_str,
        "city":           city,
        "title_raw":      rest_clean[:300],
        "date_formatted": format_date(dt, time_str),
    }


# â”€â”€â”€ OCR â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def smart_crop_text_zones(session, image_url: str):
    try:
        async with session.get(image_url, timeout=20) as resp:
            if resp.status != 200:
                return None
            data = await resp.read()

        pil_img = Image.open(io.BytesIO(data)).convert("RGB")
        img = np.array(pil_img)
        h, w, _ = img.shape
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
        ocr = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)

        crop_top, crop_bottom = 0, h
        detected_top, detected_bottom = [], []

        for i, text in enumerate(ocr["text"]):
            text = text.strip()
            if not text:
                continue
            if DATE_REGEX.search(text):
                y  = ocr["top"][i]
                bh = ocr["height"][i]
                if y < h * 0.45:
                    detected_top.append(y + bh)
                if y > h * 0.55:
                    detected_bottom.append(y)

        if detected_top:
            crop_top = max(detected_top) + 30
        if detected_bottom:
            crop_bottom = min(detected_bottom) - 30

        if crop_bottom - crop_top < h * 0.45:
            return None

        cropped = img[crop_top:crop_bottom, 0:w]
        if cropped.size == 0:
            return None

        final_img = Image.fromarray(cropped)
        buffer = io.BytesIO()
        final_img.save(buffer, format="JPEG", quality=95)
        buffer.seek(0)
        return buffer

    except Exception as e:
        logger.error(f"OCR crop error: {e}")
        return None


# â”€â”€â”€ State â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def load_posted() -> set:
    if POSTED_FILE.exists():
        try:
            with open(POSTED_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()


def save_posted(posted: set):
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)
        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(sorted(posted), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ posted_links: {e}")


# â”€â”€â”€ Ğ˜ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz",                 "name": "ER10"},
    {"url": "https://kapital.kz",              "name": "Capital"},
    {"url": "https://forbes.kz",               "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media",         "name": "Kursiv kz"},
    {"url": "https://ma7.vc",                  "name": "MA7"},
    {"url": "https://tumarventures.com",       "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io",     "name": "White hill capital"},
    {"url": "https://bigsky.vc",               "name": "Big sky ventures"},
    {"url": "https://mostfund.vc",             "name": "Most ventures"},
    {"url": "https://axiomcapital.com",        "name": "Axiom capital"},
    {"url": "https://jastarventures.com",      "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz",         "name": "NURIS"},
    {"url": "https://tech.kz",                 "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz",  "name": "Digital Business KZ"},
    {"username": "vcinsightskz",       "name": "VC Insights KZ"},
    {"username": "tech_kz",            "name": "Tech KZ"},
    {"username": "startupalmaty",      "name": "Startup Almaty"},
    {"username": "astanahub_events",   "name": "Astana Hub Events"},
]


# â”€â”€â”€ EventBot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class EventBot:
    def __init__(self):
        self.session = None
        self.posted  = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={"User-Agent": "Mozilla/5.0"})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    # â”€â”€ Ğ”Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines  = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(
                r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?"
                r"|\d{1,2}\s+(?:ÑĞ½Ğ²|Ñ„ĞµĞ²|Ğ¼Ğ°Ñ€|Ğ°Ğ¿Ñ€|Ğ¼Ğ°Ğ¹|Ğ¸ÑĞ½|Ğ¸ÑĞ»|Ğ°Ğ²Ğ³|ÑĞµĞ½|Ğ¾ĞºÑ‚|Ğ½Ğ¾Ñ|Ğ´ĞµĞº)[Ğ°-Ñ]*"
                r"(?:\s+\d{4})?)",
                line, re.IGNORECASE,
            )
            if not dm:
                i += 1
                continue

            date_raw = dm.group(0)
            rest     = line[dm.end():].strip()

            tm       = re.search(r"(?:Ğ²\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm:
                rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -â€“â€¢")

            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"):
                    link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt):
                    title_raw = nxt

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                i += 1
                continue

            ctx      = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_city_from_text(ctx) or extract_city_from_text(text)

            # âœ… Ğ§Ğ¸ÑÑ‚Ğ¸Ğ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº + Ğ²Ñ‹Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ°
            title_raw, city_from_title = extract_city_from_start(title_raw)
            title_clean = clean_title(title_raw)
            if not title_clean:
                i += 1
                continue

            events.append({
                "title":     title_clean,
                "date":      format_date(dt, time_str),
                "location":  location or city_from_title or "",
                "venue":     extract_venue(ctx),
                "link":      link or post_link,
                "source":    source,
                "image_url": image_url,
            })
            i += 1
        return events

    # â”€â”€ Telegram-ĞºĞ°Ğ½Ğ°Ğ» â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html:
            return []

        soup       = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td:
                    continue

                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30:
                    continue

                le        = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"
                norm_link = normalize_link(post_link)

                if norm_link in self.posted:
                    continue

                # Ğ’Ğ½ĞµÑˆĞ½ÑÑ ÑÑÑ‹Ğ»ĞºĞ°
                external_link = None
                for a in td.find_all("a", href=True):
                    href = normalize_link(a["href"])
                    if "t.me" not in href:
                        external_link = href
                        break
                if not external_link:
                    for raw_l in re.findall(r"(https?://[^\s]+)", text):
                        cl = normalize_link(raw_l)
                        if "t.me" not in cl:
                            external_link = cl
                            break

                final_link = external_link if external_link else norm_link

                # ĞšĞ°Ñ€Ñ‚Ğ¸Ğ½ĞºĞ°
                image_url = None
                photo_wrap = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if photo_wrap:
                    style = photo_wrap.get("style", "")
                    match = re.search(r"url\('([^']+)'\)", style)
                    if match:
                        image_url = match.group(1)
                if not image_url:
                    img_tag = td.find("img")
                    if img_tag and img_tag.get("src"):
                        image_url = img_tag["src"]

                # Ğ”Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚
                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:Ğ²\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    logger.info(f"ğŸ“‹ Ğ”Ğ°Ğ¹Ğ´Ğ¶ĞµÑÑ‚ {channel['name']}: {len(evs)}")
                    continue

                if not is_real_event(text):
                    continue

                # ĞŸĞµÑ€Ğ²Ğ°Ñ ÑÑ‚Ñ€Ğ¾ĞºĞ°
                first_line = ""
                for ln in text.split("\n"):
                    cl = strip_emoji(ln).strip()
                    if len(cl) > 10:
                        first_line = cl
                        break

                # ĞŸÑ€Ğ¸ĞºĞ»ĞµĞµĞ½Ğ½Ğ°Ñ Ğ´Ğ°Ñ‚Ğ°?
                has_glue = bool(re.match(
                    r"^\d{1,2}\s+[Ğ-Ğ¯ĞĞ°-ÑÑ‘A-Za-z]{3,}[,.\s]+\d{1,2}:\d{2}",
                    first_line,
                ))

                if has_glue:
                    glued = parse_glued_line(first_line)
                    if not glued:
                        continue

                    # âœ… Ğ“Ğ¾Ñ€Ğ¾Ğ´ ÑƒĞ¶Ğµ Ğ²Ñ‹Ñ€ĞµĞ·Ğ°Ğ½ Ğ² parse_glued_line Ñ‡ĞµÑ€ĞµĞ· extract_city_from_start
                    title_raw = glued["title_raw"]
                    title_clean = clean_title(title_raw)
                    if not title_clean:
                        continue

                    # Ğ“Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ° Ğ¿Ğ¾ÑÑ‚Ğ° ĞºĞ°Ğº fallback
                    location = glued["city"] or extract_city_from_text(text) or ""

                    all_events.append({
                        "title":     title_clean,
                        "date":      glued["date_formatted"],
                        "location":  location,
                        "venue":     extract_venue(text),
                        "link":      final_link,
                        "source":    channel["name"],
                        "image_url": image_url,
                    })
                    continue

                # ĞĞ±Ñ‹Ñ‡Ğ½Ñ‹Ğ¹ Ğ¿Ğ¾ÑÑ‚
                dt = parse_date(text)
                if not is_future(dt):
                    continue

                # âœ… Ğ’Ñ‹Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°
                raw_title, city_from_title = extract_city_from_start(first_line)
                title_clean = clean_title(raw_title)
                if not title_clean:
                    continue

                location = extract_city_from_text(text) or city_from_title or ""

                tm2      = re.search(r"\d{1,2}\s+[Ğ°-ÑÑ‘Ğ-Ğ¯Ğ]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append({
                    "title":     title_clean,
                    "date":      format_date(dt, time_str),
                    "location":  location,
                    "venue":     extract_venue(text),
                    "link":      final_link,
                    "source":    channel["name"],
                    "image_url": image_url,
                })

            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue

        return all_events

    # â”€â”€ Ğ¡Ğ°Ğ¹Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site["url"])
        if not html:
            return []

        soup   = BeautifulSoup(html, "html.parser")
        events = []

        for link_el in soup.find_all("a", href=True)[:80]:
            try:
                href      = link_el.get("href", "")
                title_raw = link_el.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue

                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)

                href = normalize_link(href)

                if href.rstrip("/") == normalize_link(site["url"]).rstrip("/"):
                    continue
                if href in self.posted:
                    continue
                if is_site_trash(title_raw):
                    continue
                if not is_real_event(title_raw):
                    continue

                parent  = link_el.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt      = parse_date(context)

                if not is_future(dt):
                    continue

                image_url = None
                if parent:
                    for img in parent.find_all("img"):
                        src = img.get("src") or img.get("data-src")
                        if not src:
                            continue
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src
                            break

                if not image_url and parent:
                    style = parent.get("style", "")
                    match = re.search(r"url\(['\"]?([^'\")]+)", style)
                    if match:
                        src = match.group(1)
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src

                # âœ… Ğ’Ñ‹Ñ€ĞµĞ·Ğ°ĞµĞ¼ Ğ³Ğ¾Ñ€Ğ¾Ğ´ Ğ¸Ğ· Ğ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²ĞºĞ°
                clean_raw, city_from_title = extract_city_from_start(title_raw)
                title_clean = clean_title(clean_raw) or strip_emoji(dedup_title(clean_raw))[:120]
                location = extract_city_from_text(context) or city_from_title or ""

                events.append({
                    "title":     title_clean,
                    "date":      format_date(dt),
                    "location":  location,
                    "venue":     extract_venue(context),
                    "link":      href,
                    "source":    site["name"],
                    "image_url": image_url,
                })

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events

    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"ğŸŒ ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ {len(URLS)} ÑĞ°Ğ¹Ñ‚Ğ¾Ğ²...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)
            if evs:
                logger.info(f"âœ… {site['name']}: {len(evs)}")

        logger.info(f"ğŸ“± ĞŸĞ°Ñ€ÑĞ¸Ğ½Ğ³ {len(TELEGRAM_CHANNELS)} ĞºĞ°Ğ½Ğ°Ğ»Ğ¾Ğ²...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)
            if evs:
                logger.info(f"âœ… {ch['name']}: {len(evs)}")

        return all_events


# â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main():
    logger.info("ğŸš€ Ğ¡Ñ‚Ğ°Ñ€Ñ‚ EventBot...")

    if not BOT_TOKEN:
        logger.error("âŒ BOT_TOKEN Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½!")
        return

    bot_obj = EventBot()
    bot_api = Bot(token=BOT_TOKEN)

    logger.info(f"ğŸ“‚ Ğ’ Ğ±Ğ°Ğ·Ğµ ÑƒĞ¶Ğµ {len(bot_obj.posted)} Ğ¾Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ñ… ÑÑÑ‹Ğ»Ğ¾Ğº.")

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"ğŸ“Š ĞĞ¾Ğ²Ñ‹Ñ… ÑƒĞ½Ğ¸ĞºĞ°Ğ»ÑŒĞ½Ñ‹Ñ… ÑĞ¾Ğ±Ñ‹Ñ‚Ğ¸Ğ¹: {len(unique)}")

        posted = 0

        for event in unique[:15]:
            norm_link = normalize_link(event.get("link", ""))

            if norm_link in bot_obj.posted:
                logger.info(f"â­ï¸ Ğ£Ğ¶Ğµ Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ»Ğ¾ÑÑŒ: {event.get('title','')[:50]}")
                continue

            text = make_post(event)
            if not text:
                continue

            try:
                photo_to_send = None

                if event.get("image_url"):
                    session = await bot_obj.get_session()
                    photo_to_send = await smart_crop_text_zones(session, event["image_url"])

                if photo_to_send:
                    await bot_api.send_photo(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        photo=photo_to_send,
                        caption=text,
                        parse_mode="HTML",
                    )
                else:
                    await bot_api.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                # âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ ÑÑ€Ğ°Ğ·Ñƒ Ğ¿Ğ¾ÑĞ»Ğµ ÑƒÑĞ¿ĞµÑˆĞ½Ğ¾Ğ¹ Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸
                bot_obj.posted.add(norm_link)
                save_posted(bot_obj.posted)

                posted += 1
                logger.info(f"âœ… ({posted}) {event.get('title', '')[:50]}")
                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ñ‚Ğ¿Ñ€Ğ°Ğ²ĞºĞ¸: {e}")

        logger.info(f"âœ… Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾! ĞĞ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ¾Ğ²Ğ°Ğ½Ğ¾: {posted}")
        logger.info(f"ğŸ“‚ Ğ’ÑĞµĞ³Ğ¾ Ğ² Ğ±Ğ°Ğ·Ğµ: {len(bot_obj.posted)} ÑÑÑ‹Ğ»Ğ¾Ğº.")

    finally:
        await bot_obj.close()


if __name__ == "__main__":
    asyncio.run(main())
