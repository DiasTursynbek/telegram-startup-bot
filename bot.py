import os
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re

logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID', "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv('MESSAGE_THREAD_ID', '4'))

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏
URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]


class EventExtractor:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ–±—ã—Ç–∏–∏"""
    
    @staticmethod
    def extract_location(text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –≥–æ—Ä–æ–¥/–º–µ—Å—Ç–æ"""
        cities = ['–ê–ª–º–∞—Ç—ã', '–ê—Å—Ç–∞–Ω–∞', '–ù—É—Ä-–°—É–ª—Ç–∞–Ω', '–®—ã–º–∫–µ–Ω—Ç', '–ö–∞—Ä–∞–≥–∞–Ω–¥–∞', 
                  '–ê–∫—Ç–æ–±–µ', '–¢–∞—Ä–∞–∑', '–ü–∞–≤–ª–æ–¥–∞—Ä', '–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫', '–û–Ω–ª–∞–π–Ω', 'Online']
        
        for city in cities:
            if city.lower() in text.lower():
                return city
        return None
    
    @staticmethod
    def extract_date(text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –¥–∞—Ç—É"""
        # –§–æ—Ä–º–∞—Ç: "20 —Ñ–µ–≤—Ä–∞–ª—è", "15.02.2026", "2026-02-20"
        patterns = [
            r'\d{1,2}\s+(?:—è–Ω–≤–∞—Ä—è|—Ñ–µ–≤—Ä–∞–ª—è|–º–∞—Ä—Ç–∞|–∞–ø—Ä–µ–ª—è|–º–∞—è|–∏—é–Ω—è|–∏—é–ª—è|–∞–≤–≥—É—Å—Ç–∞|—Å–µ–Ω—Ç—è–±—Ä—è|–æ–∫—Ç—è–±—Ä—è|–Ω–æ—è–±—Ä—è|–¥–µ–∫–∞–±—Ä—è)',
            r'\d{2}\.\d{2}\.\d{4}',
            r'\d{4}-\d{2}-\d{2}'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                return match.group(0)
        return None
    
    @staticmethod
    def parse_date_to_datetime(date_str: str) -> Optional[datetime]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å —Å—Ç—Ä–æ–∫—É –¥–∞—Ç—ã –≤ datetime –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
        if not date_str:
            return None
        
        # –ú–µ—Å—è—Ü—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        months_ru = {
            '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
            '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
            '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12
        }
        
        try:
            # –§–æ—Ä–º–∞—Ç: "20 —Ñ–µ–≤—Ä–∞–ª—è"
            match = re.search(r'(\d{1,2})\s+([–∞-—è]+)', date_str.lower())
            if match:
                day = int(match.group(1))
                month_name = match.group(2)
                month = months_ru.get(month_name)
                if month:
                    year = datetime.now().year
                    # –ï—Å–ª–∏ –º–µ—Å—è—Ü —É–∂–µ –ø—Ä–æ—à–µ–ª - –±–µ—Ä–µ–º —Å–ª–µ–¥—É—é—â–∏–π –≥–æ–¥
                    event_date = datetime(year, month, day)
                    if event_date < datetime.now():
                        event_date = datetime(year + 1, month, day)
                    return event_date
            
            # –§–æ—Ä–º–∞—Ç: "15.02.2026"
            match = re.search(r'(\d{2})\.(\d{2})\.(\d{4})', date_str)
            if match:
                day, month, year = map(int, match.groups())
                return datetime(year, month, day)
            
            # –§–æ—Ä–º–∞—Ç: "2026-02-20"
            match = re.search(r'(\d{4})-(\d{2})-(\d{2})', date_str)
            if match:
                year, month, day = map(int, match.groups())
                return datetime(year, month, day)
        except:
            pass
        
        return None
    
    @staticmethod
    def is_future_event(date_str: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —á—Ç–æ —Å–æ–±—ã—Ç–∏–µ –≤ –±—É–¥—É—â–µ–º"""
        event_date = EventExtractor.parse_date_to_datetime(date_str)
        if not event_date:
            # –ï—Å–ª–∏ –¥–∞—Ç—É –Ω–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å - —Å—á–∏—Ç–∞–µ–º —á—Ç–æ —Å–æ–±—ã—Ç–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω–æ
            return True
        
        # –°–æ–±—ã—Ç–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫–∞–∫ –º–∏–Ω–∏–º—É–º —Å–µ–≥–æ–¥–Ω—è –∏–ª–∏ –≤ –±—É–¥—É—â–µ–º
        return event_date.date() >= datetime.now().date()
    
    @staticmethod
    def extract_time(text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –≤—Ä–µ–º—è"""
        pattern = r'\d{1,2}:\d{2}'
        match = re.search(pattern, text)
        return match.group(0) if match else None
    
    @staticmethod
    def extract_venue(text: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å –º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–∏—è"""
        venues = ['Astana Hub', 'IT Park', 'Dostyk Plaza', 'Ramstore', 
                  'Esentai', '–ú–§–¶–ê', '—Ç–µ—Ö–Ω–æ–ø–∞—Ä–∫', '–∫–æ–≤–æ—Ä–∫–∏–Ω–≥']
        
        for venue in venues:
            if venue.lower() in text.lower():
                return venue
        return None
    
    @staticmethod
    def clean_title(title: str) -> str:
        """–û—á–∏—Å—Ç–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ - –æ—Å—Ç–∞–≤–∏—Ç—å —Ç–æ–ª—å–∫–æ —Å—É—Ç—å"""
        # –£–±—Ä–∞—Ç—å –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        title = re.sub(r'[¬´¬ª""‚Äû]', '', title)
        title = re.sub(r'\s+', ' ', title).strip()
        
        # –û–≥—Ä–∞–Ω–∏—á–∏—Ç—å –¥–ª–∏–Ω—É
        if len(title) > 100:
            title = title[:97] + '...'
        
        return title


class Parser:
    def __init__(self):
        self.session = None
        self.posted_cache = set()
        self.extractor = EventExtractor()
    
    async def get_session(self):
        if not self.session:
            self.session = aiohttp.ClientSession(
                headers={'User-Agent': 'Mozilla/5.0'}
            )
        return self.session
    
    async def close(self):
        if self.session:
            await self.session.close()
    
    async def fetch(self, url: str) -> str:
        try:
            session = await self.get_session()
            async with session.get(url, timeout=10) as resp:
                return await resp.text() if resp.status == 200 else ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return ""
    
    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site['url'])
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        events = []
        
        keywords = ['—Å—Ç–∞—Ä—Ç–∞–ø', 'startup', 'pitch', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'event', 'meetup', '—Ö–∞–∫–∞—Ç–æ–Ω']
        stop = ['–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–æ –Ω–∞—Å', '–ø–æ–ª–∏—Ç–∏–∫–∞', '—Ç–æ–∫–∞–µ–≤', '–º–∏–Ω–∏—Å—Ç—Ä']
        
        for link in soup.find_all('a', href=True)[:30]:
            try:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title or len(title) < 15:
                    continue
                
                if any(s in title.lower() for s in stop):
                    continue
                
                if not any(k in title.lower() for k in keywords):
                    continue
                
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                
                if href in self.posted_cache:
                    continue
                
                # –ò–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏
                parent = link.find_parent(['div', 'article'])
                context = parent.get_text() if parent else title
                
                location = self.extractor.extract_location(context)
                date = self.extractor.extract_date(context)
                time = self.extractor.extract_time(context)
                venue = self.extractor.extract_venue(context)
                
                # –§–ò–õ–¨–¢–†: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—à–µ–¥—à–∏–µ —Å–æ–±—ã—Ç–∏—è
                if date and not self.extractor.is_future_event(date):
                    logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–æ—à–µ–¥—à–µ–µ —Å–æ–±—ã—Ç–∏–µ: {title[:50]} ({date})")
                    continue
                
                # –ù–∞–π—Ç–∏ –û–î–ù–û –≥–ª–∞–≤–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = None
                if parent:
                    img = parent.find('img', src=True)
                    if img:
                        image_url = img['src']
                        if not image_url.startswith('http'):
                            from urllib.parse import urljoin
                            image_url = urljoin(site['url'], image_url)
                
                events.append({
                    'source': site['name'],
                    'title': self.extractor.clean_title(title),
                    'link': href,
                    'location': location,
                    'date': date,
                    'time': time,
                    'venue': venue,
                    'image_url': image_url
                })
                
                if len(events) >= 2:
                    break
            except:
                continue
        
        return events
    
    async def parse_telegram(self, channel: Dict) -> List[Dict]:
        try:
            url = f"https://t.me/s/{channel['username']}"
            html = await self.fetch(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            events = []
            cutoff_date = datetime.now() - timedelta(days=2)
            
            for msg in soup.find_all('div', class_='tgme_widget_message')[:10]:
                text_div = msg.find('div', class_='tgme_widget_message_text')
                if not text_div:
                    continue
                
                text = text_div.get_text(strip=True)
                if len(text) < 20:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∞—Ç—ã
                time_elem = msg.find('time')
                if time_elem:
                    try:
                        post_date_str = time_elem.get('datetime', '')
                        post_date = datetime.fromisoformat(post_date_str.replace('Z', '+00:00'))
                        if post_date < cutoff_date:
                            continue
                    except:
                        pass
                
                keywords = ['—Å—Ç–∞—Ä—Ç–∞–ø', 'event', 'meetup', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', '—Ö–∞–∫–∞—Ç–æ–Ω', 'pitch']
                if not any(k in text.lower() for k in keywords):
                    continue
                
                link_elem = msg.find('a', class_='tgme_widget_message_date')
                link = link_elem['href'] if link_elem else f"https://t.me/{channel['username']}"
                
                # –ò–∑–≤–ª–µ—á—å –¥–µ—Ç–∞–ª–∏
                location = self.extractor.extract_location(text)
                date = self.extractor.extract_date(text)
                time = self.extractor.extract_time(text)
                venue = self.extractor.extract_venue(text)
                
                # –§–ò–õ–¨–¢–†: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ—à–µ–¥—à–∏–µ —Å–æ–±—ã—Ç–∏—è
                if date and not self.extractor.is_future_event(date):
                    logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø—Ä–æ—à–µ–¥—à–µ–µ TG —Å–æ–±—ã—Ç–∏–µ ({date})")
                    continue
                
                # –û–î–ù–û –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                image_url = None
                img_div = msg.find('a', class_='tgme_widget_message_photo_wrap')
                if img_div:
                    style = img_div.get('style', '')
                    img_match = re.search(r"url\('([^']+)'\)", style)
                    if img_match:
                        image_url = img_match.group(1)
                
                events.append({
                    'source': channel['name'],
                    'title': self.extractor.clean_title(text[:100]),
                    'link': link,
                    'location': location,
                    'date': date,
                    'time': time,
                    'venue': venue,
                    'image_url': image_url
                })
            
            return events
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ TG {channel['name']}: {e}")
            return []
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []
        
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            events = await self.parse_site(site)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ {site['name']}: {len(events)} —Å–æ–±—ã—Ç–∏–π")
        
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤...")
        for channel in TELEGRAM_CHANNELS:
            events = await self.parse_telegram(channel)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ TG {channel['name']}: {len(events)} —Å–æ–±—ã—Ç–∏–π")
        
        logger.info(f"üìä –í—Å–µ–≥–æ: {len(all_events)} —Å–æ–±—ã—Ç–∏–π")
        return all_events


def format_event_post(event: Dict) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Å—Ç –≤ –∫—Ä–∞—Å–∏–≤–æ–º –≤–∏–¥–µ"""
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    post = f"üéØ <b>{event['title']}</b>\n\n"
    
    # –î–µ—Ç–∞–ª–∏ —Å–æ–±—ã—Ç–∏—è (4-5 —Å—Ç—Ä–æ–∫)
    details = []
    
    if event.get('location'):
        country = "Kazakhstan" if event['location'] not in ['–û–Ω–ª–∞–π–Ω', 'Online'] else ""
        location_str = f"üìç {event['location']}"
        if country:
            location_str += f", {country}"
        details.append(location_str)
    
    if event.get('date'):
        date_str = f"üìÖ {event['date']}"
        if event.get('time'):
            date_str += f", {event['time']}"
        details.append(date_str)
    
    if event.get('venue'):
        details.append(f"üè¢ {event['venue']}")
    
    # –ò—Å—Ç–æ—á–Ω–∏–∫
    details.append(f"üì∞ {event['source']}")
    
    post += "\n".join(details)
    
    # –°—Å—ã–ª–∫–∞
    post += f"\n\nüîó <a href='{event['link']}'>–ü–æ–¥—Ä–æ–±–Ω–µ–µ</a>"
    
    return post


async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç –ø—É–±–ª–∏–∫–∞—Ü–∏–∏...")
    
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        return
    
    parser = Parser()
    bot = Bot(token=BOT_TOKEN)
    
    try:
        events = await parser.get_all_events()
        
        if not events:
            logger.warning("‚ö†Ô∏è –°–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_events = []
        seen_links = set()
        for event in events:
            if event['link'] not in seen_links:
                unique_events.append(event)
                seen_links.add(event['link'])
        
        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique_events)}")
        
        posted = 0
        for event in unique_events[:10]:
            try:
                text = format_event_post(event)
                
                # –ï—Å–ª–∏ –µ—Å—Ç—å —Ñ–æ—Ç–æ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å —Ñ–æ—Ç–æ
                if event.get('image_url'):
                    try:
                        await bot.send_photo(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            photo=event['image_url'],
                            caption=text,
                            parse_mode='HTML'
                        )
                    except:
                        # –ï—Å–ª–∏ —Ñ–æ—Ç–æ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–æ—Å—å - –±–µ–∑ —Ñ–æ—Ç–æ
                        await bot.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode='HTML',
                            disable_web_page_preview=True
                        )
                else:
                    await bot.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode='HTML',
                        disable_web_page_preview=True
                    )
                
                parser.posted_cache.add(event['link'])
                posted += 1
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ ({posted}): {event['title'][:40]}")
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        
        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted}")
        
    finally:
        await parser.close()


if __name__ == '__main__':
    asyncio.run(main())
