import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path

import io
import numpy as np
import cv2
import pytesseract
from PIL import Image





STATE_DIR = Path("state")
POSTED_FILE = STATE_DIR / "load_posted.json"

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))



def remove_city_from_title(title: str) -> str:
    for city_key in KZ_CITIES.keys():
        pattern = re.compile(rf"{city_key}", re.IGNORECASE)
        title = pattern.sub("", title)

    title = re.sub(r"\s{2,}", " ", title)
    return title.strip(" -‚Äì‚Ä¢,")





def fix_glued_words(text: str) -> str:
    # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞ + –ª–∞—Ç–∏–Ω–∏—Ü–∞
    text = re.sub(r'([–∞-—è—ë])([A-Z])', r'\1 \2', text)
    text = re.sub(r'([a-z])([–ê-–Ø–Å])', r'\1 \2', text)
    text = re.sub(r'([–∞-—è—ë])([–ê-–Ø–Å])', r'\1 \2', text)
    return text




def is_clean_photo(url: str) -> bool:
    url = url.lower()

    blacklist = [
        "banner",
        "poster",
        "event",
        "flyer",
        "afisha",
        "1080x",
        "square",
        "card",
    ]

    return not any(word in url for word in blacklist)





#       OCR

DATE_REGEX = re.compile(
    r"\b\d{1,2}[:.]\d{2}\b|"           # 16:00
    r"\b\d{1,2}\s*(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)\b|"
    r"\b\d{4}\b|"
    r"\b(january|february|march|april|may|june|july|august|september|october|november|december)\b",
    re.IGNORECASE
)


async def smart_crop_text_zones(session, image_url: str):
    try:
        async with session.get(image_url, timeout=20) as resp:
            if resp.status != 200:
                return None
            data = await resp.read()

        pil_img = Image.open(io.BytesIO(data)).convert("RGB")
        img = np.array(pil_img)

        h, w, _ = img.shape
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)

        ocr = pytesseract.image_to_data(gray, output_type=pytesseract.Output.DICT)

        crop_top = 0
        crop_bottom = h

        detected_top = []
        detected_bottom = []

        for i, text in enumerate(ocr["text"]):
            text = text.strip()
            if not text:
                continue

            if DATE_REGEX.search(text):
                y = ocr["top"][i]
                bh = ocr["height"][i]

                # –≤–µ—Ä—Ö–Ω—è—è –∑–æ–Ω–∞
                if y < h * 0.45:
                    detected_top.append(y + bh)

                # –Ω–∏–∂–Ω—è—è –∑–æ–Ω–∞
                if y > h * 0.55:
                    detected_bottom.append(y)

        # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –≤—ã—á–∏—Å–ª—è–µ–º –∑–æ–Ω—ã –æ–±—Ä–µ–∑–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

        if detected_top:
            crop_top = max(detected_top) + 30  # –∑–∞–ø–∞—Å –≤–Ω–∏–∑

        if detected_bottom:
            crop_bottom = min(detected_bottom) - 30  # –∑–∞–ø–∞—Å –≤–≤–µ—Ä—Ö

        # –∑–∞—â–∏—Ç–∞ –æ—Ç —Å–ª–∏—à–∫–æ–º —Å–∏–ª—å–Ω–æ–≥–æ –æ–±—Ä–µ–∑–∞–Ω–∏—è
        if crop_bottom - crop_top < h * 0.45:
            return None

        cropped = img[crop_top:crop_bottom, 0:w]

        # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ –ø—É—Å—Ç–æ
        if cropped.size == 0:
            return None

        final_img = Image.fromarray(cropped)
        buffer = io.BytesIO()
        final_img.save(buffer, format="JPEG", quality=95)
        buffer.seek(0)

        return buffer

    except Exception as e:
        print("OCR crop error:", e)
        return None












def normalize_link(link: str) -> str:
    if not link:
        return ""

    link = link.strip()

    # —É–±–∏—Ä–∞–µ–º /s/ –∏–∑ telegram preview
    link = link.replace("https://t.me/s/", "https://t.me/")

    # —É–±–∏—Ä–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã ?...
    link = link.split("?")[0]

    # —É–±–∏—Ä–∞–µ–º –∑–∞–≤–µ—Ä—à–∞—é—â–∏–π /
    link = link.rstrip("/")

    return link






def load_posted() -> set:
    if POSTED_FILE.exists():
        try:
            with open(POSTED_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()




def save_posted(posted: set):
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)

        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(list(posted), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è posted_links: {e}")



URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    "—è–Ω–≤–∞—Ä—è": 1,
    "—Ñ–µ–≤—Ä–∞–ª—è": 2,
    "–º–∞—Ä—Ç–∞": 3,
    "–∞–ø—Ä–µ–ª—è": 4,
    "–º–∞—è": 5,
    "–∏—é–Ω—è": 6,
    "–∏—é–ª—è": 7,
    "–∞–≤–≥—É—Å—Ç–∞": 8,
    "—Å–µ–Ω—Ç—è–±—Ä—è": 9,
    "–æ–∫—Ç—è–±—Ä—è": 10,
    "–Ω–æ—è–±—Ä—è": 11,
    "–¥–µ–∫–∞–±—Ä—è": 12,
}
MONTHS_SHORT = {
    "—è–Ω–≤": 1,
    "—Ñ–µ–≤": 2,
    "–º–∞—Ä": 3,
    "–∞–ø—Ä": 4,
    "–º–∞–π": 5,
    "–∏—é–Ω": 6,
    "–∏—é–ª": 7,
    "–∞–≤–≥": 8,
    "—Å–µ–Ω": 9,
    "–æ–∫—Ç": 10,
    "–Ω–æ—è": 11,
    "–¥–µ–∫": 12,
}

EVENT_WORDS = [
    "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è",
    "conference",
    "—Ñ–æ—Ä—É–º",
    "forum",
    "summit",
    "—Å–∞–º–º–∏—Ç",
    "meetup",
    "–º–∏—Ç–∞–ø",
    "—Ö–∞–∫–∞—Ç–æ–Ω",
    "hackathon",
    "–≤–æ—Ä–∫—à–æ–ø",
    "workshop",
    "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å",
    "masterclass",
    "–≤–µ–±–∏–Ω–∞—Ä",
    "webinar",
    "—Å–µ–º–∏–Ω–∞—Ä",
    "pitch",
    "–ø–∏—Ç—á",
    "demo day",
    "–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä",
    "accelerator",
    "bootcamp",
    "–±—É—Ç–∫–µ–º–ø",
    "–≤—ã—Å—Ç–∞–≤–∫–∞",
    "–∫–æ–Ω–∫—É—Ä—Å",
    "competition",
    "—Ç—Ä–µ–Ω–∏–Ω–≥",
    "training",
    "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ",
    "–∏–≤–µ–Ω—Ç",
    "event",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º",
    "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è",
    "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è",
]
NOT_EVENT_WORDS = [
    "research",
    "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ",
    "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª",
    "–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥",
    "–º–ª–Ω $",
    "–º–ª—Ä–¥ $",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω",
    "—É–≤–æ–ª–µ–Ω",
    "–æ—Ç—á–µ—Ç",
    "–≤—ã—Ä—É—á–∫–∞",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞",
    "–±–∏—Ä–∂–∞",
    "–∞–∫—Ü–∏–∏",
    "—Ç–æ–∫–∞–µ–≤",
    "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ",
]
SITE_STOP_WORDS = [
    "–∫–æ–Ω—Ç–∞–∫—Ç—ã",
    "–æ –Ω–∞—Å",
    "–ø–æ–ª–∏—Ç–∏–∫–∞",
    "–≤–æ–π—Ç–∏",
    "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞",
    "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è",
    "–ø–æ–∏—Å–∫",
    "–≥–ª–∞–≤–Ω–∞—è",
    "–º–µ–Ω—é",
    "–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏",
    "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ",
    "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ",
    "privacy",
    "terms",
    "cookie",
]
DESCRIPTION_SIGNALS = [
    "—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏",
    "–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤",
    "–≤—ã —É–∑–Ω–∞–µ—Ç–µ",
    "–º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º",
    "–Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏",
    "–≤ —Ä–∞–º–∫–∞—Ö",
    "—Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤—Å—Ç—Ä–µ—á–∞",
    "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º –≤–∞—Å",
    "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å",
    "–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Å—Å—ã–ª–∫–µ",
    "—Å–≤–æ–±–æ–¥–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ",
    "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç –≤–∞—Å –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ",
    "–≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–π—Ç–∏",
]

KZ_CITIES = {
    "–∞–ª–º–∞—Ç—ã": "–ê–ª–º–∞—Ç—ã",
    "–∞—Å—Ç–∞–Ω–∞": "–ê—Å—Ç–∞–Ω–∞",
    "—à—ã–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç",
    "–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω": "–ê—Å—Ç–∞–Ω–∞",
    "—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",
    "–∫—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞",
    "–∞–∫—Ç–æ–±–µ": "–ê–∫—Ç–æ–±–µ",
    "—Ç–∞—Ä–∞–∑": "–¢–∞—Ä–∞–∑",
    "–ø–∞–≤–ª–æ–¥–∞—Ä": "–ü–∞–≤–ª–æ–¥–∞—Ä",
    "—Å–µ–º–µ–π": "–°–µ–º–µ–π",
    "–∞—Ç—ã—Ä–∞—É": "–ê—Ç—ã—Ä–∞—É",
    "–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "–∂–µ–∑“õ–∞–∑“ì–∞–Ω": "–ñ–µ–∑“õ–∞–∑“ì–∞–Ω",
    "–∞–∫—Ç–∞—É": "–ê–∫—Ç–∞—É",
    "–æ–Ω–ª–∞–π–Ω": "–û–Ω–ª–∞–π–Ω",
    "online": "–û–Ω–ª–∞–π–Ω",
    "zoom": "–û–Ω–ª–∞–π–Ω (Zoom)",
    "—Ç–∞—à–∫–µ–Ω—Ç": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω",
}

EMOJI_RE = re.compile(
    "[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]",
    re.UNICODE,
)


# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def parse_date(text: str) -> Optional[datetime]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É. –ù–ï –ø—Ä–∏–±–∞–≤–ª—è–µ—Ç +1 –≥–æ–¥ –∫ –ø—Ä–æ—à–µ–¥—à–∏–º –¥–∞—Ç–∞–º:
    –µ—Å–ª–∏ –≥–æ–¥ –Ω–µ —É–∫–∞–∑–∞–Ω –∏ –¥–∞—Ç–∞ –ø—Ä–æ—à–ª–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None.
    """
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try:
            return datetime(year, month, day)
        except Exception:
            return None

    # –î–î-–î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month:
            return make_dt(year, month, int(m.group(2)))

    # –î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î –ú–µ—Å[—Å–æ–∫—Ä] [–ì–ì–ì–ì]
    m = re.search(
        r"(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?",
        t,
    )
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    # –î–î.–ú–ú[.–ì–ì–ì–ì]
    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12:
            return make_dt(year, month, int(m.group(1)))

    return None


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: "—è–Ω–≤–∞—Ä—è",
        2: "—Ñ–µ–≤—Ä–∞–ª—è",
        3: "–º–∞—Ä—Ç–∞",
        4: "–∞–ø—Ä–µ–ª—è",
        5: "–º–∞—è",
        6: "–∏—é–Ω—è",
        7: "–∏—é–ª—è",
        8: "–∞–≤–≥—É—Å—Ç–∞",
        9: "—Å–µ–Ω—Ç—è–±—Ä—è",
        10: "–æ–∫—Ç—è–±—Ä—è",
        11: "–Ω–æ—è–±—Ä—è",
        12: "–¥–µ–∫–∞–±—Ä—è",
    }
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s


def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    known = [
        "Narxoz",
        "Nazarbayev",
        "KBTU",
        "–ö–ë–¢–£",
        "Astana Hub",
        "IT Park",
        "MOST IT Hub",
        "Holiday Inn",
        "Esentai",
        "Yandex",
        "Smart Point",
        "Almaty Arena",
    ]
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf"{re.escape(v)}[^\n,.]*", text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at = re.search(r"@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)", text)
    if at:
        return at.group(1).strip()[:60]
    return None


def is_real_event(text: str) -> bool:
    t = text.lower()
    return (any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS))


def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)


def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)


def dedup_title(title: str) -> str:
    half = len(title) // 2
    for i in range(10, half):
        if title[i:].strip() == title[:len(title)-i].strip():
            return title[:len(title)-i].strip()
    return title


def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    # 16:00–û–Ω–ª–∞–π–Ω -> 16:00 –û–Ω–ª–∞–π–Ω
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-z–ê-–Ø–∞-—è–Å—ë])", r"\1 ", s)
    # "–§–µ–≤,16:00" -> "–§–µ–≤, 16:00"
    s = re.sub(r"([–∞-—è—ë–ê-–Ø–Å]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)
    # –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
    s = re.sub(r"\s{2,}", " ", s)
    return s


def strip_leading_datetime_from_title(title: str) -> str:
    """
    –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π: –µ—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –¥–∞—Ç—ã/–≤—Ä–µ–º–µ–Ω–∏ ‚Äî —Å—Ä–µ–∑–∞–µ–º.
    –ü—Ä–∏–º–µ—Ä—ã:
    "24 –§–µ–≤, 16:00 –û–Ω–ª–∞–π–Ω –í–Ω–µ–¥—Ä–µ–Ω–∏–µ..." -> "–û–Ω–ª–∞–π–Ω –í–Ω–µ–¥—Ä–µ–Ω–∏–µ..."
    "24 —Ñ–µ–≤—Ä–∞–ª—è 16:00–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..." -> "–í–Ω–µ–¥—Ä–µ–Ω–∏–µ..."
    """
    t = strip_emoji(title).strip()

    # —É–±–∏—Ä–∞–µ–º –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–æ–µ –≤—Ä–µ–º—è –≤–Ω—É—Ç—Ä–∏
    t = normalize_glued_text(t)

    # 24 —Ñ–µ–≤, 16:00 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[–ê-–Ø–∞-—è–Å—ëA-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24 —Ñ–µ–≤—Ä–∞–ª—è 2026 ...
    t = re.sub(
        r"^\s*\d{1,2}\s+[–∞-—è—ë]{3,}(?:\s+\d{4})?\s*",
        "",
        t,
        flags=re.IGNORECASE,
    )
    # 24.02.2026 ...
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)

    return t.strip(" -‚Äì‚Ä¢.,").strip()










def clean_title_deterministic(raw_title: str) -> Optional[str]:

    s = strip_leading_datetime_from_title(raw_title)
    s = fix_glued_words(s)
    s = dedup_title(s)
    s = remove_city_from_title(s)

    # –æ–±—Ä–µ–∑–∞–µ–º —Ö–≤–æ—Å—Ç –æ–ø–∏—Å–∞–Ω–∏—è
    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx != -1 and idx > 12:
            s = s[:idx].strip(" -‚Äì‚Ä¢.,")
            break

    s = re.sub(r"\s{2,}", " ", s).strip()

    if len(s) < 5:
        return None

    if looks_like_description(s):
        return None

    return s[:120]







# ‚îÄ‚îÄ‚îÄ Parse glued line: "09 –§–µ–≤, 17:00–®—ã–º–∫–µ–Ω—Ç –ù–∞–∑–≤–∞–Ω–∏–µ" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

_GLUE_RE = re.compile(
    r"^(\d{1,2})\s+"                      # day
    r"([–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,})"             # month
    r"[,\s]+"
    r"(\d{1,2}:\d{2})"                    # time
    r"\s*"
    r"(?:(–û–Ω–ª–∞–π–Ω|online|zoom|–ê–ª–º–∞—Ç—ã|–ê—Å—Ç–∞–Ω–∞|–®—ã–º–∫–µ–Ω—Ç|–ñ–µ–∑–∫–∞–∑–≥–∞–Ω|–ñ–µ–∑“õ–∞–∑“ì–∞–Ω|–ö–∞—Ä–∞–≥–∞–Ω–¥–∞|–ö–æ—Å—Ç–∞–Ω–∞–π|–ü–∞–≤–ª–æ–¥–∞—Ä|–°–µ–º–µ–π|–ê—Ç—ã—Ä–∞—É|–ê–∫—Ç–∞—É|–ê–∫—Ç–æ–±–µ|–¢–∞—Ä–∞–∑|–ö—ã–∑—ã–ª–æ—Ä–¥–∞)\s*)?"
    r"(.+)$",                             # title rest
    re.IGNORECASE,
)


def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m:
        return None

    day_s, month_s = m.group(1), m.group(2).lower()
    time_str = m.group(3)
    possible_city = (m.group(4) or "").strip()
    title_raw = m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    try:
        dt = datetime(datetime.now().year, month_num, int(day_s))
    except Exception:
        return None

    if not is_future(dt):
        return None

    city = None
    if possible_city:
        city = KZ_CITIES.get(possible_city.lower(), possible_city)

    title_raw = dedup_title(title_raw)

    if len(title_raw) < 5:
        return None

    return {
        "dt": dt,
        "time_str": time_str,
        "city": city,
        "title_raw": title_raw[:300],
        "date_formatted": format_date(dt, time_str),
    }


# ‚îÄ‚îÄ‚îÄ Formatting post (NO date in title) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def make_post(event: Dict) -> str:
    title = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get("location", "")
    venue = event.get("venue", "")

    # –≤–∞–∂–Ω–æ: –¥–∞—Ç–∞ –≤ –∑–∞–≥–æ–ª–æ–≤–∫–µ —É–±–∏—Ä–∞–µ—Ç—Å—è –≤—Å–µ–≥–¥–∞
    title = strip_leading_datetime_from_title(title)

    lines = [f"üéØ <b>{title}</b>"]

    if location in ("–û–Ω–ª–∞–π–Ω", "–û–Ω–ª–∞–π–Ω (Zoom)"):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue:
        lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {date_str}")
    lines.append(f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)















# ‚îÄ‚îÄ‚îÄ Bot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={"User-Agent": "Mozilla/5.0"})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    # ‚îÄ‚îÄ Digest parsing ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(
                r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?"
                r"|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*"
                r"(?:\s+\d{4})?)",
                line,
                re.IGNORECASE,
            )
            if not dm:
                i += 1
                continue

            date_raw = dm.group(0)
            rest = line[dm.end():].strip()

            tm = re.search(r"(?:–≤\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm:
                rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -‚Äì‚Ä¢")

            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"):
                    link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith("http"):
                            link = "https://" + link
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt):
                    title_raw = nxt

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                logger.info(f"‚è≠Ô∏è –ü—Ä–æ—à–µ–¥—à–µ–µ (–¥–∞–π–¥–∂–µ—Å—Ç): {title_raw[:40]}")
                i += 1
                continue

            ctx = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_location(ctx) or extract_location(text)

            title_clean = clean_title_deterministic(title_raw) or dedup_title(title_raw[:120])
            if not title_clean:
                i += 1
                continue

            events.append(
                {
                    "title": title_clean,
                    "date": format_date(dt, time_str),
                    "location": location or "",
                    "venue": extract_venue(ctx),
                    "link": link or post_link,
                    "source": source,
                    "image_url": image_url,
                }
            )
            i += 1
        return events

    # ‚îÄ‚îÄ Telegram channels ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td:
                    continue

                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30:
                    continue

                le = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"
                norm_link = normalize_link(post_link)

# üî• –ò–©–ï–ú –ü–ï–†–í–û–ò–°–¢–û–ß–ù–ò–ö –í –¢–ï–ö–°–¢–ï
                external_link = None
                links_in_text = re.findall(r"(https?://[^\s]+)", text)

                for l in links_in_text:
                    clean_l = normalize_link(l)
                    if "t.me" not in clean_l:   # –∏—Å–∫–ª—é—á–∞–µ–º telegram
                        external_link = clean_l
                        break

# –µ—Å–ª–∏ –Ω–∞—à–ª–∏ –≤–Ω–µ—à–Ω–∏–π –∏—Å—Ç–æ—á–Ω–∏–∫ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
                final_link = external_link if external_link else norm_link

                # ‚ùó –ü–†–û–í–ï–†–ö–ê –¢–û–õ–¨–ö–û –ó–î–ï–°–¨
                if norm_link in self.posted:
                    continue
                
                # üî• –ò–©–ï–ú –ü–ï–†–í–û–ò–°–¢–û–ß–ù–ò–ö
                external_link = None

# 1Ô∏è‚É£ —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º <a href=...>
                for a in td.find_all("a", href=True):
                    href = normalize_link(a["href"])
                    if "t.me" not in href:
                        external_link = href
                        break

# 2Ô∏è‚É£ –µ—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ ‚Äî fallback –Ω–∞ regex
                if not external_link:
                    links_in_text = re.findall(r"(https?://[^\s]+)", text)
                    for l in links_in_text:
                        clean_l = normalize_link(l)
                        if "t.me" not in clean_l:
                            external_link = clean_l
                            break

# 3Ô∏è‚É£ –≤—ã–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Å—ã–ª–∫—É
                final_link = external_link if external_link else norm_link

                image_url = None

# Telegram photo preview
                photo_wrap = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if photo_wrap:
                    style = photo_wrap.get("style", "")
                    match = re.search(r"url\('([^']+)'\)", style)
                    if match:
                        image_url = match.group(1)

# –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî –ø–æ–ø—Ä–æ–±—É–µ–º img –≤–Ω—É—Ç—Ä–∏ —Ç–µ–∫—Å—Ç–∞
                if not image_url:
                    img_tag = td.find("img")
                    if img_tag and img_tag.get("src"):
                        image_url = img_tag["src"]

                # –î–∞–π–¥–∂–µ—Å—Ç
                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    continue

                if not is_real_event(text):
                    continue

                dt = parse_date(text)
                if not is_future(dt):
                    continue

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫
                title_candidate = None
                for ln in text.split("\n"):
                    ln = strip_emoji(ln).strip()
                    if len(ln) > 10:
                        title_candidate = ln
                        break

                title = clean_title_deterministic(title_candidate or "")
                if not title:
                    continue

                tm2 = re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append(
                    {
                        "title": title,
                        "date": format_date(dt, time_str),
                        "location": extract_location(text) or "",
                        "venue": extract_venue(text),
                        "link": final_link,
                        "source": channel["name"],
                        "image_url": image_url,
                    }
                )

            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue

        return all_events
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)

        return all_events




   

    
    # ‚îÄ‚îÄ Sites ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_site(self, site: Dict) -> List[Dict]:

        html = await self.fetch(site["url"])
        if not html:
            return []

        soup = BeautifulSoup(html, "html.parser")
        events = []

        for link in soup.find_all("a", href=True)[:80]:
            try:
                href = link.get("href", "")
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue

                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)

                # üî• –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Å—Å—ã–ª–∫–∏
                href = normalize_link(href)

                if href.rstrip("/") == normalize_link(site["url"]).rstrip("/"):
                    continue

                # üî• –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª—è —á–µ—Ä–µ–∑ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—É—é —Å—Å—ã–ª–∫—É
                if href in self.posted:
                    continue

                if is_site_trash(title_raw):
                    continue

                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt):
                    continue

                image_url = None


# –æ–±—ã—á–Ω—ã–π img
                image_url = None

                if parent:
                    imgs = parent.find_all("img")

                    for img in imgs:
                        src = img.get("src") or img.get("data-src")
                        if not src:
                            continue

                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)

                        # —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ—Å—Ç–µ—Ä—ã
                        if is_clean_photo(src):
                            image_url = src
                            break

                # fallback ‚Äî background-image
                if not image_url and parent:
                    style = parent.get("style", "")
                    match = re.search(r"url\(['\"]?([^'\")]+)", style)

                    if match:
                        src = match.group(1)

                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)

                        if is_clean_photo(src):
                            image_url = src


                title_clean = (
                    clean_title_deterministic(title_raw)
                    or strip_emoji(dedup_title(title_raw))[:120]
                )

                events.append(
                    {
                        "title": title_clean,
                        "date": format_date(dt),
                        "location": extract_location(context) or "",
                        "venue": extract_venue(context),
                        "link": href,
                        "source": site["name"],
                        "image_url": image_url,
                    }
                )

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events
        
        




    






# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")

    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot_api = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")
        logger.info(f"üì¶ –£–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {len(bot_obj.posted)}")

        posted = 0

        for event in unique[:15]:

            norm_link = normalize_link(event.get("link", ""))

            # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª—è ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            if norm_link in bot_obj.posted:
                logger.info(f"‚è≠Ô∏è –£–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ—Å—å: {event.get('title')[:50]}")
                continue

            text = make_post(event)
            if not text:
                continue

            try:
                photo_to_send = None

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ OCR –∞–≤—Ç–æ-–æ–±—Ä–µ–∑–∫–∞ —Ç–µ–∫—Å—Ç–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if event.get("image_url"):
                    session = await bot_obj.get_session()
                    photo_to_send = await smart_crop_text_zones(
                        session,
                        event["image_url"]
                    )

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –û—Ç–ø—Ä–∞–≤–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if photo_to_send:
                    await bot_api.send_photo(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        photo=photo_to_send,
                        caption=text,
                        parse_mode="HTML",
                    )
                else:
                    await bot_api.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ state —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –æ—Ç–ø—Ä–∞–≤–∫–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                bot_obj.posted.add(norm_link)
                save_posted(bot_obj.posted)

                posted += 1
                logger.info(f"‚úÖ ({posted}) {event.get('title','')[:50]}")

                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –Ω–æ–≤—ã—Ö: {posted}")

    finally:
        await bot_obj.close()


if __name__ == "__main__":
    asyncio.run(main())
