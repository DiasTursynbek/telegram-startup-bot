import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
import json
from pathlib import Path

STATE_FILE = Path("state/posted.json")
STATE_FILE.parent.mkdir(parents=True, exist_ok=True)

def load_posted() -> set:
    if STATE_FILE.exists():
        try:
            return set(json.loads(STATE_FILE.read_text(encoding="utf-8")))
        except Exception:
            return set()
    return set()

def save_posted(posted: set) -> None:
    STATE_FILE.write_text(
        json.dumps(sorted(posted), ensure_ascii=False, indent=2),
        encoding="utf-8"
    )



logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID', "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv('MESSAGE_THREAD_ID', '4'))
CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY', '')

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
    '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
    '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12,
}
MONTHS_SHORT = {
    '—è–Ω–≤': 1, '—Ñ–µ–≤': 2, '–º–∞—Ä': 3, '–∞–ø—Ä': 4,
    '–º–∞–π': 5, '–∏—é–Ω': 6, '–∏—é–ª': 7, '–∞–≤–≥': 8,
    '—Å–µ–Ω': 9, '–æ–∫—Ç': 10, '–Ω–æ—è': 11, '–¥–µ–∫': 12,
}

EVENT_WORDS = [
    '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'conference', '—Ñ–æ—Ä—É–º', 'forum', 'summit', '—Å–∞–º–º–∏—Ç',
    'meetup', '–º–∏—Ç–∞–ø', '—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon', '–≤–æ—Ä–∫—à–æ–ø', 'workshop',
    '–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å', 'masterclass', '–≤–µ–±–∏–Ω–∞—Ä', 'webinar', '—Å–µ–º–∏–Ω–∞—Ä',
    'pitch', '–ø–∏—Ç—á', 'demo day', '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', 'accelerator',
    'bootcamp', '–±—É—Ç–∫–µ–º–ø', '–≤—ã—Å—Ç–∞–≤–∫–∞', '–∫–æ–Ω–∫—É—Ä—Å', 'competition',
    '—Ç—Ä–µ–Ω–∏–Ω–≥', 'training', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '–∏–≤–µ–Ω—Ç', 'event',
    '–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç', '–ø—Ä–∏–≥–ª–∞—à–∞–µ–º', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è',
]

NOT_EVENT_WORDS = [
    'research', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ', '–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª', '–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥',
    '–º–ª–Ω $', '–º–ª—Ä–¥ $', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '—É–≤–æ–ª–µ–Ω', '–æ—Ç—á–µ—Ç', '–≤—ã—Ä—É—á–∫–∞',
    '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '–±–∏—Ä–∂–∞', '–∞–∫—Ü–∏–∏', '—Ç–æ–∫–∞–µ–≤', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ',
]

SITE_STOP_WORDS = [
    '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–æ –Ω–∞—Å', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–≤–æ–π—Ç–∏', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞',
    '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–ø–æ–∏—Å–∫', '–≥–ª–∞–≤–Ω–∞—è', '–º–µ–Ω—é', '–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏',
    '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ', 'privacy', 'terms', 'cookie',
]

KZ_CITIES = {
    '–∞–ª–º–∞—Ç—ã': '–ê–ª–º–∞—Ç—ã', '–∞—Å—Ç–∞–Ω–∞': '–ê—Å—Ç–∞–Ω–∞', '—à—ã–º–∫–µ–Ω—Ç': '–®—ã–º–∫–µ–Ω—Ç',
    '–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω': '–ê—Å—Ç–∞–Ω–∞', '—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫': '–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫',
    '–∫—ã–∑—ã–ª–æ—Ä–¥–∞': '–ö—ã–∑—ã–ª–æ—Ä–¥–∞', '–∞–∫—Ç–æ–±–µ': '–ê–∫—Ç–æ–±–µ', '—Ç–∞—Ä–∞–∑': '–¢–∞—Ä–∞–∑',
    '–ø–∞–≤–ª–æ–¥–∞—Ä': '–ü–∞–≤–ª–æ–¥–∞—Ä', '—Å–µ–º–µ–π': '–°–µ–º–µ–π', '–∞—Ç—ã—Ä–∞—É': '–ê—Ç—ã—Ä–∞—É',
    '–∂–µ–∑–∫–∞–∑–≥–∞–Ω': '–ñ–µ–∑“õ–∞–∑“ì–∞–Ω', '–∂–µ–∑“õ–∞–∑“ì–∞–Ω': '–ñ–µ–∑“õ–∞–∑“ì–∞–Ω',
    '–æ–Ω–ª–∞–π–Ω': '–û–Ω–ª–∞–π–Ω', 'online': '–û–Ω–ª–∞–π–Ω', 'zoom': '–û–Ω–ª–∞–π–Ω (Zoom)',
    '—Ç–∞—à–∫–µ–Ω—Ç': '–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω',
}

EMOJI_RE = re.compile(
    '[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]',
    re.UNICODE
)

# ‚îÄ‚îÄ‚îÄ Claude error memory: –ø–æ–º–Ω–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –æ—à–∏–±–æ–∫ –∏ –Ω–µ –ø–æ–≤—Ç–æ—Ä—è–µ–º ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_bad_patterns: List[str] = []


def remember_bad_pattern(text: str):
    """–ó–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ —á—Ç–æ–±—ã –ø—Ä–æ–ø—É—Å–∫–∞—Ç—å –≤ –±—É–¥—É—â–µ–º."""
    # –ë–µ—Ä—ë–º –ø–µ—Ä–≤—ã–µ 30 —Å–∏–º–≤–æ–ª–æ–≤ –∫–∞–∫ ¬´–æ—Ç–ø–µ—á–∞—Ç–æ–∫¬ª
    key = re.sub(r'\s+', ' ', text[:30].lower()).strip()
    if key and key not in _bad_patterns:
        _bad_patterns.append(key)
        if len(_bad_patterns) > 100:
            _bad_patterns.pop(0)


def matches_bad_pattern(text: str) -> bool:
    key = re.sub(r'\s+', ' ', text[:30].lower()).strip()
    return any(key.startswith(p[:20]) for p in _bad_patterns)


# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub('', s).strip()


def parse_date(text: str) -> Optional[datetime]:
    """
    –ü–∞—Ä—Å–∏—Ç –¥–∞—Ç—É.
    –í–ê–ñ–ù–û: –µ—Å–ª–∏ –≥–æ–¥ –ù–ï —É–∫–∞–∑–∞–Ω –∏ –¥–∞—Ç–∞ —É–∂–µ –ø—Ä–æ—à–ª–∞ ‚Äî –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç None (–Ω–µ —Å—á–∏—Ç–∞–µ–º –±—É–¥—É—â–∏–º).
    """
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try:
            return datetime(year, month, day)
        except Exception:
            return None

    # –î–î-–î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì] -> –±–µ—Ä–µ–º –≤—Ç–æ—Ä—É—é –¥–∞—Ç—É –¥–∏–∞–ø–∞–∑–æ–Ω–∞
    m = re.search(r'(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?', t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month:
            dt = make_dt(year, month, int(m.group(2)))
            if not dt:
                return None
            if not m.group(4) and dt.date() <= now.date():
                return None
            return dt

    # –î–î –ú–µ—Å—è—Ü [–ì–ì–ì–ì]
    m = re.search(r'(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?', t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        year = int(m.group(3)) if m.group(3) else now.year
        if month:
            dt = make_dt(year, month, int(m.group(1)))
            if not dt:
                return None
            if not m.group(3) and dt.date() <= now.date():
                return None
            return dt

    # –î–î –ú–µ—Å[—Å–æ–∫—Ä] [–ì–ì–ì–ì]
    m = re.search(r'(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?', t)
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        year = int(m.group(3)) if m.group(3) else now.year
        if month:
            dt = make_dt(year, month, int(m.group(1)))
            if not dt:
                return None
            if not m.group(3) and dt.date() <= now.date():
                return None
            return dt

    # –î–î.–ú–ú[.–ì–ì–ì–ì]
    m = re.search(r'(\d{1,2})\.(\d{2})(?:\.(\d{4}))?', t)
    if m:
        day = int(m.group(1))
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12:
            dt = make_dt(year, month, day)
            if not dt:
                return None
            if not m.group(3) and dt.date() <= now.date():
                return None
            return dt

    return None


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: '—è–Ω–≤–∞—Ä—è', 2: '—Ñ–µ–≤—Ä–∞–ª—è', 3: '–º–∞—Ä—Ç–∞', 4: '–∞–ø—Ä–µ–ª—è',
        5: '–º–∞—è', 6: '–∏—é–Ω—è', 7: '–∏—é–ª—è', 8: '–∞–≤–≥—É—Å—Ç–∞',
        9: '—Å–µ–Ω—Ç—è–±—Ä—è', 10: '–æ–∫—Ç—è–±—Ä—è', 11: '–Ω–æ—è–±—Ä—è', 12: '–¥–µ–∫–∞–±—Ä—è',
    }
    result = f"{dt.day} {months[dt.month]} {dt.year}"
    if time_str:
        result += f", {time_str}"
    return result


def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    venues = [
        'Narxoz', 'Nazarbayev', 'KBTU', '–ö–ë–¢–£', 'Astana Hub',
        'IT Park', 'MOST IT Hub', 'Holiday Inn', 'Esentai',
        'Yandex', 'Smart Point', 'Almaty Arena',
    ]
    for v in venues:
        if v.lower() in text.lower():
            m = re.search(rf'{v}[^\n,.]*', text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', text)
    if at_m:
        return at_m.group(1).strip()[:60]
    return None


def is_real_event(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS)


def is_site_trash(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in SITE_STOP_WORDS)


# ‚îÄ‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–∏ "09 –§–µ–≤, 17:00–®—ã–º–∫–µ–Ω—Ç –ù–∞–∑–≤–∞–Ω–∏–µ" ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_glued_line(line: str) -> Optional[Dict]:
    """
    –†–∞–∑–±–∏—Ä–∞–µ—Ç —Å—Ç—Ä–æ–∫—É –≤–∏–¥–∞ '09 –§–µ–≤, 17:00–®—ã–º–∫–µ–Ω—Ç Pre-incubation Bootcamp'
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict {date_raw, time_str, city, title} –∏–ª–∏ None.
    """
    line = strip_emoji(line).strip()

    # –ü–∞—Ç—Ç–µ—Ä–Ω: –î–î –ú–µ—Å[,] –ß–ß:–ú–ú[–ì–æ—Ä–æ–¥]–ó–∞–≥–æ–ª–æ–≤–æ–∫
    m = re.match(
        r'^(\d{1,2})\s+([–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,})[,\s]+(\d{1,2}:\d{2})'  # –¥–∞—Ç–∞ + –≤—Ä–µ–º—è
        r'([–ê-–Ø–Å][–∞-—è—ë]+(?:\s[–ê-–Ø–Å][–∞-—è—ë]+)?)?'                       # –æ–ø—Ü. –≥–æ—Ä–æ–¥
        r'\s*(.+)$',                                                     # –∑–∞–≥–æ–ª–æ–≤–æ–∫
        line
    )
    if not m:
        return None

    day_raw = m.group(1)
    month_raw = m.group(2).lower()[:3]
    time_str = m.group(3)
    possible_city = (m.group(4) or '').strip()
    title_raw = m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_raw, 0)
    if not month_num:
        # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ
        for k, v in MONTHS_RU.items():
            if m.group(2).lower().startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    year = datetime.now().year
    try:
        dt = datetime(year, month_num, int(day_raw))
        if dt.date() < datetime.now().date():
            dt = datetime(year + 1, month_num, int(day_raw))
    except Exception:
        return None

    # –ï—Å–ª–∏ possible_city ‚Äî —Ä–µ–∞–ª—å–Ω—ã–π –≥–æ—Ä–æ–¥
    city = None
    if possible_city:
        city_key = possible_city.lower()
        city = KZ_CITIES.get(city_key)

    # –ß–∏—Å—Ç–∏–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç —è–≤–Ω—ã—Ö –¥—É–±–ª–µ–π
    if len(title_raw) > 10:
        half = len(title_raw) // 2
        if title_raw[:half].strip() == title_raw[half:].strip():
            title_raw = title_raw[:half].strip()

    if len(title_raw) < 5:
        return None

    return {
        'dt': dt,
        'time_str': time_str,
        'city': city or (possible_city if possible_city else None),
        'title': title_raw[:120],
        'date_formatted': format_date(dt, time_str),
    }


# ‚îÄ‚îÄ‚îÄ Claude API: –æ—á–∏—Å—Ç–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def claude_clean_title(raw_text: str, session: aiohttp.ClientSession) -> Optional[str]:
    """
    –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –ø–æ—Å—Ç–∞ –≤ Claude.
    Claude –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¢–û–õ–¨–ö–û —á–∏—Å—Ç—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–æ–±—ã—Ç–∏—è (1 —Å—Ç—Ä–æ–∫–∞) –∏–ª–∏ 'SKIP'.
    –ï—Å–ª–∏ 'SKIP' ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–æ—Å—Ç.
    """
    if not CLAUDE_API_KEY:
        return None

    prompt = (
        "–ò–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ –∏–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è (–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏, –º–∏—Ç–∞–ø–∞, —Ö–∞–∫–∞—Ç–æ–Ω–∞ –∏ —Ç.–¥.).\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "- –í–µ—Ä–Ω–∏ –û–î–ù–£ —Å—Ç—Ä–æ–∫—É ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ, –±–µ–∑ –¥–∞—Ç—ã, –≥–æ—Ä–æ–¥–∞, —Å—Å—ã–ª–æ–∫\n"
        "- –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–ª–∏–ø—à–µ–µ—Å—è (–Ω–∞–ø—Ä. 'Bootcamp–ï—Å—Ç—å –∏–¥–µ—è'), —Ä–∞–∑–±–µ–π –ø—Ä–æ–±–µ–ª–æ–º\n"
        "- –ï—Å–ª–∏ –¥—É–±–ª—å –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ (–Ω–∞–ø—Ä. 'Data CommunityData Community'), —É–±–µ—Ä–∏ –ø–æ–≤—Ç–æ—Ä\n"
        "- –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –∏–ª–∏ –Ω–µ–ª—å–∑—è –∏–∑–≤–ª–µ—á—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ ‚Äî –æ—Ç–≤–µ—Ç—å: SKIP\n"
        "- –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ, —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ\n\n"
        f"–¢–µ–∫—Å—Ç:\n{raw_text[:600]}"
    )

    try:
        async with session.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": CLAUDE_API_KEY,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json",
            },
            json={
                "model": "claude-haiku-4-5-20251001",
                "max_tokens": 80,
                "messages": [{"role": "user", "content": prompt}],
            },
            timeout=10,
        ) as resp:
            if resp.status != 200:
                return None
            data = await resp.json()
            result = data["content"][0]["text"].strip()
            if result.upper() == "SKIP" or len(result) < 5:
                remember_bad_pattern(raw_text)
                return None
            return result[:120]
    except Exception as e:
        logger.error(f"Claude API error: {e}")
        return None


# ‚îÄ‚îÄ‚îÄ –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ—Å—Ç–∞ (4-5 —Å—Ç—Ä–æ–∫) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def make_post(event: Dict) -> str:
    """
    –§–æ—Ä–º–∞—Ç:
    üéØ –ù–∞–∑–≤–∞–Ω–∏–µ
    üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω[, üèô –ì–æ—Ä–æ–¥]
    üìç –ú–µ—Å—Ç–æ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    üìÖ –î–∞—Ç–∞[, –≤—Ä–µ–º—è]
    üîó –ß–∏—Ç–∞—Ç—å ‚Üí
    """
    title = (event.get('title') or '').strip()
    if not title or len(title) < 5:
        return ""
    if not event.get('date'):
        return ""

    location = event.get('location', '')
    venue = event.get('venue', '')

    lines = [f"üéØ <b>{title}</b>"]

    if location in ('–û–Ω–ª–∞–π–Ω', '–û–Ω–ª–∞–π–Ω (Zoom)'):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue:
        lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {event['date']}")
    lines.append(f"üîó <a href='{event['link']}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ –û—Å–Ω–æ–≤–Ω–æ–π –±–æ—Ç ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={'User-Agent': 'Mozilla/5.0'})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            session = await self.get_session()
            async with session.get(url, timeout=15) as resp:
                return await resp.text() if resp.status == 200 else ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {url} - {e}")
            return ""

    # ‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–π–¥–∂–µ—Å—Ç-–ø–æ—Å—Ç–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            date_match = re.match(
                r'^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?'
                r'|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*'
                r'(?:\s+\d{4})?)',
                line, re.IGNORECASE
            )
            if not date_match:
                i += 1
                continue

            date_raw = date_match.group(0)
            rest = line[date_match.end():].strip()

            time_match = re.search(r'(?:–≤\s*)?(\d{1,2}:\d{2})', rest)
            time_str = time_match.group(1) if time_match else None
            if time_match:
                rest = (rest[:time_match.start()] + rest[time_match.end():]).strip()

            title_raw = strip_emoji(rest).strip(' -‚Äì‚Ä¢')

            link = None
            lm = re.search(r'((?:https?://|t\.me/)\S+)', line)
            if lm:
                link = lm.group(1)
                if not link.startswith('http'):
                    link = 'https://' + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), '').strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm = re.search(r'((?:https?://|t\.me/)\S+)', lines[j])
                    if lm:
                        link = lm.group(1)
                        if not link.startswith('http'):
                            link = 'https://' + link
                        break

            venue = None
            at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', title_raw)
            if at_m:
                venue = at_m.group(1).strip()
                title_raw = title_raw[:at_m.start()].strip()

            if len(title_raw) < 5 and i + 1 < len(lines):
                next_line = strip_emoji(lines[i + 1]).strip()
                if len(next_line) > 5 and not re.match(r'^\d', next_line):
                    title_raw = next_line

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                i += 1
                continue

            context = line + ' ' + (lines[i + 1] if i + 1 < len(lines) else '')
            location = extract_location(context) or extract_location(text)
            if not venue:
                venue = extract_venue(context)

            events.append({
                'title': title_raw[:120],
                'date': format_date(dt, time_str),
                'location': location or '',
                'venue': venue,
                'link': link or post_link,
                'source': source,
                'image_url': image_url,
            })
            i += 1
        return events

    # ‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ Telegram-–∫–∞–Ω–∞–ª–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        url = f"https://t.me/s/{channel['username']}"
        html = await self.fetch(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        all_events = []
        session = await self.get_session()

        for msg in soup.find_all('div', class_='tgme_widget_message')[:20]:
            try:
                text_div = msg.find('div', class_='tgme_widget_message_text')
                if not text_div:
                    continue

                text = text_div.get_text(separator='\n', strip=True)
                if len(text) < 30:
                    continue

                link_elem = msg.find('a', class_='tgme_widget_message_date')
                post_link = link_elem['href'] if link_elem else f"https://t.me/{channel['username']}"

                if post_link in self.posted:
                    continue
                self.posted.add(post_link)

                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ä–∞–Ω–µ–µ –∑–∞–ø–æ–º–Ω–µ–Ω–Ω—ã–µ –ø–ª–æ—Ö–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
                if matches_bad_pattern(text):
                    logger.info(f"‚è≠Ô∏è –ü–ª–æ—Ö–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω (–∏–∑ –ø–∞–º—è—Ç–∏): {text[:50].strip()}")
                    continue

                image_url = None
                img_div = msg.find('a', class_='tgme_widget_message_photo_wrap')
                if img_div:
                    style = img_div.get('style', '')
                    m = re.search(r"url\('([^']+)'\)", style)
                    if m:
                        image_url = m.group(1)

                is_digest = bool(re.search(r'\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}', text))

                if is_digest:
                    events = self.parse_digest(text, post_link, channel['name'], image_url)
                    all_events.extend(events)
                    logger.info(f"üìã –î–∞–π–¥–∂–µ—Å—Ç {channel['name']}: {len(events)} —Å–æ–±—ã—Ç–∏–π")
                    continue

                if not is_real_event(text):
                    logger.info(f"‚è≠Ô∏è –ù–µ –∏–≤–µ–Ω—Ç: {text[:50].strip()}")
                    continue

                # ‚îÄ‚îÄ –ü–æ–ø—ã—Ç–∫–∞ —Ä–∞–∑–æ–±—Ä–∞—Ç—å –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω—É—é –¥–∞—Ç—É ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                first_line = ''
                for ln in text.strip().split('\n'):
                    clean = strip_emoji(ln).strip()
                    if len(clean) > 10:
                        first_line = clean
                        break

                glued_data = None
                has_glue = bool(re.search(
                    r'\d{1,2}\s+[–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,}[,\s]+\d{1,2}:\d{2}[–ê-–Ø–ÅA-Za-z]',
                    first_line
                ))

                if has_glue:
                    glued_data = parse_glued_line(first_line)
                    if not glued_data:
                        # –ù–µ —Å–º–æ–≥–ª–∏ —Ä–∞–∑–æ–±—Ä–∞—Ç—å ‚Äî –∑–∞–ø–æ–º–∏–Ω–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –∏ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
                        remember_bad_pattern(text)
                        logger.info(f"‚è≠Ô∏è –ù–µ —Å–º–æ–≥–ª–∏ —Ä–∞–∑–æ–±—Ä–∞—Ç—å –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω—É—é –¥–∞—Ç—É, –ø—Ä–æ–ø—É—Å–∫: {first_line[:60]}")
                        continue

                # ‚îÄ‚îÄ –ï—Å–ª–∏ –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–∞—è –¥–∞—Ç–∞ —Ä–∞–∑–æ–±—Ä–∞–Ω–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                if glued_data:
                    title = glued_data['title']
                    dt = glued_data['dt']
                    time_str = glued_data['time_str']
                    location = glued_data['city'] or extract_location(text) or ''

                    # –ï—Å–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –≤—Å—ë –µ—â—ë –≤—ã–≥–ª—è–¥–∏—Ç –º—É—Å–æ—Ä–Ω–æ ‚Äî —á–∏—Å—Ç–∏–º —á–µ—Ä–µ–∑ Claude
                    looks_dirty = (
                        re.search(r'[–ê-–Ø–Å]{2,}[a-zA-Z]', title) or  # –ö–∏—Ä–∏–ª–ª–∏—Ü–∞Latin–°–ª–∏–ø—à–µ–µ—Å—è
                        len(title) > 80 or
                        title.count(' ') < 1
                    )
                    if looks_dirty and CLAUDE_API_KEY:
                        clean = await claude_clean_title(text, session)
                        if clean is None:
                            logger.info(f"‚è≠Ô∏è Claude —Å–∫–∞–∑–∞–ª SKIP: {title[:50]}")
                            continue
                        title = clean

                    if not is_future(dt):
                        continue

                    all_events.append({
                        'title': title,
                        'date': format_date(dt, time_str),
                        'location': location,
                        'venue': extract_venue(text),
                        'link': post_link,
                        'source': channel['name'],
                        'image_url': image_url,
                    })
                    continue

                # ‚îÄ‚îÄ –û–±—ã—á–Ω—ã–π –ø–æ—Å—Ç –±–µ–∑ –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω–æ–π –¥–∞—Ç—ã ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
                dt = parse_date(text)
                if not is_future(dt):
                    logger.info(f"‚è≠Ô∏è {'–ü—Ä–æ—à–µ–¥—à–µ–µ' if dt else '–ù–µ—Ç –¥–∞—Ç—ã'}: {text[:50].strip()}")
                    continue

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ —á–µ—Ä–µ–∑ Claude –µ—Å–ª–∏ –µ—Å—Ç—å –∫–ª—é—á, –∏–Ω–∞—á–µ –ø—Ä–æ—Å—Ç—ã–º –ø–∞—Ä—Å–µ—Ä–æ–º
                title = None
                if CLAUDE_API_KEY:
                    title = await claude_clean_title(text, session)
                    if title is None:
                        logger.info(f"‚è≠Ô∏è Claude —Å–∫–∞–∑–∞–ª SKIP: {text[:50].strip()}")
                        continue
                else:
                    # –ü—Ä–æ—Å—Ç–æ–π fallback: –ø–µ—Ä–≤–∞—è –¥–ª–∏–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –±–µ–∑ –¥–∞—Ç—ã
                    for ln in text.split('\n'):
                        ln = strip_emoji(ln).strip()
                        if len(ln) > 10 and not re.match(r'^\d{1,2}\s+[–∞-—è—ë]', ln.lower()):
                            title = ln[:120]
                            break

                if not title:
                    logger.info(f"‚è≠Ô∏è –ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞: {text[:50].strip()}")
                    continue

                time_m = re.search(r'\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})', text)
                time_str = time_m.group(1) if time_m else None

                all_events.append({
                    'title': title,
                    'date': format_date(dt, time_str),
                    'location': extract_location(text) or '',
                    'venue': extract_venue(text),
                    'link': post_link,
                    'source': channel['name'],
                    'image_url': image_url,
                })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
                continue

        return all_events

    # ‚îÄ‚îÄ –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site['url'])
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        events = []

        for link in soup.find_all('a', href=True)[:80]:
            try:
                href = link.get('href', '')
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                if href.rstrip('/') == site['url'].rstrip('/'):
                    continue
                if href in self.posted:
                    continue
                if is_site_trash(title_raw):
                    continue
                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(['div', 'article', 'li', 'section'])
                context = parent.get_text(separator=' ', strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt):
                    continue

                location = extract_location(context) or ''
                venue = extract_venue(context)

                image_url = None
                img = (link.find('img', src=True) or
                       (parent.find('img', src=True) if parent else None))
                if img:
                    src = img.get('src', '')
                    if src and not src.startswith('http'):
                        from urllib.parse import urljoin
                        src = urljoin(site['url'], src)
                    image_url = src or None

                self.posted.add(href)
                events.append({
                    'title': strip_emoji(title_raw)[:120],
                    'date': format_date(dt),
                    'location': location,
                    'venue': venue,
                    'link': href,
                    'source': site['name'],
                    'image_url': image_url,
                })

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events

    # ‚îÄ‚îÄ –°–±–æ—Ä –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            events = await self.parse_site(site)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ {site['name']}: {len(events)}")

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤...")
        for channel in TELEGRAM_CHANNELS:
            events = await self.parse_channel(channel)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ {channel['name']}: {len(events)}")

        return all_events


# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É
        unique, seen = [], set()
        for e in events:
            key = e['title'][:40].lower()
            if key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")

        posted = 0
        for event in unique[:15]:
            text = make_post(event)
            if not text:
                continue
            try:
                if event.get('image_url'):
                    try:
                        await bot.send_photo(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            photo=event['image_url'],
                            caption=text,
                            parse_mode='HTML',
                        )
                    except Exception:
                        # –ö–∞—Ä—Ç–∏–Ω–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∑–∏–ª–∞—Å—å ‚Äî —à–ª—ë–º –±–µ–∑ –Ω–µ—ë
                        await bot.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode='HTML',
                            disable_web_page_preview=True,
                        )
                else:
                    await bot.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode='HTML',
                        disable_web_page_preview=True,
                    )
                posted += 1
                logger.info(f"‚úÖ ({posted}) {event['title'][:50]}")
                await asyncio.sleep(2)
            except Exception as e:
                logger.error(f"‚ùå {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted}")
        
    finally:
        save_posted(bot_obj.posted)
        await bot_obj.close()


if __name__ == '__main__':
    asyncio.run(main())
    
