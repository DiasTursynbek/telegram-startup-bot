import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json

logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID', "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv('MESSAGE_THREAD_ID', '4'))
CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY', '')

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
    '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
    '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12,
}
MONTHS_SHORT = {
    '—è–Ω–≤': 1, '—Ñ–µ–≤': 2, '–º–∞—Ä': 3, '–∞–ø—Ä': 4,
    '–º–∞–π': 5, '–∏—é–Ω': 6, '–∏—é–ª': 7, '–∞–≤–≥': 8,
    '—Å–µ–Ω': 9, '–æ–∫—Ç': 10, '–Ω–æ—è': 11, '–¥–µ–∫': 12,
}

# –ü–æ—Å—Ç –±–µ—Ä—ë–º –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∏–∑ —ç—Ç–∏—Ö —Å–ª–æ–≤
EVENT_WORDS = [
    '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'conference', '—Ñ–æ—Ä—É–º', 'forum', 'summit', '—Å–∞–º–º–∏—Ç',
    'meetup', '–º–∏—Ç–∞–ø', '—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon',
    '–≤–æ—Ä–∫—à–æ–ø', 'workshop', '–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å', 'masterclass',
    '–≤–µ–±–∏–Ω–∞—Ä', 'webinar', '—Å–µ–º–∏–Ω–∞—Ä',
    'pitch', '–ø–∏—Ç—á', 'demo day',
    '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', 'accelerator', 'bootcamp', '–±—É—Ç–∫–µ–º–ø',
    '–≤—ã—Å—Ç–∞–≤–∫–∞', '–∫–æ–Ω–∫—É—Ä—Å', 'competition',
    '—Ç—Ä–µ–Ω–∏–Ω–≥', 'training',
    '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '–∏–≤–µ–Ω—Ç', 'event',
    '–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç', '–ø—Ä–∏–≥–ª–∞—à–∞–µ–º', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è',
]

# –ü–æ—Å—Ç –í–´–ë–†–ê–°–´–í–ê–ï–ú –µ—Å–ª–∏ –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –æ–¥–Ω–æ –∏–∑ —ç—Ç–∏—Ö —Å–ª–æ–≤
NOT_EVENT_WORDS = [
    'research', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ', '–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª', '–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥',
    '–º–ª–Ω $', '–º–ª—Ä–¥ $', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '—É–≤–æ–ª–µ–Ω', '–æ—Ç—á–µ—Ç', '–≤—ã—Ä—É—á–∫–∞',
    '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '–±–∏—Ä–∂–∞', '–∞–∫—Ü–∏–∏', '—Ç–æ–∫–∞–µ–≤', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ',
]

# –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π –º—É—Å–æ—Ä —Å —Å–∞–π—Ç–æ–≤
SITE_STOP_WORDS = [
    '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–æ –Ω–∞—Å', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–≤–æ–π—Ç–∏', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞',
    '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–ø–æ–∏—Å–∫', '–≥–ª–∞–≤–Ω–∞—è', '–º–µ–Ω—é', '–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏',
    '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ', 'privacy', 'terms', 'cookie',
]

KZ_CITIES = {
    '–∞–ª–º–∞—Ç—ã': '–ê–ª–º–∞—Ç—ã', '–∞—Å—Ç–∞–Ω–∞': '–ê—Å—Ç–∞–Ω–∞', '—à—ã–º–∫–µ–Ω—Ç': '–®—ã–º–∫–µ–Ω—Ç',
    '–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω': '–ê—Å—Ç–∞–Ω–∞', '—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫': '–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫',
    '–∫—ã–∑—ã–ª–æ—Ä–¥–∞': '–ö—ã–∑—ã–ª–æ—Ä–¥–∞', '–∞–∫—Ç–æ–±–µ': '–ê–∫—Ç–æ–±–µ', '—Ç–∞—Ä–∞–∑': '–¢–∞—Ä–∞–∑',
    '–ø–∞–≤–ª–æ–¥–∞—Ä': '–ü–∞–≤–ª–æ–¥–∞—Ä', '—Å–µ–º–µ–π': '–°–µ–º–µ–π', '–∞—Ç—ã—Ä–∞—É': '–ê—Ç—ã—Ä–∞—É',
    '–æ–Ω–ª–∞–π–Ω': '–û–Ω–ª–∞–π–Ω', 'online': '–û–Ω–ª–∞–π–Ω', 'zoom': '–û–Ω–ª–∞–π–Ω (Zoom)',
    '—Ç–∞—à–∫–µ–Ω—Ç': '–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω',
}

# –≠–º–æ–¥–∑–∏-—Ä–µ–≥—É–ª—è—Ä–∫–∞ ‚Äî –ù–ï —Ç—Ä–æ–≥–∞–µ—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É
EMOJI_RE = re.compile(
    '[\U00010000-\U0010ffff'
    '\u2600-\u27ff'
    '\u2300-\u23ff'
    '\u25a0-\u25ff'
    '\u2B00-\u2BFF'
    ']',
    re.UNICODE
)


def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub('', s).strip()


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –î–ê–¢–ê
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_date(text: str) -> Optional[datetime]:
    t = text.lower()
    try:
        m = re.search(r'(\d{1,2})[-](\d{1,2})\s+([–∞-—è]+)(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(2))
            month = MONTHS_RU.get(m.group(3), 0)
            year = int(m.group(4)) if m.group(4) else datetime.now().year
            if month:
                return datetime(year, month, day)

        m = re.search(r'(\d{1,2})\s+([–∞-—è]+)(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = MONTHS_RU.get(m.group(2), 0)
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if month:
                dt = datetime(year, month, day)
                if not m.group(3) and dt.date() < datetime.now().date():
                    dt = datetime(year + 1, month, day)
                return dt

        m = re.search(r'(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = MONTHS_SHORT.get(m.group(2)[:3], 0)
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if month:
                dt = datetime(year, month, day)
                if not m.group(3) and dt.date() < datetime.now().date():
                    dt = datetime(year + 1, month, day)
                return dt

        m = re.search(r'(\d{1,2})\.(\d{2})(?:\.(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = int(m.group(2))
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if 1 <= month <= 12:
                return datetime(year, month, day)
    except Exception:
        pass
    return None


def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()


def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: '—è–Ω–≤–∞—Ä—è', 2: '—Ñ–µ–≤—Ä–∞–ª—è', 3: '–º–∞—Ä—Ç–∞', 4: '–∞–ø—Ä–µ–ª—è',
        5: '–º–∞—è', 6: '–∏—é–Ω—è', 7: '–∏—é–ª—è', 8: '–∞–≤–≥—É—Å—Ç–∞',
        9: '—Å–µ–Ω—Ç—è–±—Ä—è', 10: '–æ–∫—Ç—è–±—Ä—è', 11: '–Ω–æ—è–±—Ä—è', 12: '–¥–µ–∫–∞–±—Ä—è',
    }
    result = f"{dt.day} {months[dt.month]} {dt.year}"
    if time_str:
        result += f", {time_str}"
    return result


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ú–ï–°–¢–û
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None


def extract_venue(text: str) -> Optional[str]:
    venues = [
        'Narxoz', 'Nazarbayev', 'KBTU', 'K–ë–¢–£', 'Astana Hub',
        'IT Park', 'MOST IT Hub', 'Holiday Inn', 'Esentai',
        'Yandex', 'Smart Point', 'Almaty Arena',
    ]
    for v in venues:
        if v.lower() in text.lower():
            m = re.search(rf'{v}[^\n,.]*', text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', text)
    if at_m:
        return at_m.group(1).strip()[:60]
    return None


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# –ó–ê–ì–û–õ–û–í–û–ö
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ


def get_clean_title(text: str) -> Optional[str]:
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–≥–æ–ª–æ–≤–∫–∞.
    –ù–∞—Ö–æ–¥–∏—Ç '–î–î –ú–µ—Å, –ß–ß:–ú–ú–ì–æ—Ä–æ–¥' –∏ –±–µ—Ä—ë—Ç –í–°–Å –ü–û–°–õ–ï ‚Äî —ç—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è.
    –£–±–∏—Ä–∞–µ—Ç –¥—É–±–ª–∏ –∏ —Ö–≤–æ—Å—Ç–æ–≤–æ–π –º—É—Å–æ—Ä.
    """
    for line in text.strip().split('\n'):
        line = line.strip()
        if len(line) < 10:
            continue
        if 'http' in line or 't.me/' in line:
            continue

        # –£–±–∏—Ä–∞–µ–º —ç–º–æ–¥–∑–∏
        line = EMOJI_RE.sub('', line).strip()

        # –ò—â–µ–º '–î–î –ú–µ—Å, –ß–ß:–ú–ú–ì–æ—Ä–æ–¥' –∏ –±–µ—Ä—ë–º –≤—Å—ë –ø–æ—Å–ª–µ
        m = re.search(
            r'\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+\d{1,2}:\d{2}\s*[–ê-–Ø–Å][–∞-—è—ë]*\s*',
            line
        )
        title = line[m.end():].strip() if m else line.strip()

        if len(title) < 5:
            continue

        # –£–±–∏—Ä–∞–µ–º –ø–æ–ª–Ω—ã–π –¥—É–±–ª—å: 'TitleTitle' -> 'Title'
        for split in range(10, len(title) // 2 + 1):
            if title[split:].startswith(title[:split]):
                title = title[:split]
                break

        # –£–±–∏—Ä–∞–µ–º —Ö–≤–æ—Å—Ç –µ—Å–ª–∏ –ø–æ—Å–ª–µ [.!?] –∏–¥—ë—Ç –ø–æ–≤—Ç–æ—Ä –Ω–∞—á–∞–ª–∞
        for m2 in re.finditer(r'[.!?]\s*', title):
            tail = title[m2.end():]
            if len(tail) > 5 and title.startswith(tail[:min(15, len(tail))]):
                title = title[:m2.end()]
                break

        title = title.strip(' .,\u2013')

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∞—Å—å –¥–∞—Ç–∞ –∏–ª–∏ –∏–º—è –∞–≤—Ç–æ—Ä–∞
        if re.match(r'^\d{1,2}[.\-:\s]', title):
            continue
        if re.match(r'^[–ê-–Ø–Å][–∞-—è—ë]+\s+[–ê-–Ø–Å][–∞-—è—ë]+$', title):
            continue
        if len(title) < 5:
            continue

        return title[:120]
    return None


def extract_title(text: str) -> Optional[str]:
    """–û–±—ë—Ä—Ç–∫–∞ –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏."""
    return get_clean_title(text)


def make_post(event: Dict) -> str:
    """
    –§–æ—Ä–º–∞—Ç (4-5 —Å—Ç—Ä–æ–∫):
    üéØ –ù–∞–∑–≤–∞–Ω–∏–µ
    üá∞üáø –°—Ç—Ä–∞–Ω–∞, üèô –ì–æ—Ä–æ–¥
    üìç –ú–µ—Å—Ç–æ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    üìÖ –î–∞—Ç–∞ –∏ –≤—Ä–µ–º—è
    üîó –°—Å—ã–ª–∫–∞ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª
    """
    title = (event.get('title') or '').strip()
    if not title or len(title) < 5:
        return ""
    if not event.get('date'):
        return ""

    location = event.get('location', '')
    venue = event.get('venue', '')

    lines = [f"\U0001f3af <b>{title}</b>"]

    if location in ('–û–Ω–ª–∞–π–Ω', '–û–Ω–ª–∞–π–Ω (Zoom)'):
        lines.append("\U0001f310 –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"\U0001f1f0\U0001f1ff –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, \U0001f3d9 {location}")
    else:
        lines.append("\U0001f1f0\U0001f1ff –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue:
        lines.append(f"\U0001f4cd {venue}")

    lines.append(f"\U0001f4c5 {event['date']}")
    lines.append(f"\U0001f517 <a href=\'{event['link']}\'>–ß–∏—Ç–∞—Ç—å \u2192</a>")

    return "\n".join(lines)


class EventBot:
    def __init__(self):
        self.session = None
        self.posted = set()

    async def get_session(self):
        if not self.session:
            self.session = aiohttp.ClientSession(headers={'User-Agent': 'Mozilla/5.0'})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            session = await self.get_session()
            async with session.get(url, timeout=15) as resp:
                return await resp.text() if resp.status == 200 else ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {url} - {e}")
            return ""

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            date_match = re.match(
                r'^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?'
                r'|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*'
                r'(?:\s+\d{4})?)',
                line, re.IGNORECASE
            )
            if not date_match:
                i += 1
                continue

            date_raw = date_match.group(0)
            rest = line[date_match.end():].strip()

            time_match = re.search(r'(?:–≤\s*)?(\d{1,2}:\d{2})', rest)
            time_str = time_match.group(1) if time_match else None
            if time_match:
                rest = (rest[:time_match.start()] + rest[time_match.end():]).strip()

            title_raw = strip_emoji(rest).strip(' -\u2013\u2022')

            link = None
            lm = re.search(r'((?:https?://|t\.me/)\S+)', line)
            if lm:
                link = lm.group(1)
                if not link.startswith('http'):
                    link = 'https://' + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), '').strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm = re.search(r'((?:https?://|t\.me/)\S+)', lines[j])
                    if lm:
                        link = lm.group(1)
                        if not link.startswith('http'):
                            link = 'https://' + link
                        break

            venue = None
            at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', title_raw)
            if at_m:
                venue = at_m.group(1).strip()
                title_raw = title_raw[:at_m.start()].strip()

            if len(title_raw) < 5 and i + 1 < len(lines):
                next_line = strip_emoji(lines[i + 1]).strip()
                if len(next_line) > 5 and not re.match(r'^\d', next_line):
                    title_raw = next_line

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                logger.info(f"\u23ed\ufe0f –ü—Ä–æ—à–µ–¥—à–µ–µ: {title_raw[:40]} ({date_raw})")
                i += 1
                continue

            context = line + ' ' + (lines[i + 1] if i + 1 < len(lines) else '')
            location = extract_location(context) or extract_location(text)
            if not venue:
                venue = extract_venue(context)

            events.append({
                'title': title_raw[:120],
                'date': format_date(dt, time_str),
                'location': location or '',
                'venue': venue,
                'link': link or post_link,
                'source': source,
                'image_url': image_url,
            })
            i += 1
        return events

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        url = f"https://t.me/s/{channel['username']}"
        html = await self.fetch(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        all_events = []

        for msg in soup.find_all('div', class_='tgme_widget_message')[:20]:
            try:
                text_div = msg.find('div', class_='tgme_widget_message_text')
                if not text_div:
                    continue

                text = text_div.get_text(separator='\n', strip=True)

                # –í—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–Ω–æ—Å –º–µ–∂–¥—É –ß–ß:–ú–ú–ì–æ—Ä–æ–¥ –∏ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º
                # '17:00–ê–ª–º–∞—Ç—ãTitle' -> '17:00 –ê–ª–º–∞—Ç—ã\nTitle'
                text = re.sub(
                    r'(\d{1,2}:\d{2})([–ê-–Ø–Å][–∞-—è—ë]+)([–ê-–Ø–ÅA-Za-z])',
                    r'\1 \2\n\3',
                    text
                )
                if len(text) < 30:
                    continue

                link_elem = msg.find('a', class_='tgme_widget_message_date')
                post_link = link_elem['href'] if link_elem else f"https://t.me/{channel['username']}"

                if post_link in self.posted:
                    continue
                self.posted.add(post_link)

                image_url = None
                img_div = msg.find('a', class_='tgme_widget_message_photo_wrap')
                if img_div:
                    style = img_div.get('style', '')
                    m = re.search(r"url\('([^']+)'\)", style)
                    if m:
                        image_url = m.group(1)

                is_digest = bool(re.search(
                    r'\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}', text
                ))

                if is_digest:
                    events = self.parse_digest(text, post_link, channel['name'], image_url)
                    all_events.extend(events)
                    logger.info(f"\U0001f4cb –î–∞–π–¥–∂–µ—Å—Ç {channel['name']}: {len(events)} —Å–æ–±—ã—Ç–∏–π")
                    continue

                # –§–∏–ª—å—Ç—Ä 1: –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
                if not is_real_event(text):
                    logger.info(f"\u23ed\ufe0f –ù–µ –∏–≤–µ–Ω—Ç: {text[:50].strip()}")
                    continue

                # –§–∏–ª—å—Ç—Ä 2: –¥–∞—Ç–∞ –≤ –±—É–¥—É—â–µ–º
                dt = parse_date(text)
                if not is_future(dt):
                    logger.info(f"\u23ed\ufe0f {'–ü—Ä–æ—à–µ–¥—à–µ–µ' if dt else '–ù–µ—Ç –¥–∞—Ç—ã'}: {text[:50].strip()}")
                    continue

                # –§–∏–ª—å—Ç—Ä 3: –∑–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç–¥–µ–ª—å–Ω–æ –æ—Ç –¥–∞—Ç—ã
                title = extract_title(text)
                if not title:
                    logger.info(f"\u23ed\ufe0f –ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞: {text[:50].strip()}")
                    continue

                time_m = re.search(r'(?:–≤\s+)(\d{1,2}:\d{2})', text)
                time_str = time_m.group(1) if time_m else None

                all_events.append({
                    'title': title,
                    'date': format_date(dt, time_str),
                    'location': extract_location(text) or '',
                    'venue': extract_venue(text),
                    'link': post_link,
                    'source': channel['name'],
                    'image_url': image_url,
                })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞: {e}")
                continue

        return all_events

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site['url'])
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        events = []

        for link in soup.find_all('a', href=True)[:80]:
            try:
                href = link.get('href', '')
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                if href.rstrip('/') == site['url'].rstrip('/'):
                    continue
                if href in self.posted:
                    continue

                # –ù–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–π –º—É—Å–æ—Ä
                if is_site_trash(title_raw):
                    continue

                # –ò–≤–µ–Ω—Ç-—Å–ª–æ–≤–∞
                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(['div', 'article', 'li', 'section'])
                context = parent.get_text(separator=' ', strip=True) if parent else title_raw
                dt = parse_date(context)

                # –¢–æ–ª—å–∫–æ –±—É–¥—É—â–∏–µ
                if not is_future(dt):
                    continue

                location = extract_location(context) or ''
                venue = extract_venue(context)

                image_url = None
                img = (link.find('img', src=True) or
                       (parent.find('img', src=True) if parent else None))
                if img:
                    src = img.get('src', '')
                    if src and not src.startswith('http'):
                        from urllib.parse import urljoin
                        src = urljoin(site['url'], src)
                    image_url = src or None

                self.posted.add(href)
                events.append({
                    'title': strip_emoji(title_raw)[:120],
                    'date': format_date(dt),
                    'location': location,
                    'venue': venue,
                    'link': href,
                    'source': site['name'],
                    'image_url': image_url,
                })

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events

    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"\U0001f310 –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            events = await self.parse_site(site)
            all_events.extend(events)
            if events:
                logger.info(f"\u2705 {site['name']}: {len(events)}")

        logger.info(f"\U0001f4f1 –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤...")
        for channel in TELEGRAM_CHANNELS:
            events = await self.parse_channel(channel)
            all_events.extend(events)
            if events:
                logger.info(f"\u2705 {channel['name']}: {len(events)}")

        return all_events


# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# MAIN
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def main():
    logger.info("\U0001f680 –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("\u274c BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = e['title'][:40].lower()
            if key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"\U0001f4ca –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")

        posted = 0
        for event in unique[:15]:
            text = make_post(event)
            if not text:
                continue
            try:
                if event.get('image_url'):
                    try:
                        await bot.send_photo(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            photo=event['image_url'],
                            caption=text,
                            parse_mode='HTML'
                        )
                    except Exception:
                        await bot.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode='HTML',
                            disable_web_page_preview=True
                        )
                else:
                    await bot.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode='HTML',
                        disable_web_page_preview=True
                    )
                posted += 1
                logger.info(f"\u2705 ({posted}) {event['title'][:50]}")
                await asyncio.sleep(2)
            except Exception as e:
                logger.error(f"\u274c {e}")

        logger.info(f"\u2705 –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted}")

    finally:
        await bot_obj.close()


if __name__ == '__main__':
    asyncio.run(main())
