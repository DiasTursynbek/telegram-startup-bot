import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path


# ‚îÄ‚îÄ‚îÄ State (posted links) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

STATE_FILE = Path("state/posted.json")
STATE_FILE.parent.mkdir(parents=True, exist_ok=True)

def load_posted() -> set:
    if STATE_FILE.exists():
        try:
            return set(json.loads(STATE_FILE.read_text(encoding="utf-8")))
        except Exception:
            return set()
    return set()

def save_posted(posted: set) -> None:
    STATE_FILE.write_text(
        json.dumps(sorted(posted), ensure_ascii=False, indent=2),
        encoding="utf-8"
    )


# ‚îÄ‚îÄ‚îÄ Logging / env ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

logging.basicConfig(format='%(asctime)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID', "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv('MESSAGE_THREAD_ID', '4'))
CLAUDE_API_KEY = os.getenv('CLAUDE_API_KEY', '')

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {
    '—è–Ω–≤–∞—Ä—è': 1, '—Ñ–µ–≤—Ä–∞–ª—è': 2, '–º–∞—Ä—Ç–∞': 3, '–∞–ø—Ä–µ–ª—è': 4,
    '–º–∞—è': 5, '–∏—é–Ω—è': 6, '–∏—é–ª—è': 7, '–∞–≤–≥—É—Å—Ç–∞': 8,
    '—Å–µ–Ω—Ç—è–±—Ä—è': 9, '–æ–∫—Ç—è–±—Ä—è': 10, '–Ω–æ—è–±—Ä—è': 11, '–¥–µ–∫–∞–±—Ä—è': 12,
}
MONTHS_SHORT = {
    '—è–Ω–≤': 1, '—Ñ–µ–≤': 2, '–º–∞—Ä': 3, '–∞–ø—Ä': 4,
    '–º–∞–π': 5, '–∏—é–Ω': 6, '–∏—é–ª': 7, '–∞–≤–≥': 8,
    '—Å–µ–Ω': 9, '–æ–∫—Ç': 10, '–Ω–æ—è': 11, '–¥–µ–∫': 12,
}

EVENT_WORDS = [
    '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'conference', '—Ñ–æ—Ä—É–º', 'forum', 'summit', '—Å–∞–º–º–∏—Ç',
    'meetup', '–º–∏—Ç–∞–ø', '—Ö–∞–∫–∞—Ç–æ–Ω', 'hackathon', '–≤–æ—Ä–∫—à–æ–ø', 'workshop',
    '–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å', 'masterclass', '–≤–µ–±–∏–Ω–∞—Ä', 'webinar', '—Å–µ–º–∏–Ω–∞—Ä',
    'pitch', '–ø–∏—Ç—á', 'demo day', '–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä', 'accelerator',
    'bootcamp', '–±—É—Ç–∫–µ–º–ø', '–≤—ã—Å—Ç–∞–≤–∫–∞', '–∫–æ–Ω–∫—É—Ä—Å', 'competition',
    '—Ç—Ä–µ–Ω–∏–Ω–≥', 'training', '–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ', '–∏–≤–µ–Ω—Ç', 'event',
    '–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç', '–ø—Ä–∏–≥–ª–∞—à–∞–µ–º', '–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è',
]

NOT_EVENT_WORDS = [
    'research', '–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ', '–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª', '–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥',
    '–º–ª–Ω $', '–º–ª—Ä–¥ $', '–Ω–∞–∑–Ω–∞—á–µ–Ω', '—É–≤–æ–ª–µ–Ω', '–æ—Ç—á–µ—Ç', '–≤—ã—Ä—É—á–∫–∞',
    '–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞', '–±–∏—Ä–∂–∞', '–∞–∫—Ü–∏–∏', '—Ç–æ–∫–∞–µ–≤', '–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –ø—Ä–∏–Ω—è–ª–æ',
]

SITE_STOP_WORDS = [
    '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–æ –Ω–∞—Å', '–ø–æ–ª–∏—Ç–∏–∫–∞', '–≤–æ–π—Ç–∏', '—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞',
    '–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è', '–ø–æ–∏—Å–∫', '–≥–ª–∞–≤–Ω–∞—è', '–º–µ–Ω—é', '–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏',
    '—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ', '–ø–æ–¥—Ä–æ–±–Ω–µ–µ', '—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ', 'privacy', 'terms', 'cookie',
]

KZ_CITIES = {
    '–∞–ª–º–∞—Ç—ã': '–ê–ª–º–∞—Ç—ã', '–∞—Å—Ç–∞–Ω–∞': '–ê—Å—Ç–∞–Ω–∞', '—à—ã–º–∫–µ–Ω—Ç': '–®—ã–º–∫–µ–Ω—Ç',
    '–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω': '–ê—Å—Ç–∞–Ω–∞', '—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫': '–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫',
    '–∫—ã–∑—ã–ª–æ—Ä–¥–∞': '–ö—ã–∑—ã–ª–æ—Ä–¥–∞', '–∞–∫—Ç–æ–±–µ': '–ê–∫—Ç–æ–±–µ', '—Ç–∞—Ä–∞–∑': '–¢–∞—Ä–∞–∑',
    '–ø–∞–≤–ª–æ–¥–∞—Ä': '–ü–∞–≤–ª–æ–¥–∞—Ä', '—Å–µ–º–µ–π': '–°–µ–º–µ–π', '–∞—Ç—ã—Ä–∞—É': '–ê—Ç—ã—Ä–∞—É',
    '–∂–µ–∑–∫–∞–∑–≥–∞–Ω': '–ñ–µ–∑“õ–∞–∑“ì–∞–Ω', '–∂–µ–∑“õ–∞–∑“ì–∞–Ω': '–ñ–µ–∑“õ–∞–∑“ì–∞–Ω',
    '–æ–Ω–ª–∞–π–Ω': '–û–Ω–ª–∞–π–Ω', 'online': '–û–Ω–ª–∞–π–Ω', 'zoom': '–û–Ω–ª–∞–π–Ω (Zoom)',
    '—Ç–∞—à–∫–µ–Ω—Ç': '–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω',
}

EMOJI_RE = re.compile(
    '[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]',
    re.UNICODE
)

# ‚îÄ‚îÄ‚îÄ Claude error memory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
_bad_patterns: List[str] = []

def remember_bad_pattern(text: str):
    key = re.sub(r'\s+', ' ', text[:30].lower()).strip()
    if key and key not in _bad_patterns:
        _bad_patterns.append(key)
        if len(_bad_patterns) > 100:
            _bad_patterns.pop(0)

def matches_bad_pattern(text: str) -> bool:
    key = re.sub(r'\s+', ' ', text[:30].lower()).strip()
    return any(key.startswith(p[:20]) for p in _bad_patterns)


# ‚îÄ‚îÄ‚îÄ Helpers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub('', s).strip()

def normalize_glued_text(s: str) -> str:
    """
    –†–∞–∑–ª–µ–ø–ª—è–µ—Ç —Å–∫–ª–µ–π–∫–∏:
    16:00–û–Ω–ª–∞–π–Ω -> 16:00 –û–Ω–ª–∞–π–Ω
    –ê—Ç—ã—Ä–∞—É–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ -> –ê—Ç—ã—Ä–∞—É –ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (—á–∞—Å—Ç–∏—á–Ω–æ, –¥–∞–ª—å—à–µ —á–∏—Å—Ç–∏–º)
    """
    s = strip_emoji(s).strip()
    s = re.sub(r'(\d{1,2}:\d{2})(?=[A-Za-z–ê-–Ø–∞-—è–Å—ë])', r'\1 ', s)
    s = re.sub(r'([–∞-—è—ë–ê-–Ø–Å]{3,}),(\d{1,2}:\d{2})', r'\1, \2', s)
    s = re.sub(r'\s{2,}', ' ', s)
    return s

def strip_leading_datetime_from_title(title: str) -> str:
    """
    –°—Ä–µ–∑–∞–µ—Ç –≤–µ–¥—É—â—É—é –¥–∞—Ç—É/–≤—Ä–µ–º—è –∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞ (–µ—Å–ª–∏ –≤–¥—Ä—É–≥ –ø–æ–ø–∞–ª–æ):
    "24 –§–µ–≤, 16:00 –û–Ω–ª–∞–π–Ω ..." -> "–û–Ω–ª–∞–π–Ω ..."
    "24 —Ñ–µ–≤—Ä–∞–ª—è 2026 ..." -> "..."
    "24.02.2026 ..." -> "..."
    """
    t = normalize_glued_text(title)

    # 24 —Ñ–µ–≤, 16:00 ...
    t = re.sub(r'^\s*\d{1,2}\s+[–ê-–Ø–∞-—è–Å—ëA-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*', '', t, flags=re.IGNORECASE)

    # 24 —Ñ–µ–≤—Ä–∞–ª—è 2026 ...
    t = re.sub(r'^\s*\d{1,2}\s+[–∞-—è—ë]{3,}(?:\s+\d{4})?\s*', '', t, flags=re.IGNORECASE)

    # 24.02.2026 ...
    t = re.sub(r'^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*', '', t)

    return t.strip(" -‚Äì‚Ä¢,.:").strip()

def _city_variants(city: str) -> List[str]:
    """
    –í–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è –≥–æ—Ä–æ–¥–∞ –¥–ª—è –≥—Ä—É–±–æ–≥–æ –º–∞—Ç—á–∏–Ω–≥–∞.
    –ñ–µ–∑“õ–∞–∑“ì–∞–Ω <-> –ñ–µ–∑–∫–∞–∑–≥–∞–Ω
    """
    if not city:
        return []
    c = city.strip()
    vars_ = {c}

    # –ø—Ä–æ—Å—Ç—ã–µ –∑–∞–º–µ–Ω—ã
    vars_.add(c.replace('“õ', '–∫'))
    vars_.add(c.replace('–∫', '“õ'))
    vars_.add(c.replace('“ö', '–ö'))
    vars_.add(c.replace('–ö', '“ö'))
    return sorted(vars_, key=len, reverse=True)

def strip_leading_city_from_title(title: str, location: str = "") -> str:
    """
    –°—Ä–µ–∑–∞–µ—Ç –≥–æ—Ä–æ–¥/–æ–Ω–ª–∞–π–Ω –≤ –Ω–∞—á–∞–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –≤–∫–ª—é—á–∞—è —Å–∫–ª–µ–π–∫—É:
    "–ê—Ç—ã—Ä–∞—É–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ..." -> "–ü—Ä–æ–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ..."
    "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω–¶–∏—Ñ—Ä–æ–≤–∞—è..." -> "–¶–∏—Ñ—Ä–æ–≤–∞—è..."
    """
    t = normalize_glued_text(title).strip()

    # –µ—Å–ª–∏ location –∏–∑–≤–µ—Å—Ç–µ–Ω ‚Äî —Ä–µ–∂–µ–º –µ–≥–æ –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ (–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã)
    for v in _city_variants(location):
        if t.lower().startswith(v.lower()):
            rest = t[len(v):].lstrip(" -‚Äì‚Ä¢,.:")
            if len(rest) >= 5:
                return rest

    # —Ä–µ–∂–µ–º –ª—é–±—ã–µ –≥–æ—Ä–æ–¥–∞ –∏–∑ —Å–ª–æ–≤–∞—Ä—è (–∑–Ω–∞—á–µ–Ω–∏—è)
    city_values = sorted(set(KZ_CITIES.values()), key=len, reverse=True)
    for city in city_values:
        if t.lower().startswith(city.lower()):
            rest = t[len(city):].lstrip(" -‚Äì‚Ä¢,.:")
            if len(rest) >= 5:
                return rest

    # —Ä–µ–∂–µ–º –∫–ª—é—á–∏ —Ç–æ–∂–µ (–Ω–∞ —Å–ª—É—á–∞–π –ª–∞—Ç–∏–Ω–∏—Ü—ã/–æ–Ω–ª–∞–π–Ω)
    city_keys = sorted(set(KZ_CITIES.keys()), key=len, reverse=True)
    for ck in city_keys:
        if t.lower().startswith(ck.lower()):
            rest = t[len(ck):].lstrip(" -‚Äì‚Ä¢,.:")
            if len(rest) >= 5:
                return rest

    return t

def cleanup_title(title: str, location: str = "") -> str:
    """
    –§–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ title:
    - —É–±–∏—Ä–∞–µ–º –¥–∞—Ç—É/–≤—Ä–µ–º—è –≤ –Ω–∞—á–∞–ª–µ
    - —É–±–∏—Ä–∞–µ–º –≥–æ—Ä–æ–¥/–æ–Ω–ª–∞–π–Ω –≤ –Ω–∞—á–∞–ª–µ (–≤–∫–ª—é—á–∞—è —Å–∫–ª–µ–π–∫—É)
    - —É–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏
    """
    t = strip_leading_datetime_from_title(title)
    t = strip_leading_city_from_title(t, location=location)

    # —É–±—Ä–∞—Ç—å –¥—É–±–ª—å —Ç–∏–ø–∞ "X X" –∏–ª–∏ "XX" (–æ—á–µ–Ω—å –≥—Ä—É–±–æ)
    if len(t) > 12:
        half = len(t) // 2
        if t[:half].strip() == t[half:].strip():
            t = t[:half].strip()

    t = re.sub(r'\s{2,}', ' ', t).strip()
    return t[:120]

def parse_date(text: str) -> Optional[datetime]:
    t = text.lower()
    try:
        m = re.search(r'(\d{1,2})[-](\d{1,2})\s+([–∞-—è]+)(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(2))
            month = MONTHS_RU.get(m.group(3), 0)
            year = int(m.group(4)) if m.group(4) else datetime.now().year
            if month:
                return datetime(year, month, day)

        m = re.search(r'(\d{1,2})\s+([–∞-—è]+)(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = MONTHS_RU.get(m.group(2), 0)
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if month:
                dt = datetime(year, month, day)
                if not m.group(3) and dt.date() < datetime.now().date():
                    dt = datetime(year + 1, month, day)
                return dt

        m = re.search(r'(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = MONTHS_SHORT.get(m.group(2)[:3], 0)
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if month:
                dt = datetime(year, month, day)
                if not m.group(3) and dt.date() < datetime.now().date():
                    dt = datetime(year + 1, month, day)
                return dt

        m = re.search(r'(\d{1,2})\.(\d{2})(?:\.(\d{4}))?', t)
        if m:
            day = int(m.group(1))
            month = int(m.group(2))
            year = int(m.group(3)) if m.group(3) else datetime.now().year
            if 1 <= month <= 12:
                return datetime(year, month, day)
    except Exception:
        pass
    return None

def is_future(dt: Optional[datetime]) -> bool:
    if not dt:
        return False
    return dt.date() > datetime.now().date()

def format_date(dt: datetime, time_str: str = None) -> str:
    months = {
        1: '—è–Ω–≤–∞—Ä—è', 2: '—Ñ–µ–≤—Ä–∞–ª—è', 3: '–º–∞—Ä—Ç–∞', 4: '–∞–ø—Ä–µ–ª—è',
        5: '–º–∞—è', 6: '–∏—é–Ω—è', 7: '–∏—é–ª—è', 8: '–∞–≤–≥—É—Å—Ç–∞',
        9: '—Å–µ–Ω—Ç—è–±—Ä—è', 10: '–æ–∫—Ç—è–±—Ä—è', 11: '–Ω–æ—è–±—Ä—è', 12: '–¥–µ–∫–∞–±—Ä—è',
    }
    result = f"{dt.day} {months[dt.month]} {dt.year}"
    if time_str:
        result += f", {time_str}"
    return result

def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t:
            return value
    return None

def extract_venue(text: str) -> Optional[str]:
    venues = [
        'Narxoz', 'Nazarbayev', 'KBTU', '–ö–ë–¢–£', 'Astana Hub',
        'IT Park', 'MOST IT Hub', 'Holiday Inn', 'Esentai',
        'Yandex', 'Smart Point', 'Almaty Arena',
    ]
    for v in venues:
        if v.lower() in text.lower():
            m = re.search(rf'{re.escape(v)}[^\n,.]*', text, re.IGNORECASE)
            if m:
                return m.group(0).strip()[:60]
    at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', text)
    if at_m:
        return at_m.group(1).strip()[:60]
    return None

def is_real_event(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS)

def is_site_trash(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in SITE_STOP_WORDS)


# ‚îÄ‚îÄ‚îÄ glued line parser ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line).strip()

    m = re.match(
        r'^(\d{1,2})\s+([–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,})[,\s]+(\d{1,2}:\d{2})'
        r'([–ê-–Ø–Å][–∞-—è—ë]+(?:\s[–ê-–Ø–Å][–∞-—è—ë]+)?)?'
        r'\s*(.+)$',
        line
    )
    if not m:
        return None

    day_raw = m.group(1)
    month_raw = m.group(2).lower()[:3]
    time_str = m.group(3)
    possible_city = (m.group(4) or '').strip()
    title_raw = m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_raw, 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if m.group(2).lower().startswith(k[:3]):
                month_num = v
                break
    if not month_num:
        return None

    year = datetime.now().year
    try:
        dt = datetime(year, month_num, int(day_raw))
        if dt.date() < datetime.now().date():
            dt = datetime(year + 1, month_num, int(day_raw))
    except Exception:
        return None

    city = None
    if possible_city:
        city_key = possible_city.lower()
        city = KZ_CITIES.get(city_key)

    title_raw = cleanup_title(title_raw, location=city or possible_city or "")

    if len(title_raw) < 5:
        return None

    return {
        'dt': dt,
        'time_str': time_str,
        'city': city or (possible_city if possible_city else None),
        'title': title_raw[:120],
        'date_formatted': format_date(dt, time_str),
    }


# ‚îÄ‚îÄ‚îÄ Claude API (–æ—Å—Ç–∞–≤–∏–ª –∫–∞–∫ —É —Ç–µ–±—è) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def claude_clean_title(raw_text: str, session: aiohttp.ClientSession) -> Optional[str]:
    if not CLAUDE_API_KEY:
        return None

    prompt = (
        "–ò–∑ —Ç–µ–∫—Å—Ç–∞ –Ω–∏–∂–µ –∏–∑–≤–ª–µ–∫–∏ –¢–û–õ–¨–ö–û –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è (–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏, –º–∏—Ç–∞–ø–∞, —Ö–∞–∫–∞—Ç–æ–Ω–∞ –∏ —Ç.–¥.).\n"
        "–ü—Ä–∞–≤–∏–ª–∞:\n"
        "- –í–µ—Ä–Ω–∏ –û–î–ù–£ —Å—Ç—Ä–æ–∫—É ‚Äî —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ, –±–µ–∑ –¥–∞—Ç—ã, –≥–æ—Ä–æ–¥–∞, —Å—Å—ã–ª–æ–∫\n"
        "- –ï—Å–ª–∏ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å–ª–∏–ø—à–µ–µ—Å—è ‚Äî —Ä–∞–∑–±–µ–π –ø—Ä–æ–±–µ–ª–æ–º\n"
        "- –ï—Å–ª–∏ –¥—É–±–ª—å –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ ‚Äî —É–±–µ—Ä–∏ –ø–æ–≤—Ç–æ—Ä\n"
        "- –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –∏–ª–∏ –Ω–µ–ª—å–∑—è –∏–∑–≤–ª–µ—á—å –Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ ‚Äî –æ—Ç–≤–µ—Ç—å: SKIP\n"
        "- –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ, —Ç–æ–ª—å–∫–æ –Ω–∞–∑–≤–∞–Ω–∏–µ\n\n"
        f"–¢–µ–∫—Å—Ç:\n{raw_text[:600]}"
    )

    try:
        async with session.post(
            "https://api.anthropic.com/v1/messages",
            headers={
                "x-api-key": CLAUDE_API_KEY,
                "anthropic-version": "2023-06-01",
                "content-type": "application/json",
            },
            json={
                "model": "claude-haiku-4-5-20251001",
                "max_tokens": 80,
                "messages": [{"role": "user", "content": prompt}],
            },
            timeout=10,
        ) as resp:
            if resp.status != 200:
                return None
            data = await resp.json()
            result = data["content"][0]["text"].strip()
            if result.upper() == "SKIP" or len(result) < 5:
                remember_bad_pattern(raw_text)
                return None
            return result[:120]
    except Exception as e:
        logger.error(f"Claude API error: {e}")
        return None


# ‚îÄ‚îÄ‚îÄ Post formatting ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def make_post(event: Dict) -> str:
    title = (event.get('title') or '').strip()
    if not title or len(title) < 5:
        return ""
    if not event.get('date') or not event.get('link'):
        return ""

    location = event.get('location', '') or ''
    venue = event.get('venue', '') or ''

    # –ö–ª—é—á–µ–≤–∞—è —á–∞—Å—Ç—å: —á–∏—Å—Ç–∏–º title –æ—Ç –¥–∞—Ç—ã/–≥–æ—Ä–æ–¥–∞ –í–°–ï–ì–î–ê
    title = cleanup_title(title, location=location)

    lines = [f"üéØ <b>{title}</b>"]

    if location in ('–û–Ω–ª–∞–π–Ω', '–û–Ω–ª–∞–π–Ω (Zoom)'):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue:
        lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {event['date']}")
    lines.append(f"üîó <a href='{event['link']}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)


# ‚îÄ‚îÄ‚îÄ Bot ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(headers={'User-Agent': 'Mozilla/5.0'})
        return self.session

    async def close(self):
        if self.session:
            await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            session = await self.get_session()
            async with session.get(url, timeout=15) as resp:
                return await resp.text() if resp.status == 200 else ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞: {url} - {e}")
            return ""

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split('\n')
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            date_match = re.match(
                r'^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?'
                r'|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*'
                r'(?:\s+\d{4})?)',
                line, re.IGNORECASE
            )
            if not date_match:
                i += 1
                continue

            date_raw = date_match.group(0)
            rest = line[date_match.end():].strip()

            time_match = re.search(r'(?:–≤\s*)?(\d{1,2}:\d{2})', rest)
            time_str = time_match.group(1) if time_match else None
            if time_match:
                rest = (rest[:time_match.start()] + rest[time_match.end():]).strip()

            title_raw = strip_emoji(rest).strip(' -‚Äì‚Ä¢')

            link = None
            lm = re.search(r'((?:https?://|t\.me/)\S+)', line)
            if lm:
                link = lm.group(1)
                if not link.startswith('http'):
                    link = 'https://' + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), '').strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm = re.search(r'((?:https?://|t\.me/)\S+)', lines[j])
                    if lm:
                        link = lm.group(1)
                        if not link.startswith('http'):
                            link = 'https://' + link
                        break

            venue = None
            at_m = re.search(r'@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)', title_raw)
            if at_m:
                venue = at_m.group(1).strip()
                title_raw = title_raw[:at_m.start()].strip()

            if len(title_raw) < 5 and i + 1 < len(lines):
                next_line = strip_emoji(lines[i + 1]).strip()
                if len(next_line) > 5 and not re.match(r'^\d', next_line):
                    title_raw = next_line

            if len(title_raw) < 5:
                i += 1
                continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                i += 1
                continue

            context = line + ' ' + (lines[i + 1] if i + 1 < len(lines) else '')
            location = extract_location(context) or extract_location(text) or ''
            if not venue:
                venue = extract_venue(context)

            title_raw = cleanup_title(title_raw, location=location)

            events.append({
                'title': title_raw[:120],
                'date': format_date(dt, time_str),
                'location': location,
                'venue': venue,
                'link': link or post_link,
                'source': source,
                'image_url': image_url,
            })
            i += 1
        return events

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        url = f"https://t.me/s/{channel['username']}"
        html = await self.fetch(url)
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        all_events = []
        session = await self.get_session()

        for msg in soup.find_all('div', class_='tgme_widget_message')[:20]:
            try:
                text_div = msg.find('div', class_='tgme_widget_message_text')
                if not text_div:
                    continue

                text = text_div.get_text(separator='\n', strip=True)
                if len(text) < 30:
                    continue

                link_elem = msg.find('a', class_='tgme_widget_message_date')
                post_link = link_elem['href'] if link_elem else f"https://t.me/{channel['username']}"

                if post_link in self.posted:
                    continue
                self.posted.add(post_link)

                if matches_bad_pattern(text):
                    logger.info(f"‚è≠Ô∏è –ü–ª–æ—Ö–æ–π –ø–∞—Ç—Ç–µ—Ä–Ω (–∏–∑ –ø–∞–º—è—Ç–∏): {text[:50].strip()}")
                    continue

                image_url = None
                img_div = msg.find('a', class_='tgme_widget_message_photo_wrap')
                if img_div:
                    style = img_div.get('style', '')
                    m = re.search(r"url\('([^']+)'\)", style)
                    if m:
                        image_url = m.group(1)

                is_digest = bool(re.search(r'\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}', text))
                if is_digest:
                    events = self.parse_digest(text, post_link, channel['name'], image_url)
                    all_events.extend(events)
                    logger.info(f"üìã –î–∞–π–¥–∂–µ—Å—Ç {channel['name']}: {len(events)} —Å–æ–±—ã—Ç–∏–π")
                    continue

                if not is_real_event(text):
                    logger.info(f"‚è≠Ô∏è –ù–µ –∏–≤–µ–Ω—Ç: {text[:50].strip()}")
                    continue

                first_line = ''
                for ln in text.strip().split('\n'):
                    clean = strip_emoji(ln).strip()
                    if len(clean) > 10:
                        first_line = clean
                        break

                glued_data = None
                has_glue = bool(re.search(
                    r'\d{1,2}\s+[–ê-–Ø–Å–∞-—è—ëA-Za-z]{3,}[,\s]+\d{1,2}:\d{2}[–ê-–Ø–ÅA-Za-z]',
                    first_line
                ))

                if has_glue:
                    glued_data = parse_glued_line(first_line)
                    if not glued_data:
                        remember_bad_pattern(text)
                        logger.info(f"‚è≠Ô∏è –ù–µ —Å–º–æ–≥–ª–∏ —Ä–∞–∑–æ–±—Ä–∞—Ç—å –ø—Ä–∏–∫–ª–µ–µ–Ω–Ω—É—é –¥–∞—Ç—É, –ø—Ä–æ–ø—É—Å–∫: {first_line[:60]}")
                        continue

                if glued_data:
                    title = glued_data['title']
                    dt = glued_data['dt']
                    time_str = glued_data['time_str']
                    location = glued_data['city'] or extract_location(text) or ''

                    looks_dirty = (
                        re.search(r'[–ê-–Ø–Å]{2,}[a-zA-Z]', title) or
                        len(title) > 80 or
                        title.count(' ') < 1
                    )
                    if looks_dirty and CLAUDE_API_KEY:
                        clean = await claude_clean_title(text, session)
                        if clean is None:
                            logger.info(f"‚è≠Ô∏è Claude —Å–∫–∞–∑–∞–ª SKIP: {title[:50]}")
                            continue
                        title = clean

                    if not is_future(dt):
                        continue

                    # —Ñ–∏–Ω–∞–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ (–¥–∞—Ç–∞/–≥–æ—Ä–æ–¥ –∏–∑ title)
                    title = cleanup_title(title, location=location)

                    all_events.append({
                        'title': title,
                        'date': format_date(dt, time_str),
                        'location': location,
                        'venue': extract_venue(text),
                        'link': post_link,
                        'source': channel['name'],
                        'image_url': image_url,
                    })
                    continue

                dt = parse_date(text)
                if not is_future(dt):
                    logger.info(f"‚è≠Ô∏è {'–ü—Ä–æ—à–µ–¥—à–µ–µ' if dt else '–ù–µ—Ç –¥–∞—Ç—ã'}: {text[:50].strip()}")
                    continue

                title = None
                if CLAUDE_API_KEY:
                    title = await claude_clean_title(text, session)
                    if title is None:
                        logger.info(f"‚è≠Ô∏è Claude —Å–∫–∞–∑–∞–ª SKIP: {text[:50].strip()}")
                        continue
                else:
                    for ln in text.split('\n'):
                        ln = strip_emoji(ln).strip()
                        if len(ln) > 10 and not re.match(r'^\d{1,2}\s+[–∞-—è—ë]', ln.lower()):
                            title = ln[:120]
                            break

                if not title:
                    logger.info(f"‚è≠Ô∏è –ù–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∞: {text[:50].strip()}")
                    continue

                time_m = re.search(r'\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})', text)
                time_str = time_m.group(1) if time_m else None

                location = extract_location(text) or ''
                title = cleanup_title(title, location=location)

                all_events.append({
                    'title': title,
                    'date': format_date(dt, time_str),
                    'location': location,
                    'venue': extract_venue(text),
                    'link': post_link,
                    'source': channel['name'],
                    'image_url': image_url,
                })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
                continue

        return all_events

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site['url'])
        if not html:
            return []

        soup = BeautifulSoup(html, 'html.parser')
        events = []

        for link in soup.find_all('a', href=True)[:80]:
            try:
                href = link.get('href', '')
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15:
                    continue
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                if href.rstrip('/') == site['url'].rstrip('/'):
                    continue
                if href in self.posted:
                    continue
                if is_site_trash(title_raw):
                    continue
                if not is_real_event(title_raw):
                    continue

                parent = link.find_parent(['div', 'article', 'li', 'section'])
                context = parent.get_text(separator=' ', strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt):
                    continue

                location = extract_location(context) or ''
                venue = extract_venue(context)

                image_url = None
                img = (link.find('img', src=True) or
                       (parent.find('img', src=True) if parent else None))
                if img:
                    src = img.get('src', '')
                    if src and not src.startswith('http'):
                        from urllib.parse import urljoin
                        src = urljoin(site['url'], src)
                    image_url = src or None

                self.posted.add(href)

                title_clean = cleanup_title(title_raw, location=location)

                events.append({
                    'title': title_clean,
                    'date': format_date(dt),
                    'location': location,
                    'venue': venue,
                    'link': href,
                    'source': site['name'],
                    'image_url': image_url,
                })

                if len(events) >= 5:
                    break

            except Exception:
                continue

        return events

    async def get_all_events(self) -> List[Dict]:
        all_events = []

        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            events = await self.parse_site(site)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ {site['name']}: {len(events)}")

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤...")
        for channel in TELEGRAM_CHANNELS:
            events = await self.parse_channel(channel)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ {channel['name']}: {len(events)}")

        return all_events


# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = (e.get('title', '')[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")

        posted = 0
        for event in unique[:15]:
            text = make_post(event)
            if not text:
                continue
            try:
                if event.get('image_url'):
                    try:
                        await bot.send_photo(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            photo=event['image_url'],
                            caption=text,
                            parse_mode='HTML',
                        )
                    except Exception:
                        await bot.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode='HTML',
                            disable_web_page_preview=True,
                        )
                else:
                    await bot.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode='HTML',
                        disable_web_page_preview=True,
                    )

                posted += 1
                logger.info(f"‚úÖ ({posted}) {event.get('title','')[:50]}")
                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"‚ùå {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted}")

    finally:
        save_posted(bot_obj.posted)
        await bot_obj.close()


if __name__ == '__main__':
    asyncio.run(main())
