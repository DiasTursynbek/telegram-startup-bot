import os
import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict
import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot

logging.basicConfig(
    format='%(asctime)s - %(levelname)s - %(message)s',
    level=logging.INFO
)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏
BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID', "@vc_supergroup")
MESSAGE_THREAD_ID = int(os.getenv('MESSAGE_THREAD_ID', '4'))  # –¢–æ–ø–∏–∫ "–ê–Ω–æ–Ω—Å—ã –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–π"

# –ò—Å—Ç–æ—á–Ω–∏–∫–∏
URLS = [
    # –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –ø–æ—Ä—Ç–∞–ª—ã
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    
    # –í–µ–Ω—á—É—Ä–Ω—ã–µ —Ñ–æ–Ω–¥—ã
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    
    # –£–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç—ã
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    
    # Big Tech / –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]


class Parser:
    def __init__(self):
        self.session = None
        self.posted_cache = set()  # –ö—ç—à –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—ã—Ö —Å—Å—ã–ª–æ–∫
    
    async def get_session(self):
        if not self.session:
            self.session = aiohttp.ClientSession(
                headers={'User-Agent': 'Mozilla/5.0'}
            )
        return self.session
    
    async def close(self):
        if self.session:
            await self.session.close()
    
    async def fetch(self, url: str) -> str:
        try:
            session = await self.get_session()
            async with session.get(url, timeout=10) as resp:
                return await resp.text() if resp.status == 200 else ""
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
            return ""
    
    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site['url'])
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        events = []
        
        keywords = ['—Å—Ç–∞—Ä—Ç–∞–ø', 'startup', 'pitch', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', 'event', 'meetup']
        stop = ['–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–æ –Ω–∞—Å', '–ø–æ–ª–∏—Ç–∏–∫–∞']
        
        for link in soup.find_all('a', href=True)[:30]:
            try:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                if not href or not title or len(title) < 15:
                    continue
                
                if any(s in title.lower() for s in stop):
                    continue
                
                if not any(k in title.lower() for k in keywords):
                    continue
                
                if not href.startswith('http'):
                    from urllib.parse import urljoin
                    href = urljoin(site['url'], href)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —É–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–∏
                if href in self.posted_cache:
                    continue
                
                events.append({
                    'source': site['name'],
                    'description': title[:150],
                    'link': href
                })
                
                if len(events) >= 2:
                    break
            except:
                continue
        
        return events
    
    async def parse_telegram(self, channel: Dict) -> List[Dict]:
        try:
            url = f"https://t.me/s/{channel['username']}"
            html = await self.fetch(url)
            if not html:
                return []
            
            soup = BeautifulSoup(html, 'html.parser')
            events = []
            
            # –¢–æ–ª—å–∫–æ –ø–æ—Å—Ç—ã –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –¥–Ω—è
            cutoff_date = datetime.now() - timedelta(days=2)
            
            for msg in soup.find_all('div', class_='tgme_widget_message')[:10]:
                text_div = msg.find('div', class_='tgme_widget_message_text')
                if not text_div:
                    continue
                
                text = text_div.get_text(strip=True)
                if len(text) < 20:
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É –ø–æ—Å—Ç–∞
                time_elem = msg.find('time')
                if time_elem:
                    try:
                        post_date_str = time_elem.get('datetime', '')
                        post_date = datetime.fromisoformat(post_date_str.replace('Z', '+00:00'))
                        
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç–∞—Ä—ã–µ –ø–æ—Å—Ç—ã
                        if post_date < cutoff_date:
                            continue
                    except:
                        pass
                
                keywords = ['—Å—Ç–∞—Ä—Ç–∞–ø', 'event', 'meetup', '–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è', '—Ö–∞–∫–∞—Ç–æ–Ω', 'pitch']
                if not any(k in text.lower() for k in keywords):
                    continue
                
                link_elem = msg.find('a', class_='tgme_widget_message_date')
                link = link_elem['href'] if link_elem else f"https://t.me/{channel['username']}"
                
                events.append({
                    'source': channel['name'],
                    'description': text[:150],
                    'link': link
                })
            
            return events
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ TG {channel['name']}: {e}")
            return []
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []
        
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            events = await self.parse_site(site)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ {site['name']}: {len(events)} —Å–æ–±—ã—Ç–∏–π")
        
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} Telegram –∫–∞–Ω–∞–ª–æ–≤...")
        for channel in TELEGRAM_CHANNELS:
            events = await self.parse_telegram(channel)
            all_events.extend(events)
            if events:
                logger.info(f"‚úÖ TG {channel['name']}: {len(events)} —Å–æ–±—ã—Ç–∏–π")
        
        logger.info(f"üìä –í—Å–µ–≥–æ: {len(all_events)} —Å–æ–±—ã—Ç–∏–π")
        return all_events


async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç –ø—É–±–ª–∏–∫–∞—Ü–∏–∏...")
    
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        return
    
    parser = Parser()
    bot = Bot(token=BOT_TOKEN)
    
    try:
        events = await parser.get_all_events()
        
        if not events:
            logger.warning("‚ö†Ô∏è –°–æ–±—ã—Ç–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return
        
        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ —Å—Å—ã–ª–∫–µ
        unique_events = []
        seen_links = set()
        for event in events:
            if event['link'] not in seen_links:
                unique_events.append(event)
                seen_links.add(event['link'])
        
        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique_events)}")
        
        posted = 0
        for event in unique_events[:10]:
            try:
                text = f"<b>{event['source']}</b>\n\n{event['description']}\n\n{event['link']}"
                
                await bot.send_message(
                    chat_id=CHANNEL_ID,
                    message_thread_id=MESSAGE_THREAD_ID,  # –ü—É–±–ª–∏–∫—É–µ–º –≤ —Ç–æ–ø–∏–∫
                    text=text,
                    parse_mode='HTML',
                    disable_web_page_preview=False
                )
                
                parser.posted_cache.add(event['link'])
                posted += 1
                logger.info(f"‚úÖ –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ ({posted})")
                await asyncio.sleep(2)
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        
        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {posted}")
        
    finally:
        await parser.close()


if __name__ == '__main__':
    asyncio.run(main())
