import os
import asyncio
import logging
from datetime import datetime
from typing import List, Dict, Optional

import aiohttp
from bs4 import BeautifulSoup
from telegram import Bot
import re
import json
from pathlib import Path





STATE_DIR = Path("state")
POSTED_FILE = STATE_DIR / "load_posted.json"

logging.basicConfig(format="%(asctime)s - %(levelname)s - %(message)s", level=logging.INFO)
logger = logging.getLogger(__name__)

BOT_TOKEN = os.getenv("BOT_TOKEN")
CHANNEL_ID = os.getenv("CHANNEL_ID", "-1003812789640")
MESSAGE_THREAD_ID = int(os.getenv("MESSAGE_THREAD_ID", "4"))

def strip_intro_phrases(text: str) -> str:
    patterns = [
        r"^–≤\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–∫–∞–∂–¥(—É—é|—ã–π|–æ–µ)?\s+(–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫|–≤—Ç–æ—Ä–Ω–∏–∫|—Å—Ä–µ–¥—É|—á–µ—Ç–≤–µ—Ä–≥|–ø—è—Ç–Ω–∏—Ü—É|—Å—É–±–±–æ—Ç—É|–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ)\s+",
        r"^–≤\s+\w+\s+",  
        r"^–ø—Ä–∏–≥–ª–∞—à–∞–µ–º\s+",
        r"^—Å–æ—Å—Ç–æ–∏—Ç—Å—è\s+",
        r"^–±—É–¥–µ—Ç\s+",
    ]
    s = text.strip()
    for p in patterns:
        s = re.sub(p, "", s, flags=re.IGNORECASE)
    return s.strip(" -‚Äì‚Ä¢,")

def remove_city_from_title(title: str) -> str:
    for city_key in KZ_CITIES.keys():
        # –£–±–∏–≤–∞–µ–º –≥–æ—Ä–æ–¥ –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ —Å—Ç—Ä–æ–∫–∏ (–¥–∞–∂–µ –µ—Å–ª–∏ —Ç–∞–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è)
        title = re.sub(rf"^[^\w–∞-—è–ê-–Ø—ë–Åa-zA-Z]*{city_key}\b", "", title, flags=re.IGNORECASE)
        # –£–±–∏–≤–∞–µ–º –≥–æ—Ä–æ–¥, –µ—Å–ª–∏ –æ–Ω –≥–¥–µ-—Ç–æ –≤ —Ç–µ–∫—Å—Ç–µ
        title = re.sub(rf"\b{city_key}\b", "", title, flags=re.IGNORECASE)
        
    # –ó–∞—á–∏—â–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –∏ –≤–∏—Å—è—á–∏–µ –∑–∞–ø—è—Ç—ã–µ/—Ç–∏—Ä–µ –≤ –Ω–∞—á–∞–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
    title = re.sub(r"\s{2,}", " ", title)
    title = re.sub(r"^[,\-\s]+", "", title) 
    return title.strip(" -‚Äì‚Ä¢:,!")

def fix_glued_words(text: str) -> str:
    # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞—Å–∫–ª–µ–∏–≤–∞–µ—Ç "–≤Qostanai" -> "–≤ Qostanai" –∏ "Hub–ø—Ä–æ–π–¥–µ—Ç" -> "Hub –ø—Ä–æ–π–¥–µ—Ç"
    text = re.sub(r'([–∞-—è—ë–ê-–Ø–Å])([A-Za-z])', r'\1 \2', text)
    text = re.sub(r'([A-Za-z])([–∞-—è—ë–ê-–Ø–Å])', r'\1 \2', text)
    text = re.sub(r'([–∞-—è—ë])([–ê-–Ø–Å])', r'\1 \2', text)
    
    # –£–±–∏—Ä–∞–µ–º –¥–≤–æ–π–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–≥–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥–ª–∏ –ø–æ—è–≤–∏—Ç—å—Å—è ("–≤ –≤ Qostanai" -> "–≤ Qostanai")
    text = re.sub(r'\b(–≤|–Ω–∞)\s+\1\b', r'\1', text, flags=re.IGNORECASE)
    return text

def extract_city_from_title(title: str) -> Optional[str]:
    lower = title.lower()
    for key, value in KZ_CITIES.items():
        if key in lower:
            return value
    return None

def is_clean_photo(url: str) -> bool:
    url = url.lower()
    blacklist = [
        "icon", "logo", "avatar", "thumbnail", "svg", "button", "background", "footer"
    ]
    return not any(word in url for word in blacklist)

def remove_weekday_from_start(text: str) -> str:
    lower = text.lower()
    for key in WEEK_DAYS.keys():
        if lower.startswith(key + " "):
            return text[len(key):].strip(" -‚Äì‚Ä¢, ")
        if lower.startswith("–≤ " + key + " "):
            return text[len("–≤ " + key):].strip(" -‚Äì‚Ä¢, ")
        if lower.startswith("–∫–∞–∂–¥—É—é " + key + " "):
            return text[len("–∫–∞–∂–¥—É—é " + key):].strip(" -‚Äì‚Ä¢, ")
    return text

def generate_universal_description(full_text: str, title: str) -> str:
    text = strip_emoji(full_text)
    text = normalize_glued_text(text)

    if title:
        text = re.sub(re.escape(title), "", text, flags=re.IGNORECASE)

    text = re.sub(r"http\S+", "", text)
    text = re.sub(r'\b–≤\s+–≤\b', '–≤', text, flags=re.IGNORECASE)
    
    paragraphs = [p.strip() for p in text.split("\n") if len(p.strip()) > 50]

    for p in paragraphs:
        low = p.lower()
        if any(w in low for w in ["—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å", "telegram", "facebook", "whatsapp", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"]):
            continue
        if any(x in low for x in ["—É–ª.", "—É–ª–∏—Ü–∞", "–ø—Ä.", "–ø—Ä–æ—Å–ø–µ–∫—Ç", "—ç—Ç–∞–∂", "–æ—Ñ–∏—Å", "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü", "–∑–¥–∞–Ω–∏–µ", "—Ä–∞–π–æ–Ω", "–æ—Å—Ç–∞–Ω–æ–≤"]):
            continue
        if re.search(r"\d{1,2}:\d{2}", p):
            continue
        if re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]+", p):
            continue

        words = p.split()
        if len(words) > 12:  
            if len(words) > 40:
                return " ".join(words[:40]) + "..."
            return p
    return ""

def generate_fallback_description(title: str) -> str:
    t = title.lower()
    if "career" in t: return "–ü—Ä–æ—Ñ–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏–æ–Ω–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —à–∫–æ–ª—å–Ω–∏–∫–æ–≤ –∏ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –æ –≤—ã–±–æ—Ä–µ –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏ –∏ –∫–∞—Ä—å–µ—Ä–Ω—ã—Ö –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è—Ö."
    if "movie" in t: return "–ö–∏–Ω–æ-–≤—Å—Ç—Ä–µ—á–∞ —Å –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π –∏ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ IT."
    if "ai" in t or "–∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç" in t: return "–ú–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ, –ø–æ—Å–≤—è—â—ë–Ω–Ω–æ–µ –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–æ–º—É –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É –∏ –µ–≥–æ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—é –≤ –±–∏–∑–Ω–µ—Å–µ –∏ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è—Ö."
    if "meetup" in t: return "–ù–µ—Ñ–æ—Ä–º–∞–ª—å–Ω–∞—è –≤—Å—Ç—Ä–µ—á–∞ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª–æ–≤ –¥–ª—è –æ–±–º–µ–Ω–∞ –æ–ø—ã—Ç–æ–º –∏ –Ω–µ—Ç–≤–æ—Ä–∫–∏–Ω–≥–∞."
    if "—Ñ–æ—Ä—É–º" in t or "conference" in t: return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ —Å–æ–±—ã—Ç–∏–µ —Å —É—á–∞—Å—Ç–∏–µ–º —ç–∫—Å–ø–µ—Ä—Ç–æ–≤ –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ–º –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –æ—Ç—Ä–∞—Å–ª–µ–≤—ã—Ö —Ç–µ–º."
    return "–ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ–µ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –∏ –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π."

def extract_program_block(full_text: str) -> str:
    text = strip_emoji(full_text)
    lines = text.split("\n")
    trigger_words = ["—á—Ç–æ —Ç–µ–±—è –∂–¥", "—á—Ç–æ –≤–∞—Å –∂–¥", "–≤ –ø—Ä–æ–≥—Ä–∞–º–º–µ", "–ø—Ä–æ–≥—Ä–∞–º–º–∞", "–≤—ã —É–∑–Ω–∞–µ—Ç–µ"]
    start_index = None

    for i, line in enumerate(lines):
        low = line.lower()
        if any(word in low for word in trigger_words):
            start_index = i
            break

    if start_index is None:
        return ""

    collected = []
    for line in lines[start_index + 1:]:
        clean = line.strip()
        if not clean:
            continue
        if any(x in clean.lower() for x in ["üìç", "‚è∞", "http", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ"]):
            break
        if re.search(r"\d{1,2}:\d{2}", clean):
            break
        if len(clean) < 5:
            continue
        collected.append(clean)
        if len(collected) >= 4:
            break

    if not collected:
        return ""

    result = "\n".join(collected)
    words = result.split()
    if len(words) > 40:
        result = " ".join(words[:40]) + "..."
    return result.strip()

def normalize_link(link: str) -> str:
    if not link: return ""
    link = link.strip()
    link = link.replace("https://t.me/s/", "https://t.me/")
    link = link.split("?")[0]
    link = link.rstrip("/")
    return link

def load_posted() -> set:
    if POSTED_FILE.exists():
        try:
            with open(POSTED_FILE, "r", encoding="utf-8") as f:
                return set(json.load(f))
        except Exception:
            return set()
    return set()

def save_posted(posted: set):
    try:
        STATE_DIR.mkdir(parents=True, exist_ok=True)
        with open(POSTED_FILE, "w", encoding="utf-8") as f:
            json.dump(list(posted), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è posted_links: {e}")

URLS = [
    {"url": "https://astanahub.com/ru/event/", "name": "Astana Hub"},
    {"url": "https://er10.kz", "name": "ER10"},
    {"url": "https://kapital.kz", "name": "Capital"},
    {"url": "https://forbes.kz", "name": "Forbes kz"},
    {"url": "https://kz.kursiv.media", "name": "Kursiv kz"},
    {"url": "https://ma7.vc", "name": "MA7"},
    {"url": "https://tumarventures.com", "name": "Tumar ventures"},
    {"url": "https://whitehillcapital.io", "name": "White hill capital"},
    {"url": "https://bigsky.vc", "name": "Big sky ventures"},
    {"url": "https://mostfund.vc", "name": "Most ventures"},
    {"url": "https://axiomcapital.com", "name": "Axiom capital"},
    {"url": "https://jastarventures.com", "name": "Jas ventures"},
    {"url": "https://nuris.nu.edu.kz", "name": "NURIS"},
    {"url": "https://tech.kz", "name": "Big Tech"},
]

TELEGRAM_CHANNELS = [
    {"username": "startup_course_com", "name": "Startup Course"},
    {"username": "digitalbusinesskz", "name": "Digital Business KZ"},
    {"username": "vcinsightskz", "name": "VC Insights KZ"},
    {"username": "tech_kz", "name": "Tech KZ"},
    {"username": "startupalmaty", "name": "Startup Almaty"},
    {"username": "astanahub_events", "name": "Astana Hub Events"},
]

MONTHS_RU = {"—è–Ω–≤–∞—Ä—è": 1, "—Ñ–µ–≤—Ä–∞–ª—è": 2, "–º–∞—Ä—Ç–∞": 3, "–∞–ø—Ä–µ–ª—è": 4, "–º–∞—è": 5, "–∏—é–Ω—è": 6, "–∏—é–ª—è": 7, "–∞–≤–≥—É—Å—Ç–∞": 8, "—Å–µ–Ω—Ç—è–±—Ä—è": 9, "–æ–∫—Ç—è–±—Ä—è": 10, "–Ω–æ—è–±—Ä—è": 11, "–¥–µ–∫–∞–±—Ä—è": 12}
MONTHS_SHORT = {"—è–Ω–≤": 1, "—Ñ–µ–≤": 2, "–º–∞—Ä": 3, "–∞–ø—Ä": 4, "–º–∞–π": 5, "–∏—é–Ω": 6, "–∏—é–ª": 7, "–∞–≤–≥": 8, "—Å–µ–Ω": 9, "–æ–∫—Ç": 10, "–Ω–æ—è": 11, "–¥–µ–∫": 12}

EVENT_WORDS = [
    "–∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏—è", "conference", "—Ñ–æ—Ä—É–º", "forum", "summit", "—Å–∞–º–º–∏—Ç", "meetup", "–º–∏—Ç–∞–ø",
    "—Ö–∞–∫–∞—Ç–æ–Ω", "hackathon", "–≤–æ—Ä–∫—à–æ–ø", "workshop", "–º–∞—Å—Ç–µ—Ä-–∫–ª–∞—Å—Å", "masterclass", "–≤–µ–±–∏–Ω–∞—Ä",
    "webinar", "—Å–µ–º–∏–Ω–∞—Ä", "pitch", "–ø–∏—Ç—á", "demo day", "–∞–∫—Å–µ–ª–µ—Ä–∞—Ç–æ—Ä", "accelerator",
    "bootcamp", "–±—É—Ç–∫–µ–º–ø", "–≤—ã—Å—Ç–∞–≤–∫–∞", "–∫–æ–Ω–∫—É—Ä—Å", "competition", "—Ç—Ä–µ–Ω–∏–Ω–≥", "training",
    "–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–µ", "–∏–≤–µ–Ω—Ç", "event", "–ø—Ä–∏–≥–ª–∞—à–∞–µ—Ç", "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º", "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è"
]

NOT_EVENT_WORDS = [
    "research", "–∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø–æ–∫–∞–∑–∞–ª–æ", "–∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª", "–ø—Ä–∏–≤–ª–µ–∫ —Ä–∞—É–Ω–¥", "–º–ª–Ω $", "–º–ª—Ä–¥ $",
    "–∫—É—Ä—Å –¥–æ–ª–ª–∞—Ä–∞", "–±–∏—Ä–∂–∞", "–∞–∫—Ü–∏–∏", "—Ç–æ–∫–∞–µ–≤", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–Ω–∞–∑–Ω–∞—á–µ–Ω", "—É–≤–æ–ª–µ–Ω", "–≤—ã—Ä—É—á–∫–∞",
    "–≥–æ—Å–¥–æ—Ö–æ–¥", "–∑–∞–Ω—è—Ç–æ—Å—Ç–∏ –Ω–∞—Å–µ–ª–µ–Ω–∏—è", "–∏–Ω—Å–ø–µ–∫—Ç–æ—Ä", "–≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö", "–≥–æ—Å–æ—Ä–≥–∞–Ω", 
    "–∞–∫–∏–º–∞—Ç", "–º–∏–Ω–∏—Å—Ç–µ—Ä—Å—Ç–≤", "–Ω–∞–ª–æ–≥–æ–≤", "–±—É—Ö–≥–∞–ª—Ç–µ—Ä", "–∫–∞–¥—Ä–æ–≤–æ–π —Å–ª—É–∂–±", "–ø–∞–ª–∞—Ç –ø—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª–µ–π", 
    "–∞—Ç–∞–º–µ–∫–µ–Ω", "–≥–æ—Å–∑–∞–∫—É–ø", "—Å—É–±—Å–∏–¥–∏", "—Å–µ–ª—å—Å–∫–æ–≥–æ —Ö–æ–∑—è–π—Å—Ç–≤", "–∑–∫–æ", "–≤–∫–æ", "—é–∫–æ", "—Å“õ–æ",
    "–ø–µ–Ω—Å–∏–æ–Ω–Ω", "–∑–∞–∫–æ–Ω–æ–ø—Ä–æ–µ–∫—Ç", "–¥–µ–ø—É—Ç–∞—Ç", "–º–∞—Å–ª–∏—Ö–∞—Ç", "–º–∞–∂–∏–ª–∏—Å", "–Ω–∞–ª–æ–≥–æ–æ–±–ª–æ–∂–µ–Ω", "—à—Ç—Ä–∞—Ñ", 
    "–ø—Ä–æ–≤–µ—Ä–∫", "–∏–Ω—Å–ø–µ–∫—Ü–∏", "—Å–¥–∞—á–∞ –æ—Ç—á–µ—Ç", "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ-—Ä–∞–∑—ä—è—Å–Ω–∏—Ç–µ–ª—å–Ω–∞—è",
    "–∫–¥–ø", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã–º –¥–∞–Ω–Ω—ã–º", "–ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö", "—Ä–µ–≥–ª–∞–º–µ–Ω—Ç", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä", 
    "–∫–æ–º–ø–ª–∞–µ–Ω—Å", "compliance", "—Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º —Ä–µ–≥—É–ª—è—Ç–æ—Ä–æ–≤", "–æ—Ö—Ä–∞–Ω–∞ —Ç—Ä—É–¥–∞", "—Ç–µ—Ö–Ω–∏–∫–∞ –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏",
    "—Å–∞–ª–æ–Ω –∫—Ä–∞—Å–æ—Ç—ã", "–º–∞–Ω–∏–∫—é—Ä", "–ø–æ–≤–∞—Ä", "–∫—É–ª–∏–Ω–∞—Ä", "—à–≤–µ—è", "—Ä–µ–º–æ–Ω—Ç", "—Å–∞–Ω—Ç–µ—Ö–Ω–∏–∫",
    "–¥–µ—Ç—Å–∫–∏–π —Å–∞–¥", "—É—Ç—Ä–µ–Ω–Ω–∏–∫", "—à–∫–æ–ª—å–Ω–∞—è —è—Ä–º–∞—Ä–∫–∞",
    "machine learning", "–º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ", "–º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é", "–º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
    "python", "javascript", "c++", "c#", "php", "golang", "ruby", "swift", "kotlin", "java",
    "html", "css", "—Ñ—Ä–æ–Ω—Ç–µ–Ω–¥", "–±—ç–∫–µ–Ω–¥", "frontend", "backend",
    "—Å—Ç–∞—Ä—Ç –≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏", "—è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", "—è–∑—ã–∫–æ–≤ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è", 
    "–Ω–∞—É—á–∏—Ç—å—Å—è –∫–æ–¥–∏—Ç—å", "—Å—Ç–∞–Ω—å —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º", "–ø—Ä–æ—Ñ–µ—Å—Å–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤—â–∏–∫", "–≤–æ–π—Ç–∏ –≤ it",
    "—Å –Ω—É–ª—è –¥–æ", "–æ–±—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é", "–±–∞–∑–æ–≤—ã–π –∫—É—Ä—Å", "–¥–ª—è –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–≤"
]

SITE_STOP_WORDS = [
    "–∫–æ–Ω—Ç–∞–∫—Ç—ã", "–æ –Ω–∞—Å", "–ø–æ–ª–∏—Ç–∏–∫–∞", "–≤–æ–π—Ç–∏", "—Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –∞–∫–∫–∞—É–Ω—Ç–∞", "–ø–æ–¥–ø–∏—Å–∞—Ç—å—Å—è",
    "–ø–æ–∏—Å–∫", "–≥–ª–∞–≤–Ω–∞—è", "–º–µ–Ω—é", "–≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏", "—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ", "—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ", "privacy", "terms", "cookie"
]

DESCRIPTION_SIGNALS = [
    "—Ñ–æ—Ä–º–∞—Ç –≤—Å—Ç—Ä–µ—á–∏", "–≤—ã—Å—Ç—É–ø–ª–µ–Ω–∏–µ —Å–ø–∏–∫–µ—Ä–æ–≤", "–≤—ã —É–∑–Ω–∞–µ—Ç–µ", "–º—ã —Ä–∞—Å—Å–∫–∞–∂–µ–º", "–Ω–∞ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏–∏",
    "–≤ —Ä–∞–º–∫–∞—Ö", "—Å–æ—Å—Ç–æ–∏—Ç—Å—è –≤—Å—Ç—Ä–µ—á–∞", "–ø—Ä–∏–≥–ª–∞—à–∞–µ–º –≤–∞—Å", "–∑–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å", "–ø–æ–¥—Ä–æ–±–Ω–µ–µ –ø–æ —Å—Å—ã–ª–∫–µ",
    "—Å–≤–æ–±–æ–¥–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ", "–ø—Ä–∏–≥–ª–∞—à–∞—é—Ç –≤–∞—Å –ø—Ä–∏–Ω—è—Ç—å —É—á–∞—Å—Ç–∏–µ", "–≥–æ—Ç–æ–≤—ã –ø–µ—Ä–µ–π—Ç–∏", "–ø–æ–≥–æ–≤–æ—Ä–∏–º –æ", "–æ–±—Å—É–¥–∏–º", "—Ä–∞–∑–±–µ—Ä–µ–º" 
]

KZ_CITIES = {
    # –ú–µ–≥–∞–ø–æ–ª–∏—Å—ã
    "–∞–ª–º–∞—Ç—ã": "–ê–ª–º–∞—Ç—ã", "almaty": "–ê–ª–º–∞—Ç—ã",
    "–∞—Å—Ç–∞–Ω–∞": "–ê—Å—Ç–∞–Ω–∞", "astana": "–ê—Å—Ç–∞–Ω–∞", "nur-sultan": "–ê—Å—Ç–∞–Ω–∞", "nursultan": "–ê—Å—Ç–∞–Ω–∞", "–Ω—É—Ä-—Å—É–ª—Ç–∞–Ω": "–ê—Å—Ç–∞–Ω–∞", "–∞“õ–º–æ–ª–∞": "–ê—Å—Ç–∞–Ω–∞", "aqmola": "–ê—Å—Ç–∞–Ω–∞", "—Ü–µ–ª–∏–Ω–æ–≥—Ä–∞–¥": "–ê—Å—Ç–∞–Ω–∞", "tselinograd": "–ê—Å—Ç–∞–Ω–∞",
    "—à—ã–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç", "shymkent": "–®—ã–º–∫–µ–Ω—Ç", "—á–∏–º–∫–µ–Ω—Ç": "–®—ã–º–∫–µ–Ω—Ç", "chimkent": "–®—ã–º–∫–µ–Ω—Ç",

    # –û–±–ª–∞—Å—Ç–Ω—ã–µ —Ü–µ–Ω—Ç—Ä—ã –∏ –∫—Ä—É–ø–Ω—ã–µ –≥–æ—Ä–æ–¥–∞
    "–∫–∞—Ä–∞–≥–∞–Ω–¥–∞": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "karaganda": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "“õ–∞—Ä–∞“ì–∞–Ω–¥—ã": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "qaraghandy": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞", "qaragandi": "–ö–∞—Ä–∞–≥–∞–Ω–¥–∞",
    "–∞–∫—Ç–æ–±–µ": "–ê–∫—Ç–æ–±–µ", "aktobe": "–ê–∫—Ç–æ–±–µ", "–∞“õ—Ç”©–±–µ": "–ê–∫—Ç–æ–±–µ", "aqtobe": "–ê–∫—Ç–æ–±–µ", "–∞–∫—Ç—é–±–∏–Ω—Å–∫": "–ê–∫—Ç–æ–±–µ", "aktyubinsk": "–ê–∫—Ç–æ–±–µ",
    "—Ç–∞—Ä–∞–∑": "–¢–∞—Ä–∞–∑", "taraz": "–¢–∞—Ä–∞–∑", "–∂–∞–º–±—ã–ª": "–¢–∞—Ä–∞–∑", "zhambyl": "–¢–∞—Ä–∞–∑", "–¥–∂–∞–º–±—É–ª": "–¢–∞—Ä–∞–∑", "djambul": "–¢–∞—Ä–∞–∑", "jambul": "–¢–∞—Ä–∞–∑",
    "–ø–∞–≤–ª–æ–¥–∞—Ä": "–ü–∞–≤–ª–æ–¥–∞—Ä", "pavlodar": "–ü–∞–≤–ª–æ–¥–∞—Ä",
    "—É—Å—Ç—å-–∫–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "ust-kamenogorsk": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "”©—Å–∫–µ–º–µ–Ω": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "oskemen": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫", "ust-kamen": "–£—Å—Ç—å-–ö–∞–º–µ–Ω–æ–≥–æ—Ä—Å–∫",
    "—Å–µ–º–µ–π": "–°–µ–º–µ–π", "semey": "–°–µ–º–µ–π", "semei": "–°–µ–º–µ–π", "—Å–µ–º–∏–ø–∞–ª–∞—Ç–∏–Ω—Å–∫": "–°–µ–º–µ–π", "semipalatinsk": "–°–µ–º–µ–π",
    "–∞—Ç—ã—Ä–∞—É": "–ê—Ç—ã—Ä–∞—É", "atyrau": "–ê—Ç—ã—Ä–∞—É", "–≥—É—Ä—å–µ–≤": "–ê—Ç—ã—Ä–∞—É", "guriev": "–ê—Ç—ã—Ä–∞—É",
    "–∫–æ—Å—Ç–∞–Ω–∞–π": "–ö–æ—Å—Ç–∞–Ω–∞–π", "kostanay": "–ö–æ—Å—Ç–∞–Ω–∞–π", "“õ–æ—Å—Ç–∞–Ω–∞–π": "–ö–æ—Å—Ç–∞–Ω–∞–π", "qostanai": "–ö–æ—Å—Ç–∞–Ω–∞–π", "qostanay": "–ö–æ—Å—Ç–∞–Ω–∞–π", "–∫—É—Å—Ç–∞–Ω–∞–π": "–ö–æ—Å—Ç–∞–Ω–∞–π", "kustanai": "–ö–æ—Å—Ç–∞–Ω–∞–π", "kustanay": "–ö–æ—Å—Ç–∞–Ω–∞–π",
    "–∫—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "kyzylorda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "“õ—ã–∑—ã–ª–æ—Ä–¥–∞": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "qyzylorda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞", "kzyil-orda": "–ö—ã–∑—ã–ª–æ—Ä–¥–∞",
    "—É—Ä–∞–ª—å—Å–∫": "–£—Ä–∞–ª—å—Å–∫", "uralsk": "–£—Ä–∞–ª—å—Å–∫", "–æ—Ä–∞–ª": "–£—Ä–∞–ª—å—Å–∫", "oral": "–£—Ä–∞–ª—å—Å–∫",
    "–ø–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "petropavlovsk": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "–ø–µ—Ç—Ä–æ–ø–∞–≤–ª": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫", "petropavl": "–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫",
    "—Ç—É—Ä–∫–µ—Å—Ç–∞–Ω": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω", "turkestan": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω", "—Ç“Ø—Ä–∫—ñ—Å—Ç–∞–Ω": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω", "turkistan": "–¢—É—Ä–∫–µ—Å—Ç–∞–Ω",
    "–∞–∫—Ç–∞—É": "–ê–∫—Ç–∞—É", "aktau": "–ê–∫—Ç–∞—É", "–∞“õ—Ç–∞—É": "–ê–∫—Ç–∞—É", "aqtau": "–ê–∫—Ç–∞—É", "—à–µ–≤—á–µ–Ω–∫–æ": "–ê–∫—Ç–∞—É", "shevchenko": "–ê–∫—Ç–∞—É",
    "—Ç–µ–º–∏—Ä—Ç–∞—É": "–¢–µ–º–∏—Ä—Ç–∞—É", "temirtau": "–¢–µ–º–∏—Ä—Ç–∞—É",
    "–∫–æ–∫—à–µ—Ç–∞—É": "–ö–æ–∫—à–µ—Ç–∞—É", "kokshetau": "–ö–æ–∫—à–µ—Ç–∞—É", "–∫”©–∫—à–µ—Ç–∞—É": "–ö–æ–∫—à–µ—Ç–∞—É", "qokshetau": "–ö–æ–∫—à–µ—Ç–∞—É", "–∫–æ–∫—á–µ—Ç–∞–≤": "–ö–æ–∫—à–µ—Ç–∞—É", "kokchetav": "–ö–æ–∫—à–µ—Ç–∞—É",
    "—Ç–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "taldykorgan": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "—Ç–∞–ª–¥—ã“õ–æ—Ä“ì–∞–Ω": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "taldyqorgan": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω", "taldikorgan": "–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω",
    "—ç–∫–∏–±–∞—Å—Ç—É–∑": "–≠–∫–∏–±–∞—Å—Ç—É–∑", "ekibastuz": "–≠–∫–∏–±–∞—Å—Ç—É–∑", "–µ–∫—ñ–±–∞—Å—Ç“±–∑": "–≠–∫–∏–±–∞—Å—Ç—É–∑",
    "—Ä—É–¥–Ω—ã–π": "–†—É–¥–Ω—ã–π", "rudny": "–†—É–¥–Ω—ã–π", "rudniy": "–†—É–¥–Ω—ã–π", "rudnyi": "–†—É–¥–Ω—ã–π",
    "–∂–∞–Ω–∞–æ–∑–µ–Ω": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "zhanaozen": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "–∂–∞“£–∞”©–∑–µ–Ω": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "janaozen": "–ñ–∞–Ω–∞–æ–∑–µ–Ω", "–Ω–æ–≤—ã–π —É–∑–µ–Ω—å": "–ñ–∞–Ω–∞–æ–∑–µ–Ω",
    "–∫–æ–Ω–∞–µ–≤": "–ö–æ–Ω–∞–µ–≤", "konaev": "–ö–æ–Ω–∞–µ–≤", "“õ–æ–Ω–∞–µ–≤": "–ö–æ–Ω–∞–µ–≤", "qonaev": "–ö–æ–Ω–∞–µ–≤", "qonayev": "–ö–æ–Ω–∞–µ–≤", "–∫–∞–ø—á–∞–≥–∞–π": "–ö–æ–Ω–∞–µ–≤", "kapchagay": "–ö–æ–Ω–∞–µ–≤", "“õ–∞–ø—à–∞“ì–∞–π": "–ö–æ–Ω–∞–µ–≤", "qapshaghay": "–ö–æ–Ω–∞–µ–≤",
    "–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "zhezkazgan": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "–∂–µ–∑“õ–∞–∑“ì–∞–Ω": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "jezqazgan": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "–¥–∂–µ–∑–∫–∞–∑–≥–∞–Ω": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω", "dzhezkazgan": "–ñ–µ–∑–∫–∞–∑–≥–∞–Ω",
    "–±–∞–ª—Ö–∞—à": "–ë–∞–ª—Ö–∞—à", "balkhash": "–ë–∞–ª—Ö–∞—à", "–±–∞–ª“õ–∞—à": "–ë–∞–ª—Ö–∞—à", "balqash": "–ë–∞–ª—Ö–∞—à",
    "—Å–∞—Ç–ø–∞–µ–≤": "–°–∞—Ç–ø–∞–µ–≤", "satpayev": "–°–∞—Ç–ø–∞–µ–≤", "—Å”ô—Ç–±–∞–µ–≤": "–°–∞—Ç–ø–∞–µ–≤", "satbayev": "–°–∞—Ç–ø–∞–µ–≤",
    "–∫–∞—Å–∫–µ–ª–µ–Ω": "–ö–∞—Å–∫–µ–ª–µ–Ω", "kaskelen": "–ö–∞—Å–∫–µ–ª–µ–Ω", "“õ–∞—Å–∫–µ–ª–µ“£": "–ö–∞—Å–∫–µ–ª–µ–Ω", "qaskelen": "–ö–∞—Å–∫–µ–ª–µ–Ω",
    "–∫—É–ª—å—Å–∞—Ä—ã": "–ö—É–ª—å—Å–∞—Ä—ã", "kulsary": "–ö—É–ª—å—Å–∞—Ä—ã", "“õ“±–ª—Å–∞—Ä—ã": "–ö—É–ª—å—Å–∞—Ä—ã", "qulsary": "–ö—É–ª—å—Å–∞—Ä—ã",


    "–æ–Ω–ª–∞–π–Ω": "–û–Ω–ª–∞–π–Ω", "online": "–û–Ω–ª–∞–π–Ω", "zoom": "–û–Ω–ª–∞–π–Ω (Zoom)", "–æ–Ω–ª–∞–π–Ω (zoom)": "–û–Ω–ª–∞–π–Ω (Zoom)",
    "—Ç–∞—à–∫–µ–Ω—Ç": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω", "tashkent": "–¢–∞—à–∫–µ–Ω—Ç, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω",
    "–±–∏—à–∫–µ–∫": "–ë–∏—à–∫–µ–∫, –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω", "bishkek": "–ë–∏—à–∫–µ–∫, –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω",
}

WEEK_DAYS = {
    "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–∞": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫—É": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–ø–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫–æ–º": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "monday": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
    "–≤—Ç–æ—Ä–Ω–∏–∫": "–í—Ç–æ—Ä–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫–∞": "–í—Ç–æ—Ä–Ω–∏–∫", "–≤—Ç–æ—Ä–Ω–∏–∫—É": "–í—Ç–æ—Ä–Ω–∏–∫", "tuesday": "–í—Ç–æ—Ä–Ω–∏–∫",
    "—Å—Ä–µ–¥–∞": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥—É": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥—ã": "–°—Ä–µ–¥–∞", "—Å—Ä–µ–¥–µ": "–°—Ä–µ–¥–∞", "wednesday": "–°—Ä–µ–¥–∞",
    "—á–µ—Ç–≤–µ—Ä–≥": "–ß–µ—Ç–≤–µ—Ä–≥", "—á–µ—Ç–≤–µ—Ä–≥–∞": "–ß–µ—Ç–≤–µ—Ä–≥", "—á–µ—Ç–≤–µ—Ä–≥—É": "–ß–µ—Ç–≤–µ—Ä–≥", "thursday": "–ß–µ—Ç–≤–µ—Ä–≥",
    "–ø—è—Ç–Ω–∏—Ü–∞": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü—É": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü—ã": "–ü—è—Ç–Ω–∏—Ü–∞", "–ø—è—Ç–Ω–∏—Ü–µ": "–ü—è—Ç–Ω–∏—Ü–∞", "friday": "–ü—è—Ç–Ω–∏—Ü–∞",
    "—Å—É–±–±–æ—Ç–∞": "–°—É–±–±–æ—Ç–∞", "—Å—É–±–±–æ—Ç—É": "–°—É–±–±–æ—Ç–∞", "—Å—É–±–±–æ—Ç—ã": "–°—É–±–±–æ—Ç–∞", "saturday": "–°—É–±–±–æ—Ç–∞",
    "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—è": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "–≤–æ—Å–∫—Ä–µ—Å–µ–Ω—å—é": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ", "sunday": "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ",
}

EMOJI_RE = re.compile("[\U00010000-\U0010ffff\u2600-\u27ff\u2300-\u23ff\u25a0-\u25ff\u2B00-\u2BFF]", re.UNICODE)

def strip_emoji(s: str) -> str:
    return EMOJI_RE.sub("", s).strip()

def is_future(dt: Optional[datetime]) -> bool:
    if not dt: return False
    return dt.date() > datetime.now().date()

def parse_date(text: str) -> Optional[datetime]:
    t = text.lower()
    now = datetime.now()

    def make_dt(year, month, day):
        try: return datetime(year, month, day)
        except: return None

    m = re.search(r"(\d{1,2})[-](\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(3), 0)
        year = int(m.group(4)) if m.group(4) else now.year
        if month: return make_dt(year, month, int(m.group(2)))

    m = re.search(r"(\d{1,2})\s+([–∞-—è—ë]+)(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_RU.get(m.group(2), 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(r"(\d{1,2})\s+(—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+(\d{4}))?", t)
    if m:
        month = MONTHS_SHORT.get(m.group(2)[:3], 0)
        if month:
            year = int(m.group(3)) if m.group(3) else now.year
            return make_dt(year, month, int(m.group(1)))

    m = re.search(r"(\d{1,2})\.(\d{2})(?:\.(\d{4}))?", t)
    if m:
        month = int(m.group(2))
        year = int(m.group(3)) if m.group(3) else now.year
        if 1 <= month <= 12: return make_dt(year, month, int(m.group(1)))

    return None

def format_date(dt: datetime, time_str: str = None) -> str:
    months = {1:"—è–Ω–≤–∞—Ä—è", 2:"—Ñ–µ–≤—Ä–∞–ª—è", 3:"–º–∞—Ä—Ç–∞", 4:"–∞–ø—Ä–µ–ª—è", 5:"–º–∞—è", 6:"–∏—é–Ω—è", 7:"–∏—é–ª—è", 8:"–∞–≤–≥—É—Å—Ç–∞", 9:"—Å–µ–Ω—Ç—è–±—Ä—è", 10:"–æ–∫—Ç—è–±—Ä—è", 11:"–Ω–æ—è–±—Ä—è", 12:"–¥–µ–∫–∞–±—Ä—è"}
    s = f"{dt.day} {months[dt.month]} {dt.year}"
    return f"{s}, {time_str}" if time_str else s

def extract_location(text: str) -> Optional[str]:
    t = text.lower()
    for key, value in KZ_CITIES.items():
        if key in t: return value
    return None

def extract_venue(text: str) -> Optional[str]:
    known = ["Narxoz", "Nazarbayev", "KBTU", "–ö–ë–¢–£", "Astana Hub", "IT Park", "MOST IT Hub", "Holiday Inn", "Esentai", "Yandex", "Smart Point", "Almaty Arena"]
    for v in known:
        if v.lower() in text.lower():
            m = re.search(rf"{re.escape(v)}[^\n,.]*", text, re.IGNORECASE)
            if m: return m.group(0).strip()[:60]
    at = re.search(r"@\s+([^@\n]+?)(?:\s+(?:https?://|t\.me/)|\s*$)", text)
    if at: return at.group(1).strip()[:60]
    return None

def is_real_event(text: str) -> bool:
    t = text.lower()
    return any(w in t for w in EVENT_WORDS) and not any(w in t for w in NOT_EVENT_WORDS)

def is_site_trash(title: str) -> bool:
    return any(s in title.lower() for s in SITE_STOP_WORDS)

def looks_like_description(title: str) -> bool:
    t = title.lower()
    return any(s in t for s in DESCRIPTION_SIGNALS)

def dedup_title(title: str) -> str:
    half = len(title) // 2
    for i in range(10, half):
        if title[i:].strip() == title[:len(title)-i].strip():
            return title[:len(title)-i].strip()
    return title

def normalize_glued_text(s: str) -> str:
    s = strip_emoji(s).strip()
    s = re.sub(r"(\d{1,2}:\d{2})(?=[A-Za-z–ê-–Ø–∞-—è–Å—ë])", r"\1 ", s)
    s = re.sub(r"([–∞-—è—ë–ê-–Ø–Å]{3,}),(\d{1,2}:\d{2})", r"\1, \2", s)
    s = re.sub(r"\s{2,}", " ", s)
    return s

def strip_leading_datetime_from_title(title: str) -> str:
    t = strip_emoji(title).strip()
    t = normalize_glued_text(t)
    t = re.sub(r"^\s*\d{1,2}\s+[–ê-–Ø–∞-—è–Å—ëA-Za-z]{3,}[,]?\s+\d{1,2}:\d{2}\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^\s*\d{1,2}\s+[–∞-—è—ë]{3,}(?:\s+\d{4})?\s*", "", t, flags=re.IGNORECASE)
    t = re.sub(r"^\s*\d{1,2}\.\d{2}(?:\.\d{4})?\s*", "", t)
    return t.strip(" -‚Äì‚Ä¢.,").strip()

def remove_dates_and_times(text: str) -> str:
    if not text:
        return ""
    text = re.sub(r'\b\d{1,2}:\d{2}(?:-\d{1,2}:\d{2})?(?:\s*[aApP][mM])?\b', '', text)
    text = re.sub(r'\b\d{1,2}(?:-[–∞-—è]{1,2})?\s+(?:—è–Ω–≤[–∞-—è]*|—Ñ–µ–≤[–∞-—è]*|–º–∞—Ä[–∞-—è]*|–∞–ø—Ä[–∞-—è]*|–º–∞—è|–º–∞–π|–∏—é–Ω[–∞-—è]*|–∏—é–ª[–∞-—è]*|–∞–≤–≥[–∞-—è]*|—Å–µ–Ω[–∞-—è]*|–æ–∫—Ç[–∞-—è]*|–Ω–æ—è[–∞-—è]*|–¥–µ–∫[–∞-—è]*)\s*(?:,?\s*\d{4}(?:\s*–≥\.?)?)?\b', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\b(?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[a-z]*|jul[a-z]*|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)\s+\d{1,2}(?:st|nd|rd|th)?(?:,?\s*\d{4})?\b', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\b\d{1,2}\s+(?:jan[a-z]*|feb[a-z]*|mar[a-z]*|apr[a-z]*|may|jun[a-z]*|jul[a-z]*|aug[a-z]*|sep[a-z]*|oct[a-z]*|nov[a-z]*|dec[a-z]*)\s*(?:,?\s*\d{4})?\b', '', text, flags=re.IGNORECASE)
    text = re.sub(r'\b\d{1,2}[./-]\d{1,2}(?:[./-]\d{2,4})?\b', '', text)
    text = re.sub(r',\s*\|', ' |', text) 
    text = re.sub(r'\|\s*\|', '|', text)
    text = re.sub(r'\s{2,}', ' ', text)
    return text.strip(" -‚Äì‚Ä¢.,|")

def clean_title_deterministic(raw_title: str) -> Optional[str]:
    s = strip_leading_datetime_from_title(raw_title)
    s = remove_weekday_from_start(s)
    s = strip_intro_phrases(s)
    s = fix_glued_words(s)
    s = dedup_title(s)
    s = remove_city_from_title(s)

    low = s.lower()
    for sig in DESCRIPTION_SIGNALS:
        idx = low.find(sig)
        if idx != -1 and idx > 12:
            s = s[:idx].strip(" -‚Äì‚Ä¢.,")
            break

    # –£–±–∏—Ä–∞–µ–º –ø–æ–≤–∏—Å—à–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∏ –≤ –∫–æ–Ω—Ü–µ –∏ –¥–≤–æ–π–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–≥–∏ ("–≤ –≤ Hub")
    s = re.sub(r'\b–≤\s+–≤\b', '–≤', s, flags=re.IGNORECASE)
    s = re.sub(r'\s+(–≤|–Ω–∞|—Å|–∏|–¥–ª—è|–æ—Ç|–∑–∞|–∫|–ø–æ|–∏–∑|—É|–æ|–æ–±|at|in|on|for|and|to|the)\s*$', '', s, flags=re.IGNORECASE)
    s = re.sub(r"\s{2,}", " ", s).strip(" -‚Äì‚Ä¢.,")
    
    if len(s) < 5 or looks_like_description(s): return None
    return s[:120]

city_pattern = "|".join([re.escape(v) for v in KZ_CITIES.values()])
_GLUE_RE = re.compile(
    rf"^(\d{{1,2}})\s+([–ê-–Ø–Å–∞-—è—ëA-Za-z]{{3,}})[,\s]+(\d{{1,2}}:\d{{2}})\s*(?:(–û–Ω–ª–∞–π–Ω|online|zoom|{city_pattern})\s*)?(.+)$",
    re.IGNORECASE
)

def parse_glued_line(line: str) -> Optional[Dict]:
    line = normalize_glued_text(line)
    m = _GLUE_RE.match(line)
    if not m: return None

    day_s, month_s, time_str = m.group(1), m.group(2).lower(), m.group(3)
    possible_city, title_raw = (m.group(4) or "").strip(), m.group(5).strip()

    month_num = MONTHS_SHORT.get(month_s[:3], 0)
    if not month_num:
        for k, v in MONTHS_RU.items():
            if month_s.startswith(k[:3]):
                month_num = v
                break
    if not month_num: return None

    try: dt = datetime(datetime.now().year, month_num, int(day_s))
    except: return None
    if not is_future(dt): return None

    city = KZ_CITIES.get(possible_city.lower(), possible_city) if possible_city else None
    title_raw = dedup_title(title_raw)
    if len(title_raw) < 5: return None

    return {"dt": dt, "time_str": time_str, "city": city, "title_raw": title_raw[:300], "date_formatted": format_date(dt, time_str)}

# ‚îÄ‚îÄ‚îÄ Formatting post ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def make_post(event: Dict) -> str:
    title = (event.get("title") or "").strip()
    date_str = (event.get("date") or "").strip()
    link = (event.get("link") or "").strip()

    if not title or len(title) < 5 or not date_str or not link:
        return ""

    location = event.get("location", "")
    venue = event.get("venue", "")
    title = strip_leading_datetime_from_title(title)

    deep_description = event.get("deep_description", "")
    # üî• –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –∫—Ä—É—Ç–æ–π –≤—Å—Ç—É–ø–∏—Ç–µ–ª—å–Ω—ã–π –∞–±–∑–∞—Ü, –∞ —Ç–æ–ª—å–∫–æ –ø–æ—Ç–æ–º —Å–ø–∏—Å–∫–∏
    univ_desc = generate_universal_description(event.get("full_text", ""), title)
    program_block = extract_program_block(event.get("full_text", ""))

    if deep_description:
        description = deep_description
    elif univ_desc: 
        description = univ_desc
    elif program_block:
        description = program_block
    else:
        description = generate_fallback_description(title)

    if description:
        desc_clean = strip_emoji(description).strip()
        desc_prefix = desc_clean[:25]
        
        if len(desc_prefix) > 15:
            idx = title.lower().find(desc_prefix.lower())
            if idx > 3:
                title = title[:idx].strip(" -‚Äì‚Ä¢.,:;|")
                title = re.sub(r'\s+(–≤|–Ω–∞|—Å|–∏|–¥–ª—è|–æ—Ç|–∑–∞|–∫|–ø–æ|–∏–∑|—É|–æ|–æ–±|at|in|on|for|and|to|the)\s*$', '', title, flags=re.IGNORECASE)
                title = title.strip()

    title = remove_dates_and_times(title)
    if description:
        description = remove_dates_and_times(description)

    lines = [f"üéØ <b>{title}</b>"]

    if description:
        lines.append("")
        lines.append(f"üìù {description}")

    if location in ("–û–Ω–ª–∞–π–Ω", "–û–Ω–ª–∞–π–Ω (Zoom)"):
        lines.append("üåê –û–Ω–ª–∞–π–Ω")
    elif location:
        lines.append(f"üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω, üèô {location}")
    else:
        lines.append("üá∞üáø –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω")

    if venue: 
        lines.append(f"üìç {venue}")

    lines.append(f"üìÖ {date_str}")
    lines.append(f"üîó <a href='{link}'>–ß–∏—Ç–∞—Ç—å ‚Üí</a>")

    return "\n".join(lines)


class EventBot:
    def __init__(self):
        self.session = None
        self.posted = load_posted()

    async def get_session(self) -> aiohttp.ClientSession:
        if not self.session:
            self.session = aiohttp.ClientSession(
                headers={"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"}
            )
        return self.session

    async def close(self):
        if self.session: await self.session.close()

    async def fetch(self, url: str) -> str:
        try:
            s = await self.get_session()
            async with s.get(url, timeout=15) as r:
                return await r.text() if r.status == 200 else ""
        except Exception as e:
            logger.error(f"fetch {url}: {e}")
            return ""

    async def fetch_event_details(self, url: str) -> Dict[str, str]:
        result = {"desc": "", "image": ""}
        if not url or not url.startswith("http") or "t.me" in url:
            return result

        try:
            html = await self.fetch(url)
            if not html: return result
            soup = BeautifulSoup(html, "html.parser")

            # üî• –ù–ê–•–û–î–ò–ú –ì–õ–ê–í–ù–û–ï –§–û–¢–û –°–ê–ô–¢–ê (og:image) - –û–Ω–æ 100% –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ!
            og_image = soup.find("meta", property="og:image")
            if og_image and og_image.get("content"):
                result["image"] = og_image["content"]

            for tag in soup(["script", "style", "nav", "footer", "header", "aside", "menu", "form"]):
                tag.decompose()

            bad_words = [
                "sedo domain", "domain parking", "webpage was generated", 
                "website is for sale", "source for information", "maintained by the domain owner",
                "disclaimer", "cloudflare", "access denied", "not found"
            ]

            for text in soup.stripped_strings:
                if len(text) > 80:
                    low = text.lower()
                    if any(bad in low for bad in bad_words):
                        return result
                    
                    latin_only = re.fullmatch(r'[A-Za-z0-9\s\.,!\?\-\(\)]+', text)
                    if latin_only and len(text) > 100:
                        return result

                    text = re.sub(r"\s{2,}", " ", text)
                    text = re.sub(r'\b–≤\s+–≤\b', '–≤', text, flags=re.IGNORECASE)
                    
                    words = text.split()
                    result["desc"] = " ".join(words[:40]) + "..." if len(words) > 40 else text
                    break # –ù–∞—à–ª–∏ —Ö–æ—Ä–æ—à–µ–µ –æ–ø–∏—Å–∞–Ω–∏–µ, –≤—ã—Ö–æ–¥–∏–º

            if not result["desc"]:
                meta_desc = soup.find("meta", attrs={"name": "description"})
                if meta_desc and meta_desc.get("content"):
                    desc = meta_desc["content"].strip()
                    if not any(bad in desc.lower() for bad in bad_words):
                        result["desc"] = desc
        except Exception:
            pass
            
        return result

    def parse_digest(self, text: str, post_link: str, source: str, image_url: str) -> List[Dict]:
        events = []
        lines = text.split("\n")
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if not line:
                i += 1
                continue

            dm = re.match(r"^(\d{1,2}[-]?\d{0,2}[.\s]\d{2}(?:\.\d{4})?|\d{1,2}\s+(?:—è–Ω–≤|—Ñ–µ–≤|–º–∞—Ä|–∞–ø—Ä|–º–∞–π|–∏—é–Ω|–∏—é–ª|–∞–≤–≥|—Å–µ–Ω|–æ–∫—Ç|–Ω–æ—è|–¥–µ–∫)[–∞-—è]*(?:\s+\d{4})?)", line, re.IGNORECASE)
            if not dm:
                i += 1; continue

            date_raw = dm.group(0)
            rest = line[dm.end():].strip()
            tm = re.search(r"(?:–≤\s*)?(\d{1,2}:\d{2})", rest)
            time_str = tm.group(1) if tm else None
            if tm: rest = (rest[:tm.start()] + rest[tm.end():]).strip()

            title_raw = strip_emoji(rest).strip(" -‚Äì‚Ä¢")
            link = None
            lm = re.search(r"((?:https?://|t\.me/)\S+)", line)
            if lm:
                link = lm.group(1)
                if not link.startswith("http"): link = "https://" + link
                title_raw = title_raw.replace(strip_emoji(lm.group(0)), "").strip()
            else:
                for j in range(i + 1, min(i + 4, len(lines))):
                    lm2 = re.search(r"((?:https?://|t\.me/)\S+)", lines[j])
                    if lm2:
                        link = lm2.group(1)
                        if not link.startswith("http"): link = "https://" + link
                        break

            if len(title_raw) < 5 and i + 1 < len(lines):
                nxt = strip_emoji(lines[i + 1]).strip()
                if len(nxt) > 5 and not re.match(r"^\d", nxt): title_raw = nxt

            if len(title_raw) < 5:
                i += 1; continue

            dt = parse_date(date_raw)
            if not is_future(dt):
                i += 1; continue

            ctx = line + " " + (lines[i + 1] if i + 1 < len(lines) else "")
            location = extract_location(ctx) or extract_location(text)
            title_clean = clean_title_deterministic(title_raw) or dedup_title(title_raw[:120])
            if not title_clean:
                i += 1; continue

            events.append({
                "title": title_clean, "date": format_date(dt, time_str), "location": location or "",
                "venue": extract_venue(ctx), "link": link or post_link, "source": source, "image_url": image_url
            })
            i += 1
        return events

    async def parse_channel(self, channel: Dict) -> List[Dict]:
        html = await self.fetch(f"https://t.me/s/{channel['username']}")
        if not html: return []
        soup = BeautifulSoup(html, "html.parser")
        all_events = []

        for msg in soup.find_all("div", class_="tgme_widget_message")[:20]:
            try:
                td = msg.find("div", class_="tgme_widget_message_text")
                if not td: continue
                text = td.get_text(separator="\n", strip=True)
                if len(text) < 30: continue

                le = msg.find("a", class_="tgme_widget_message_date")
                post_link = le["href"] if le else f"https://t.me/{channel['username']}"
                norm_link = normalize_link(post_link)

                external_link = None
                links_in_text = re.findall(r"(https?://[^\s]+)", text)
                for l in links_in_text:
                    clean_l = normalize_link(l)
                    if "t.me" not in clean_l:
                        external_link = clean_l
                        break

                final_link = external_link if external_link else norm_link
                if norm_link in self.posted: continue

                external_link = None
                for a in td.find_all("a", href=True):
                    href = normalize_link(a["href"])
                    if "t.me" not in href:
                        external_link = href
                        break

                if not external_link:
                    links_in_text = re.findall(r"(https?://[^\s]+)", text)
                    for l in links_in_text:
                        clean_l = normalize_link(l)
                        if "t.me" not in clean_l:
                            external_link = clean_l
                            break

                final_link = external_link if external_link else norm_link
                image_url = None

                photo_wrap = msg.find("a", class_="tgme_widget_message_photo_wrap")
                if photo_wrap:
                    style = photo_wrap.get("style", "")
                    match = re.search(r"url\('([^']+)'\)", style)
                    if match: image_url = match.group(1)

                if not image_url:
                    img_tag = td.find("img")
                    if img_tag and img_tag.get("src"): image_url = img_tag["src"]

                if re.search(r"\d{1,2}[.\-]\d{2}\s+(?:–≤\s+)?\d{1,2}:\d{2}", text):
                    evs = self.parse_digest(text, post_link, channel["name"], image_url)
                    all_events.extend(evs)
                    continue

                if not is_real_event(text): continue
                dt = parse_date(text)
                if not is_future(dt): continue

                title_candidate = None
                for ln in text.split("\n"):
                    ln = strip_emoji(ln).strip()
                    if len(ln) > 10:
                        title_candidate = ln
                        break

                raw_title = title_candidate or ""
                city_from_title = extract_city_from_title(raw_title)
                title = clean_title_deterministic(raw_title)
                if not title: continue

                tm2 = re.search(r"\d{1,2}\s+[–∞-—è—ë–ê-–Ø–Å]{3,}[,\s]+(\d{1,2}:\d{2})", text)
                time_str = tm2.group(1) if tm2 else None

                all_events.append({
                    "title": title, "date": format_date(dt, time_str), "location": extract_location(text) or city_from_title or "",
                    "venue": extract_venue(text), "link": final_link, "source": channel["name"], "full_text": text, "image_url": image_url
                })
            except Exception as e:
                logger.error(f"parse_channel error: {e}")
                continue
        return all_events
    
    async def get_all_events(self) -> List[Dict]:
        all_events = []
        logger.info(f"üåê –ü–∞—Ä—Å–∏–Ω–≥ {len(URLS)} —Å–∞–π—Ç–æ–≤...")
        for site in URLS:
            evs = await self.parse_site(site)
            all_events.extend(evs)

        logger.info(f"üì± –ü–∞—Ä—Å–∏–Ω–≥ {len(TELEGRAM_CHANNELS)} –∫–∞–Ω–∞–ª–æ–≤...")
        for ch in TELEGRAM_CHANNELS:
            evs = await self.parse_channel(ch)
            all_events.extend(evs)

        return all_events

    async def parse_site(self, site: Dict) -> List[Dict]:
        html = await self.fetch(site["url"])
        if not html: return []
        soup = BeautifulSoup(html, "html.parser")
        events = []

        for link in soup.find_all("a", href=True)[:80]:
            try:
                href = link.get("href", "")
                title_raw = link.get_text(strip=True)

                if not href or not title_raw or len(title_raw) < 15: continue
                if not href.startswith("http"):
                    from urllib.parse import urljoin
                    href = urljoin(site["url"], href)

                href = normalize_link(href)
                if href.rstrip("/") == normalize_link(site["url"]).rstrip("/"): continue
                if href in self.posted: continue
                if is_site_trash(title_raw): continue
                if not is_real_event(title_raw): continue

                parent = link.find_parent(["div", "article", "li", "section"])
                context = parent.get_text(separator=" ", strip=True) if parent else title_raw
                dt = parse_date(context)

                if not is_future(dt): continue
                image_url = None

                if parent:
                    imgs = parent.find_all("img")
                    for img in imgs:
                        src = img.get("src") or img.get("data-src")
                        if not src: continue
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src
                            break

                if not image_url and parent:
                    style = parent.get("style", "")
                    match = re.search(r"url\(['\"]?([^'\")]+)", style)
                    if match:
                        src = match.group(1)
                        if not src.startswith("http"):
                            from urllib.parse import urljoin
                            src = urljoin(site["url"], src)
                        if is_clean_photo(src):
                            image_url = src

                title_clean = clean_title_deterministic(title_raw) or strip_emoji(dedup_title(title_raw))[:120]
                events.append({
                    "title": title_clean, "date": format_date(dt), "location": extract_location(context) or "",
                    "venue": extract_venue(context), "link": href, "full_text": context, "source": site["name"], "image_url": image_url
                })
                if len(events) >= 5: break
            except Exception:
                continue
        return events
# ‚îÄ‚îÄ‚îÄ main ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
async def main():
    logger.info("üöÄ –°—Ç–∞—Ä—Ç...")
    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    bot_obj = EventBot()
    bot_api = Bot(token=BOT_TOKEN)

    try:
        events = await bot_obj.get_all_events()

        unique, seen = [], set()
        for e in events:
            key = (e.get("title", "")[:60]).lower()
            if key and key not in seen:
                unique.append(e)
                seen.add(key)

        logger.info(f"üìä –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π: {len(unique)}")
        logger.info(f"üì¶ –£–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {len(bot_obj.posted)}")

        posted = 0

        for event in unique[:15]:
            norm_link = normalize_link(event.get("link", ""))

            if norm_link in bot_obj.posted:
                logger.info(f"‚è≠Ô∏è –£–∂–µ –ø—É–±–ª–∏–∫–æ–≤–∞–ª–æ—Å—å: {event.get('title')[:50]}")
                continue

            # üî• 1. –ü–æ–ª—É—á–∞–µ–º –æ–ø–∏—Å–∞–Ω–∏–µ –∏ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —Ñ–æ—Ç–æ
            details = await bot_obj.fetch_event_details(norm_link)
            
            # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ details —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å (—Å –Ω–æ–≤—ã–º –∫–æ–¥–æ–º)
            if isinstance(details, dict):
                if details.get("desc"):
                    event["deep_description"] = details["desc"]
                if details.get("image"):
                    event["image_url"] = details["image"]
            # –ù–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ details —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ —Å—Ç—Ä–æ–∫–∞ (—Å–æ —Å—Ç–∞—Ä—ã–º –∫–æ–¥–æ–º)
            elif isinstance(details, str) and details:
                event["deep_description"] = details

            text = make_post(event)
            if not text:
                continue

            try:
                photo_url = event.get("image_url")
                
                if photo_url:
                    # üî• 2. –ù–ê–î–ï–ñ–ù–ê–Ø –û–¢–ü–†–ê–í–ö–ê: –°–∫–∞—á–∏–≤–∞–µ–º —Ñ–æ—Ç–æ –≤ –±—É—Ñ–µ—Ä, —á—Ç–æ–±—ã –¢–µ–ª–µ–≥—Ä–∞–º –Ω–µ –∫–∞–ø—Ä–∏–∑–Ω–∏—á–∞–ª
                    try:
                        session = await bot_obj.get_session()
                        async with session.get(photo_url, timeout=15) as resp:
                            if resp.status == 200:
                                photo_bytes = await resp.read()
                                await bot_api.send_photo(
                                    chat_id=CHANNEL_ID,
                                    message_thread_id=MESSAGE_THREAD_ID,
                                    photo=photo_bytes,
                                    caption=text,
                                    parse_mode="HTML",
                                )
                            else:
                                raise Exception("Bad HTTP status for image")
                    except Exception as img_e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ñ–æ—Ç–æ, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç. –û—à–∏–±–∫–∞: {img_e}")
                        await bot_api.send_message(
                            chat_id=CHANNEL_ID,
                            message_thread_id=MESSAGE_THREAD_ID,
                            text=text,
                            parse_mode="HTML",
                            disable_web_page_preview=True,
                        )
                else:
                    # –ï—Å–ª–∏ —Ñ–æ—Ç–æ –≤–æ–æ–±—â–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
                    await bot_api.send_message(
                        chat_id=CHANNEL_ID,
                        message_thread_id=MESSAGE_THREAD_ID,
                        text=text,
                        parse_mode="HTML",
                        disable_web_page_preview=True,
                    )

                bot_obj.posted.add(norm_link)
                save_posted(bot_obj.posted)

                posted += 1
                logger.info(f"‚úÖ ({posted}) {event.get('title','')[:50]}")

                await asyncio.sleep(2)

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {e}")

        logger.info(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –û–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –Ω–æ–≤—ã—Ö: {posted}")

    finally:
        await bot_obj.close()

if __name__ == "__main__":
    asyncio.run(main())
